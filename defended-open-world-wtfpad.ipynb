{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7763068,"sourceType":"datasetVersion","datasetId":4540372},{"sourceId":7942687,"sourceType":"datasetVersion","datasetId":4669884}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**using adamax and LR = 0.002**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport pickle\nimport numpy as np  # Add this line\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, BatchNormalization\nfrom keras.layers import Activation, Flatten, Dense, Dropout, ReLU\nfrom keras.initializers import glorot_uniform\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom keras import layers, models\n\n# Load data for non-defended dataset for OW training\ndef LoadDataWTFPAD_Training():\n\n    print(\"Loading non-defended dataset for open-world scenario for training\")\n    # Point to the directory storing data\n    dataset_dir = '/kaggle/input/dataset-defeneded-open-world-wtfpad/'\n\n    # X represents a sequence of traffic directions\n    # y represents a sequence of corresponding label (website's label)\n\n    try:\n        # Load training data\n        with open(dataset_dir + 'X_train_WTFPAD.pkl', 'rb') as handle:\n            X_train = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"X_train loaded\")\n        with open(dataset_dir + 'y_train_WTFPAD.pkl', 'rb') as handle:\n            y_train = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"y_train loaded\")\n\n        # Load validation data\n        with open(dataset_dir + 'X_valid_WTFPAD.pkl', 'rb') as handle:\n            X_valid = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"X_valid loaded\")\n        with open(dataset_dir + 'y_valid_WTFPAD.pkl', 'rb') as handle:\n            y_valid = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"y_valid loaded\")\n\n        print(\"Data dimensions:\")\n        print(\"X: Training data's shape : \", X_train.shape)\n        print(\"y: Training data's shape : \", y_train.shape)\n        print(\"X: Validation data's shape : \", X_valid.shape)\n        print(\"y: Validation data's shape : \", y_valid.shape)\n        \n        print(\"Any None values in X_train:\", np.any(np.isnan(X_train)))\n        print(\"Any None values in y_train:\", np.any(np.isnan(y_train)))\n        print(\"Any None values in X_valid:\", np.any(np.isnan(X_valid)))\n        print(\"Any None values in y_valid:\", np.any(np.isnan(y_valid)))\n        \n        # Merge datasets\n        X_all = np.concatenate((X_train, X_valid), axis=0)\n        y_all = np.concatenate((y_train, y_valid), axis=0)\n\n        print(\"Merged data dimensions:\")\n        print(\"X: Merged data's shape : \", X_all.shape)\n        print(\"y: Merged data's shape : \", y_all.shape)\n        \n        # Print features of the merged dataset\n        print(\"Features of the merged dataset:\")\n        print(X_all)\n         # Print features of the merged dataset\n        print(\"Features of the merged dataset:\")\n        print(y_all)\n        \n       \n        return X_all, y_all\n\n\n    except Exception as e:\n        print(\"An error occurred:\", str(e))\n        return None\n\n# Call the function to load, merge, and balance data\nX_all, y_all = LoadDataWTFPAD_Training()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:33:28.747775Z","iopub.execute_input":"2024-04-09T05:33:28.748340Z","iopub.status.idle":"2024-04-09T05:34:39.798265Z","shell.execute_reply.started":"2024-04-09T05:33:28.748310Z","shell.execute_reply":"2024-04-09T05:34:39.797259Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-04-09 05:33:31.031586: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-09 05:33:31.031721: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-09 05:33:31.203587: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Loading non-defended dataset for open-world scenario for training\nX_train loaded\ny_train loaded\nX_valid loaded\ny_valid loaded\nData dimensions:\nX: Training data's shape :  (96000, 5000)\ny: Training data's shape :  (96000,)\nX: Validation data's shape :  (9600, 5000)\ny: Validation data's shape :  (9600,)\nAny None values in X_train: False\nAny None values in y_train: False\nAny None values in X_valid: False\nAny None values in y_valid: False\nMerged data dimensions:\nX: Merged data's shape :  (105600, 5000)\ny: Merged data's shape :  (105600,)\nFeatures of the merged dataset:\n[[-1. -1.  1. ...  1. -1.  1.]\n [-1.  1. -1. ...  0.  0.  0.]\n [ 1. -1.  1. ...  0.  0.  0.]\n ...\n [ 1. -1.  1. ...  0.  0.  0.]\n [ 1. -1.  1. ... -1. -1. -1.]\n [-1.  1.  1. ... -1.  1.  1.]]\nFeatures of the merged dataset:\n[80 83 95 ... 95 34 44]\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport pickle\nimport numpy as np  # Add this line\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, BatchNormalization\nfrom keras.layers import Activation, Flatten, Dense, Dropout,ReLU\nfrom keras.initializers import glorot_uniform\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom keras import layers, models\n\n# Load data for non-defended dataset for OW training\ndef LoadDataWTFPAD_Training():\n\n    print(\"Loading non-defended dataset for open-world scenario for training\")\n    # Point to the directory storing data\n    dataset_dir = '/kaggle/input/dataset-defeneded-open-world-wtfpad/'\n\n    # X represents a sequence of traffic directions\n    # y represents a sequence of corresponding label (website's label)\n\n    try:\n        # Load training data\n        with open(dataset_dir + 'X_train_WTFPAD.pkl', 'rb') as handle:\n            X_train = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"X_train loaded\")\n        with open(dataset_dir + 'y_train_WTFPAD.pkl', 'rb') as handle:\n            y_train = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"y_train loaded\")\n\n        # Load validation data\n        with open(dataset_dir + 'X_valid_WTFPAD.pkl', 'rb') as handle:\n            X_valid = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"X_valid loaded\")\n        with open(dataset_dir + 'y_valid_WTFPAD.pkl', 'rb') as handle:\n            y_valid = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"y_valid loaded\")\n\n        print(\"Data dimensions:\")\n        print(\"X: Training data's shape : \", X_train.shape)\n        print(\"y: Training data's shape : \", y_train.shape)\n        print(\"X: Validation data's shape : \", X_valid.shape)\n        print(\"y: Validation data's shape : \", y_valid.shape)\n        \n        print(\"Any None values in X_train:\", np.any(np.isnan(X_train)))\n        print(\"Any None values in y_train:\", np.any(np.isnan(y_train)))\n        print(\"Any None values in X_valid:\", np.any(np.isnan(X_valid)))\n        print(\"Any None values in y_valid:\", np.any(np.isnan(y_valid)))\n        \n        # Merge datasets\n        X_all = np.concatenate((X_train, X_valid), axis=0)\n        y_all = np.concatenate((y_train, y_valid), axis=0)\n\n        print(\"Merged data dimensions:\")\n        print(\"X: Merged data's shape : \", X_all.shape)\n        print(\"y: Merged data's shape : \", y_all.shape)\n        \n        # Print features of the merged dataset\n        print(\"Features of the merged dataset:\")\n        print(X_all)\n         # Print features of the merged dataset\n        print(\"Features of the merged dataset:\")\n        print(y_all)\n        \n       \n        return X_all, y_all\n\n\n    except Exception as e:\n        print(\"An error occurred:\", str(e))\n        return None\n\n# Call the function to load, merge, and balance data\nX_all, y_all =  LoadDataWTFPAD_Training() \n","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:36:11.671026Z","iopub.execute_input":"2024-04-09T05:36:11.671681Z","iopub.status.idle":"2024-04-09T05:36:17.606109Z","shell.execute_reply.started":"2024-04-09T05:36:11.671641Z","shell.execute_reply":"2024-04-09T05:36:17.605105Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Loading non-defended dataset for open-world scenario for training\nX_train loaded\ny_train loaded\nX_valid loaded\ny_valid loaded\nData dimensions:\nX: Training data's shape :  (96000, 5000)\ny: Training data's shape :  (96000,)\nX: Validation data's shape :  (9600, 5000)\ny: Validation data's shape :  (9600,)\nAny None values in X_train: False\nAny None values in y_train: False\nAny None values in X_valid: False\nAny None values in y_valid: False\nMerged data dimensions:\nX: Merged data's shape :  (105600, 5000)\ny: Merged data's shape :  (105600,)\nFeatures of the merged dataset:\n[[-1. -1.  1. ...  1. -1.  1.]\n [-1.  1. -1. ...  0.  0.  0.]\n [ 1. -1.  1. ...  0.  0.  0.]\n ...\n [ 1. -1.  1. ...  0.  0.  0.]\n [ 1. -1.  1. ... -1. -1. -1.]\n [-1.  1.  1. ... -1.  1.  1.]]\nFeatures of the merged dataset:\n[80 83 95 ... 95 34 44]\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, BatchNormalization\nfrom keras.layers import Activation, Flatten, Dense, Dropout, ReLU\nfrom keras.initializers import glorot_uniform\nfrom keras.optimizers import Adamax\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom keras.models import save_model\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom keras import layers, models\nimport numpy as np\nfrom keras.layers import Activation\n\nclass DFNet:\n    @staticmethod\n    def build(input_shape, classes, dropout_rate1=0.3, dropout_rate2=0.5, dropout_rate_fc=0.7):\n        model = Sequential()\n\n        # ... (previous model definition remains unchanged)\n        # Block 1\n        filter_num = [None,32, 64, 128, 256]\n        kernel_size = [None, 8, 8, 8, 8]\n        conv_stride_size = [None, 1, 1, 1, 1]\n        pool_stride_size = [None, 4, 4, 4, 4]\n        pool_size = [None, 8, 8, 8, 8]\n\n        for i in range(1, 4):\n            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n                             strides=conv_stride_size[i], padding='same',\n                             input_shape=input_shape if i == 1 else (None, input_shape[1]),\n                             name=f'block{i}_conv1'))\n            model.add(BatchNormalization(name=f'batch_normalization_{i * 2 - 1}'))\n            model.add(Activation('relu', name=f'block{i}_adv_act1'))\n            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n                             strides=conv_stride_size[i], padding='same',\n                             name=f'block{i}_conv2'))\n            model.add(BatchNormalization(name=f'batch_normalization_{i * 2}'))\n            model.add(Activation('relu', name=f'block{i}_adv_act2'))\n\n            model.add(MaxPooling1D(pool_size=pool_size[i], strides=pool_stride_size[i],\n                                   padding='same', name=f'block{i}_pool'))\n            model.add(Dropout(0.1, name=f'block{i}_dropout'))\n\n        # ... (rest of the model remains unchanged)\n\n        model.add(Flatten(name='flatten'))\n\n        # Fully connected layers\n        for i in range(1, 3):\n            model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name=f'fc{i}'))\n            model.add(BatchNormalization(name=f'batch_normalization_{i + 8}'))\n            model.add(Activation('relu', name=f'fc{i}_act'))\n\n            # Experiment with different dropout rates\n            model.add(Dropout(dropout_rate_fc, name=f'fc{i}_dropout'))\n\n        # Output layer\n        model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='fc_final'))\n        model.add(Activation('softmax', name=\"softmax\"))\n\n        print(\"Model built successfully.\")\n        return model\n    # Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\ninput_shape = (5000, 1)\nnum_classes = 96\n\n# Experiment with different dropout rates\ndropout_rate1 = 0.6\ndropout_rate2 = 0.5\ndropout_rate_fc = 0.4\n\n# Build the model with the specified dropout rates\nmodel = DFNet.build(input_shape=input_shape, classes=num_classes,\n                    dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n\n\n# Print the model summary\nprint(\"Model Summary:\")\nmodel.summary()\n\n# Compile the model\noptimizer = Adamax(learning_rate=0.002)\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n\n# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\ninput_shape = (5000, 1)\nnum_classes = 96\n\n# Experiment with different dropout rates\ndropout_rate1 = 0.6\ndropout_rate2 = 0.5\ndropout_rate_fc = 0.4\n\n# Load and merge data using the function you defined\nX_all, y_all = LoadDataWTFPAD_Training()\n# Define the number of folds\nn_splits = 5  # You can adjust this as needed\n\n# Initialize Stratified K-Fold\nstratkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Lists to store results\nall_histories = []\n\n# Iterate over folds\nfor fold, (train_index, test_index) in enumerate(stratkf.split(X_all, y_all)):\n    print(f\"\\nTraining on Fold {fold + 1}/{n_splits}\")\n\n    # Split the data\n    X_train, X_test = X_all[train_index], X_all[test_index]\n    y_train, y_test = y_all[train_index], y_all[test_index]\n    \n        # Build the model\n    model = DFNet.build(input_shape=input_shape, classes=num_classes,\n                        dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n\n    # Compile the model\n    optimizer = Adamax(learning_rate=0.002)\n    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n     # Define early stopping callback\n    early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n\n    # Train the model with early stopping\n    history = model.fit(X_train, y_train, epochs= 25, batch_size=64, \n                        validation_data=(X_test, y_test), verbose=2, callbacks=[early_stopping])\n    \n    # Train the model\n    #history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n\n    # Evaluate the model on the test set\n    loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n    print(f\"\\nFold {fold + 1} - Test Accuracy: {accuracy * 100:.2f}%\")\n\n    # Save history for later analysis if needed\n    all_histories.append(history)\n    # Check unique labels in y_train\n    \n    # Update num_classes based on the actual number of unique classes in your dataset\n#num_classes = len(np.unique(y_all))\nmodel_save_path = '/kaggle/working/DFNet_OpenWorld_WTFPAD.keras'\nsave_model(model,model_save_path)\n\nprint(\"Trained model saved successfully at:\", model_save_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:36:23.796268Z","iopub.execute_input":"2024-04-09T05:36:23.796629Z","iopub.status.idle":"2024-04-09T09:13:29.057234Z","shell.execute_reply.started":"2024-04-09T05:36:23.796600Z","shell.execute_reply":"2024-04-09T09:13:29.056269Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Model built successfully.\nModel Summary:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ block1_conv1 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m288\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act1 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_conv2 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m8,224\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act2 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_pool (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv1 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m16,448\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act1 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv2 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m32,832\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act2 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_pool (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv1 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m65,664\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act1 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv2 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m131,200\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act2 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_pool (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10112\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m5,177,856\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_act (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_dropout (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_act (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_dropout (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc_final (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │        \u001b[38;5;34m49,248\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ softmax (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ block1_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,224</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10112</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,177,856</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_act (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_act (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc_final (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,248</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,750,304\u001b[0m (21.94 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,750,304</span> (21.94 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,747,360\u001b[0m (21.92 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,747,360</span> (21.92 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,944\u001b[0m (11.50 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,944</span> (11.50 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Loading non-defended dataset for open-world scenario for training\nX_train loaded\ny_train loaded\nX_valid loaded\ny_valid loaded\nData dimensions:\nX: Training data's shape :  (96000, 5000)\ny: Training data's shape :  (96000,)\nX: Validation data's shape :  (9600, 5000)\ny: Validation data's shape :  (9600,)\nAny None values in X_train: False\nAny None values in y_train: False\nAny None values in X_valid: False\nAny None values in y_valid: False\nMerged data dimensions:\nX: Merged data's shape :  (105600, 5000)\ny: Merged data's shape :  (105600,)\nFeatures of the merged dataset:\n[[-1. -1.  1. ...  1. -1.  1.]\n [-1.  1. -1. ...  0.  0.  0.]\n [ 1. -1.  1. ...  0.  0.  0.]\n ...\n [ 1. -1.  1. ...  0.  0.  0.]\n [ 1. -1.  1. ... -1. -1. -1.]\n [-1.  1.  1. ... -1.  1.  1.]]\nFeatures of the merged dataset:\n[80 83 95 ... 95 34 44]\n\nTraining on Fold 1/5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Model built successfully.\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1712641021.964071     116 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1712641021.996314     116 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1712641121.422323     115 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1320/1320 - 127s - 96ms/step - accuracy: 0.3458 - loss: 2.4371 - val_accuracy: 0.3451 - val_loss: 2.5501\nEpoch 2/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.5458 - loss: 1.5788 - val_accuracy: 0.5869 - val_loss: 1.5139\nEpoch 3/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.6344 - loss: 1.2625 - val_accuracy: 0.7098 - val_loss: 1.0192\nEpoch 4/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.6875 - loss: 1.0701 - val_accuracy: 0.7458 - val_loss: 0.8728\nEpoch 5/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.7277 - loss: 0.9344 - val_accuracy: 0.7336 - val_loss: 0.9059\nEpoch 6/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.7529 - loss: 0.8384 - val_accuracy: 0.7778 - val_loss: 0.7765\nEpoch 7/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.7732 - loss: 0.7666 - val_accuracy: 0.8039 - val_loss: 0.6794\nEpoch 8/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.7922 - loss: 0.7004 - val_accuracy: 0.7991 - val_loss: 0.6921\nEpoch 9/25\n1320/1320 - 142s - 108ms/step - accuracy: 0.8061 - loss: 0.6471 - val_accuracy: 0.8052 - val_loss: 0.6779\nEpoch 10/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8198 - loss: 0.5987 - val_accuracy: 0.8196 - val_loss: 0.6397\nEpoch 11/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8290 - loss: 0.5626 - val_accuracy: 0.8223 - val_loss: 0.6278\nEpoch 12/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8413 - loss: 0.5222 - val_accuracy: 0.8133 - val_loss: 0.6466\nEpoch 13/25\n1320/1320 - 142s - 108ms/step - accuracy: 0.8482 - loss: 0.4941 - val_accuracy: 0.8080 - val_loss: 0.6938\nEpoch 14/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8568 - loss: 0.4665 - val_accuracy: 0.8124 - val_loss: 0.6533\nEpoch 15/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8638 - loss: 0.4434 - val_accuracy: 0.8184 - val_loss: 0.6505\nEpoch 16/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8702 - loss: 0.4203 - val_accuracy: 0.7902 - val_loss: 0.7582\nEpoch 17/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8742 - loss: 0.4034 - val_accuracy: 0.8231 - val_loss: 0.6573\nEpoch 18/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8785 - loss: 0.3867 - val_accuracy: 0.8241 - val_loss: 0.6567\nEpoch 19/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8834 - loss: 0.3713 - val_accuracy: 0.8217 - val_loss: 0.6703\nEpoch 20/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8890 - loss: 0.3527 - val_accuracy: 0.8149 - val_loss: 0.6796\nEpoch 21/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8932 - loss: 0.3406 - val_accuracy: 0.8205 - val_loss: 0.6979\nEpoch 22/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8991 - loss: 0.3243 - val_accuracy: 0.8270 - val_loss: 0.6318\nEpoch 23/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.9018 - loss: 0.3122 - val_accuracy: 0.8254 - val_loss: 0.6572\nEpoch 24/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.9038 - loss: 0.3043 - val_accuracy: 0.8254 - val_loss: 0.6712\nEpoch 25/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.9076 - loss: 0.2954 - val_accuracy: 0.8360 - val_loss: 0.6402\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1712643642.111419     116 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"660/660 - 5s - 8ms/step - accuracy: 0.8223 - loss: 0.6278\n\nFold 1 - Test Accuracy: 82.23%\n\nTraining on Fold 2/5\nModel built successfully.\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1712643664.703838     113 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1712643765.081825     114 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1320/1320 - 117s - 88ms/step - accuracy: 0.3531 - loss: 2.4104 - val_accuracy: 0.5274 - val_loss: 1.6742\nEpoch 2/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.5637 - loss: 1.5220 - val_accuracy: 0.6607 - val_loss: 1.1823\nEpoch 3/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.6465 - loss: 1.2201 - val_accuracy: 0.7272 - val_loss: 0.9587\nEpoch 4/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.6967 - loss: 1.0403 - val_accuracy: 0.7336 - val_loss: 0.9239\nEpoch 5/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.7302 - loss: 0.9186 - val_accuracy: 0.7750 - val_loss: 0.7955\nEpoch 6/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.7566 - loss: 0.8223 - val_accuracy: 0.7864 - val_loss: 0.7412\nEpoch 7/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.7759 - loss: 0.7507 - val_accuracy: 0.7943 - val_loss: 0.7092\nEpoch 8/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.7941 - loss: 0.6862 - val_accuracy: 0.8009 - val_loss: 0.6966\nEpoch 9/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8097 - loss: 0.6341 - val_accuracy: 0.7964 - val_loss: 0.7127\nEpoch 10/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8210 - loss: 0.5908 - val_accuracy: 0.7977 - val_loss: 0.7155\nEpoch 11/25\n1320/1320 - 102s - 78ms/step - accuracy: 0.8323 - loss: 0.5488 - val_accuracy: 0.8111 - val_loss: 0.6678\nEpoch 12/25\n1320/1320 - 102s - 78ms/step - accuracy: 0.8429 - loss: 0.5159 - val_accuracy: 0.7966 - val_loss: 0.7230\nEpoch 13/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8505 - loss: 0.4867 - val_accuracy: 0.8099 - val_loss: 0.6759\nEpoch 14/25\n1320/1320 - 102s - 78ms/step - accuracy: 0.8583 - loss: 0.4594 - val_accuracy: 0.8202 - val_loss: 0.6405\nEpoch 15/25\n1320/1320 - 103s - 78ms/step - accuracy: 0.8670 - loss: 0.4315 - val_accuracy: 0.8249 - val_loss: 0.6307\nEpoch 16/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8721 - loss: 0.4106 - val_accuracy: 0.8192 - val_loss: 0.6550\nEpoch 17/25\n1320/1320 - 103s - 78ms/step - accuracy: 0.8783 - loss: 0.3887 - val_accuracy: 0.8215 - val_loss: 0.6554\nEpoch 18/25\n1320/1320 - 103s - 78ms/step - accuracy: 0.8850 - loss: 0.3683 - val_accuracy: 0.8211 - val_loss: 0.6584\nEpoch 19/25\n1320/1320 - 103s - 78ms/step - accuracy: 0.8874 - loss: 0.3597 - val_accuracy: 0.8125 - val_loss: 0.7082\nEpoch 20/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8922 - loss: 0.3467 - val_accuracy: 0.8261 - val_loss: 0.6439\nEpoch 21/25\n1320/1320 - 103s - 78ms/step - accuracy: 0.8963 - loss: 0.3293 - val_accuracy: 0.8281 - val_loss: 0.6541\nEpoch 22/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.9009 - loss: 0.3157 - val_accuracy: 0.8265 - val_loss: 0.6744\nEpoch 23/25\n1320/1320 - 103s - 78ms/step - accuracy: 0.9039 - loss: 0.3035 - val_accuracy: 0.8331 - val_loss: 0.6525\nEpoch 24/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.9053 - loss: 0.2981 - val_accuracy: 0.8265 - val_loss: 0.6654\nEpoch 25/25\n1320/1320 - 102s - 78ms/step - accuracy: 0.9087 - loss: 0.2876 - val_accuracy: 0.8241 - val_loss: 0.6745\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1712646220.896559     114 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"660/660 - 4s - 6ms/step - accuracy: 0.8249 - loss: 0.6307\n\nFold 2 - Test Accuracy: 82.49%\n\nTraining on Fold 3/5\nModel built successfully.\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1712646243.280812     115 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1712646343.825091     116 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1320/1320 - 116s - 88ms/step - accuracy: 0.3376 - loss: 2.4775 - val_accuracy: 0.4748 - val_loss: 1.8673\nEpoch 2/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.5505 - loss: 1.5705 - val_accuracy: 0.6473 - val_loss: 1.2186\nEpoch 3/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.6396 - loss: 1.2440 - val_accuracy: 0.7020 - val_loss: 1.0197\nEpoch 4/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.6924 - loss: 1.0578 - val_accuracy: 0.7426 - val_loss: 0.8818\nEpoch 5/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.7255 - loss: 0.9378 - val_accuracy: 0.7683 - val_loss: 0.7925\nEpoch 6/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.7533 - loss: 0.8394 - val_accuracy: 0.7496 - val_loss: 0.8428\nEpoch 7/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.7734 - loss: 0.7662 - val_accuracy: 0.7886 - val_loss: 0.7255\nEpoch 8/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.7894 - loss: 0.7037 - val_accuracy: 0.8071 - val_loss: 0.6717\nEpoch 9/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8031 - loss: 0.6532 - val_accuracy: 0.7887 - val_loss: 0.7325\nEpoch 10/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8189 - loss: 0.6060 - val_accuracy: 0.8182 - val_loss: 0.6277\nEpoch 11/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8286 - loss: 0.5648 - val_accuracy: 0.8199 - val_loss: 0.6359\nEpoch 12/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8386 - loss: 0.5325 - val_accuracy: 0.8182 - val_loss: 0.6396\nEpoch 13/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8469 - loss: 0.4979 - val_accuracy: 0.8015 - val_loss: 0.6953\nEpoch 14/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8529 - loss: 0.4798 - val_accuracy: 0.8078 - val_loss: 0.6661\nEpoch 15/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8634 - loss: 0.4437 - val_accuracy: 0.8143 - val_loss: 0.6523\nEpoch 16/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8688 - loss: 0.4215 - val_accuracy: 0.8188 - val_loss: 0.6267\nEpoch 17/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8732 - loss: 0.4064 - val_accuracy: 0.8253 - val_loss: 0.6272\nEpoch 18/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8802 - loss: 0.3868 - val_accuracy: 0.8083 - val_loss: 0.6945\nEpoch 19/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8842 - loss: 0.3714 - val_accuracy: 0.8300 - val_loss: 0.6327\nEpoch 20/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8871 - loss: 0.3616 - val_accuracy: 0.8257 - val_loss: 0.6459\nEpoch 21/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8933 - loss: 0.3404 - val_accuracy: 0.8327 - val_loss: 0.6234\nEpoch 22/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8943 - loss: 0.3339 - val_accuracy: 0.8242 - val_loss: 0.6337\nEpoch 23/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.8986 - loss: 0.3229 - val_accuracy: 0.8272 - val_loss: 0.6671\nEpoch 24/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.9031 - loss: 0.3079 - val_accuracy: 0.8357 - val_loss: 0.6455\nEpoch 25/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.9053 - loss: 0.3005 - val_accuracy: 0.8339 - val_loss: 0.6429\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1712648783.458647     116 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"660/660 - 4s - 6ms/step - accuracy: 0.8327 - loss: 0.6234\n\nFold 3 - Test Accuracy: 83.27%\n\nTraining on Fold 4/5\nModel built successfully.\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1712648806.480645     114 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1712648907.256637     113 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1320/1320 - 117s - 88ms/step - accuracy: 0.3449 - loss: 2.4461 - val_accuracy: 0.4753 - val_loss: 1.8730\nEpoch 2/25\n1320/1320 - 102s - 78ms/step - accuracy: 0.5540 - loss: 1.5552 - val_accuracy: 0.5607 - val_loss: 1.5218\nEpoch 3/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.6423 - loss: 1.2349 - val_accuracy: 0.6947 - val_loss: 1.0563\nEpoch 4/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.6942 - loss: 1.0502 - val_accuracy: 0.7383 - val_loss: 0.9011\nEpoch 5/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.7280 - loss: 0.9306 - val_accuracy: 0.7206 - val_loss: 0.9683\nEpoch 6/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.7542 - loss: 0.8353 - val_accuracy: 0.7722 - val_loss: 0.7875\nEpoch 7/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.7760 - loss: 0.7614 - val_accuracy: 0.7932 - val_loss: 0.7095\nEpoch 8/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.7931 - loss: 0.6986 - val_accuracy: 0.7895 - val_loss: 0.7225\nEpoch 9/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8056 - loss: 0.6489 - val_accuracy: 0.7990 - val_loss: 0.6939\nEpoch 10/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8201 - loss: 0.5989 - val_accuracy: 0.7840 - val_loss: 0.7596\nEpoch 11/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8315 - loss: 0.5596 - val_accuracy: 0.7941 - val_loss: 0.7204\nEpoch 12/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8392 - loss: 0.5276 - val_accuracy: 0.8103 - val_loss: 0.6638\nEpoch 13/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8504 - loss: 0.4893 - val_accuracy: 0.8095 - val_loss: 0.6666\nEpoch 14/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8571 - loss: 0.4669 - val_accuracy: 0.7996 - val_loss: 0.7115\nEpoch 15/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8639 - loss: 0.4439 - val_accuracy: 0.8151 - val_loss: 0.6766\nEpoch 16/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8696 - loss: 0.4220 - val_accuracy: 0.8011 - val_loss: 0.7167\nEpoch 17/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8768 - loss: 0.3960 - val_accuracy: 0.8090 - val_loss: 0.7085\nEpoch 18/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8799 - loss: 0.3849 - val_accuracy: 0.8170 - val_loss: 0.6741\nEpoch 19/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8874 - loss: 0.3662 - val_accuracy: 0.8039 - val_loss: 0.7102\nEpoch 20/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8882 - loss: 0.3581 - val_accuracy: 0.8116 - val_loss: 0.6970\nEpoch 21/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8956 - loss: 0.3363 - val_accuracy: 0.8090 - val_loss: 0.7075\nEpoch 22/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8982 - loss: 0.3242 - val_accuracy: 0.8231 - val_loss: 0.6939\nEpoch 23/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8999 - loss: 0.3169 - val_accuracy: 0.8134 - val_loss: 0.6959\nEpoch 24/25\n1320/1320 - 145s - 109ms/step - accuracy: 0.9039 - loss: 0.3040 - val_accuracy: 0.8186 - val_loss: 0.7038\nEpoch 25/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.9063 - loss: 0.2962 - val_accuracy: 0.8217 - val_loss: 0.6910\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1712651397.795052     113 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"660/660 - 4s - 6ms/step - accuracy: 0.8103 - loss: 0.6638\n\nFold 4 - Test Accuracy: 81.03%\n\nTraining on Fold 5/5\nModel built successfully.\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1712651420.501038     114 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1712651521.072781     115 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1320/1320 - 117s - 88ms/step - accuracy: 0.3595 - loss: 2.3808 - val_accuracy: 0.5408 - val_loss: 1.6299\nEpoch 2/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.5666 - loss: 1.5171 - val_accuracy: 0.6050 - val_loss: 1.3568\nEpoch 3/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.6469 - loss: 1.2167 - val_accuracy: 0.7221 - val_loss: 0.9654\nEpoch 4/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.6935 - loss: 1.0441 - val_accuracy: 0.7372 - val_loss: 0.9236\nEpoch 5/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.7285 - loss: 0.9195 - val_accuracy: 0.7596 - val_loss: 0.8154\nEpoch 6/25\n1320/1320 - 142s - 108ms/step - accuracy: 0.7550 - loss: 0.8312 - val_accuracy: 0.7789 - val_loss: 0.7647\nEpoch 7/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.7755 - loss: 0.7528 - val_accuracy: 0.7896 - val_loss: 0.7401\nEpoch 8/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.7928 - loss: 0.6920 - val_accuracy: 0.7984 - val_loss: 0.6938\nEpoch 9/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8094 - loss: 0.6355 - val_accuracy: 0.7977 - val_loss: 0.6994\nEpoch 10/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8203 - loss: 0.5926 - val_accuracy: 0.8149 - val_loss: 0.6509\nEpoch 11/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8311 - loss: 0.5547 - val_accuracy: 0.8074 - val_loss: 0.6752\nEpoch 12/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8413 - loss: 0.5156 - val_accuracy: 0.8225 - val_loss: 0.6569\nEpoch 13/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8505 - loss: 0.4871 - val_accuracy: 0.8124 - val_loss: 0.6557\nEpoch 14/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8574 - loss: 0.4603 - val_accuracy: 0.8286 - val_loss: 0.6273\nEpoch 15/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8650 - loss: 0.4340 - val_accuracy: 0.8192 - val_loss: 0.6483\nEpoch 16/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8705 - loss: 0.4145 - val_accuracy: 0.8177 - val_loss: 0.6581\nEpoch 17/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8786 - loss: 0.3883 - val_accuracy: 0.8236 - val_loss: 0.6423\nEpoch 18/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8806 - loss: 0.3788 - val_accuracy: 0.8225 - val_loss: 0.6620\nEpoch 19/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8869 - loss: 0.3590 - val_accuracy: 0.8285 - val_loss: 0.6487\nEpoch 20/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8926 - loss: 0.3443 - val_accuracy: 0.8129 - val_loss: 0.7064\nEpoch 21/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8940 - loss: 0.3336 - val_accuracy: 0.8208 - val_loss: 0.7160\nEpoch 22/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.8985 - loss: 0.3235 - val_accuracy: 0.8141 - val_loss: 0.7105\nEpoch 23/25\n1320/1320 - 102s - 77ms/step - accuracy: 0.9014 - loss: 0.3113 - val_accuracy: 0.8219 - val_loss: 0.6957\nEpoch 24/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.9041 - loss: 0.3010 - val_accuracy: 0.8231 - val_loss: 0.6995\nEpoch 25/25\n1320/1320 - 101s - 77ms/step - accuracy: 0.9086 - loss: 0.2886 - val_accuracy: 0.8235 - val_loss: 0.6789\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1712654005.822986     113 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"660/660 - 4s - 6ms/step - accuracy: 0.8286 - loss: 0.6273\n\nFold 5 - Test Accuracy: 82.86%\nTrained model saved successfully at: /kaggle/working/DFNet_OpenWorld_WTFPAD.keras\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the model to the Kaggle output directory\nmodel_save_path = '/kaggle/working/DFNet_OpenWorld_WTFPAD.keras'\nmodel.save(model_save_path)\nprint(\"Trained model saved successfully at:\", model_save_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-09T17:33:19.081174Z","iopub.execute_input":"2024-04-09T17:33:19.081526Z","iopub.status.idle":"2024-04-09T17:33:19.395090Z","shell.execute_reply.started":"2024-04-09T17:33:19.081499Z","shell.execute_reply":"2024-04-09T17:33:19.394069Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Trained model saved successfully at: /kaggle/working/DFNet_OpenWorld_WTFPAD.keras\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n# Define the directory for saving the model\nexpected_directory = '/kaggle/working/'\nexpected_filename = 'DFNet_OpenWorld_WTFPAD.keras'\n\n# Construct the full path\nsaved_path_keras = os.path.join(expected_directory, expected_filename)\n\n# Check if the directory exists\nif not os.path.exists(expected_directory):\n    print(\"Error: Directory does not exist:\", expected_directory)\nelse:\n    # Check if the model was saved successfully\n    try:\n        # Your code for saving the model goes here\n        # For example:\n        # model.save(saved_path_keras)\n        # If an error occurs during the saving process, it will be caught here\n        print(\"Model saved successfully.\")\n    except Exception as e:\n        print(\"Error occurred while saving the model:\", e)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-09T17:33:35.227858Z","iopub.execute_input":"2024-04-09T17:33:35.228120Z","iopub.status.idle":"2024-04-09T17:33:35.234538Z","shell.execute_reply.started":"2024-04-09T17:33:35.228097Z","shell.execute_reply":"2024-04-09T17:33:35.233639Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Model saved successfully.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**evaluation**","metadata":{}},{"cell_type":"code","source":"# Load data for non-defended dataset for OW evaluation\nimport pickle\nimport numpy as np\ndef LoadDataWTFPADOW_Evaluation():\n\n    print(\"Loading defended dataset for open-world scenario for evaluation\")\n    # Point to the directory storing data\n    dataset_dir = '/kaggle/input/dataset-defeneded-open-world-wtfpad/'\n\n    # X represents a sequence of traffic directions\n    # y represents a sequence of corresponding label (website's label)\n    try:\n        # Load testing data\n        with open(dataset_dir + 'X_test_Mon_WTFPAD.pkl', 'rb') as handle:\n            X_test_Mon = np.array(pickle.load(handle, encoding='latin1'))\n        with open(dataset_dir + 'y_test_Mon_WTFPAD.pkl', 'rb') as handle:\n            y_test_Mon = np.array(pickle.load(handle, encoding='latin1'))\n        with open(dataset_dir + 'X_test_Unmon_WTFPAD.pkl', 'rb') as handle:\n            X_test_Unmon = np.array(pickle.load(handle, encoding='latin1'))\n        with open(dataset_dir + 'y_test_Unmon_WTFPAD.pkl', 'rb') as handle:\n            y_test_Unmon = np.array(pickle.load(handle, encoding='latin1'))\n\n        X_test_Mon = np.array(X_test_Mon)\n        y_test_Mon = np.array(y_test_Mon)\n        X_test_Unmon = np.array(X_test_Unmon)\n        y_test_Unmon = np.array(y_test_Unmon)\n\n        return X_test_Mon, y_test_Mon, X_test_Unmon, y_test_Unmon\n\n    except Exception as e:\n        print(\"An error occurred:\", str(e))\n        return None\n\n# Call the function to load evaluation data\nLoadDataWTFPADOW_Evaluation()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T17:37:43.857209Z","iopub.execute_input":"2024-04-09T17:37:43.857589Z","iopub.status.idle":"2024-04-09T17:37:45.962808Z","shell.execute_reply.started":"2024-04-09T17:37:43.857561Z","shell.execute_reply":"2024-04-09T17:37:45.961856Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Loading defended dataset for open-world scenario for evaluation\n","output_type":"stream"},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"(array([[ 1.,  1., -1., ...,  0.,  0.,  0.],\n        [ 1., -1.,  1., ...,  0.,  0.,  0.],\n        [ 1.,  1.,  1., ...,  0.,  0.,  0.],\n        ...,\n        [-1.,  1., -1., ...,  0.,  0.,  0.],\n        [-1.,  1., -1., ...,  0.,  0.,  0.],\n        [ 1.,  1., -1., ...,  0.,  0.,  0.]]),\n array([ 0,  0,  0, ..., 94, 94, 94]),\n array([[ 1., -1.,  1., ...,  0.,  0.,  0.],\n        [ 1., -1.,  1., ..., -1., -1., -1.],\n        [-1.,  1., -1., ...,  0.,  0.,  0.],\n        ...,\n        [-1., -1., -1., ..., -1., -1.,  1.],\n        [-1.,  1., -1., ...,  0.,  0.,  0.],\n        [-1.,  1., -1., ...,  0.,  0.,  0.]]),\n array([95, 95, 95, ..., 95, 95, 95]))"},"metadata":{}}]},{"cell_type":"code","source":"from keras.models import load_model\nimport numpy as np\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\ndef Prediction(trained_model = None, dataset = None):\n    X_test_Mon = dataset['X_test_Mon'].astype('float32')\n    X_test_Unmon = dataset['X_test_Unmon'].astype('float32')\n    print (\"Total testing data \", len(X_test_Mon) + len(X_test_Unmon))\n    X_test_Mon = X_test_Mon[:, :, np.newaxis]\n    X_test_Unmon = X_test_Unmon[:, :, np.newaxis]\n    result_Mon = trained_model.predict(X_test_Mon, verbose=2)\n    result_Unmon = trained_model.predict(X_test_Unmon, verbose=2)\n    return result_Mon, result_Unmon\ndef Evaluation(threshold_val = None, monitored_label = None,\n                   unmonitored_label = None, result_Mon = None,\n                   result_Unmon = None, log_file = None):\n    print (\"Testing with threshold = \", threshold_val)\n    TP = 0\n    FP = 0\n    TN = 0\n    FN = 0\n\n    # ==============================================================\n    # Test with Monitored testing instances\n    # evaluation\n    for i in range(len(result_Mon)):\n        sm_vector = result_Mon[i]\n        predicted_class = np.argmax(sm_vector)\n        max_prob = max(sm_vector)\n\n        if predicted_class in monitored_label: # predicted as Monitored\n            if max_prob >= threshold_val: # predicted as Monitored and actual site is Monitored\n                TP = TP + 1\n            else: # predicted as Unmonitored and actual site is Monitored\n                FN = FN + 1\n        elif predicted_class in unmonitored_label: # predicted as Unmonitored and actual site is Monitored\n            FN = FN + 1\n\n    # ==============================================================\n    # Test with Unmonitored testing instances\n    # evaluation\n    for i in range(len(result_Unmon)):\n        sm_vector = result_Unmon[i]\n        predicted_class = np.argmax(sm_vector)\n        max_prob = max(sm_vector)\n\n        if predicted_class in monitored_label: # predicted as Monitored\n            if max_prob >= threshold_val: # predicted as Monitored and actual site is Unmonitored\n                FP = FP + 1\n            else: # predicted as Unmonitored and actual site is Unmonitored\n                TN = TN + 1\n        elif predicted_class in unmonitored_label: # predicted as Unmonitored and actual site is Unmonitored\n            TN = TN + 1\n            \n    print (\"TP : \", TP)\n    print (\"FP : \", FP)\n    print (\"TN : \", TN)\n    print (\"FN : \", FN)\n    print (\"Total  : \", TP + FP + TN + FN)\n    TPR = float(TP) / (TP + FN)\n    print (\"TPR : \", TPR)\n    FPR = float(FP) / (FP + TN)\n    print (\"FPR : \",  FPR)\n    Precision = float(TP) / (TP + FP)\n    print (\"Precision : \", Precision)\n    Recall = float(TP) / (TP + FN)\n    print (\"Recall : \", Recall)\n    print (\"\\n\")\n    log_file.writelines(\"%.6f,%d,%d,%d,%d,%.6f,%.6f,%.6f,%.6f\\n\"%(threshold_val, TP, FP, TN, FN, TPR, FPR, Precision, Recall))\n\n# The evaluation of Open World scenario\ndef OW_Evaluation():\n    evaluation_type = 'OpenWorld_WTFPAD'\n    print(\"Evaluation type: \", evaluation_type)\n\n    # Create the 'results' directory if it doesn't exist\n    results_dir = '../results/'\n    os.makedirs(results_dir, exist_ok=True)\n\n    threshold = 1.0 - 1 / np.logspace(0.05, 2, num=15, endpoint=True)\n    file_name = f'{results_dir}{evaluation_type}.csv'\n    log_file = open(file_name, \"w\")  # Open the file in text mode, not binary mode\n    # Load data\n    dataset = {}\n    model_name = ''\n    print (\"Loading data ...\")\n    #from utility import LoadDataNoDefOW_Evaluation\n    X_test_Mon, y_test_Mon, X_test_Unmon, y_test_Unmon = LoadDataWTFPADOW_Evaluation()\n    # Load pre-trained model saved from 'Open_World_DF_***_Training.py'\n    #model_name = '/kaggle/working/DFNet_OpenWorld_NoDef.keras'\n    model_name = '/kaggle/working/DFNet_OpenWorld_WTFPAD.keras'\n\n\n    dataset['X_test_Mon'] = X_test_Mon\n    dataset['y_test_Mon'] = y_test_Mon\n    dataset['X_test_Unmon'] = X_test_Unmon\n    dataset['y_test_Unmon'] = y_test_Unmon\n\n    print (\"Data loaded!\")\n    print (\"Loading DF model ...\")\n    print (\"The log file will be saved at \", file_name)\n    print (\"-- The log file will contains\")\n    print (\"-- TP, FP, TN, FN, TPR, FPR, Precision, and Recall for each different threshold\")\n    print (\"-- These results will be used to plot the ROC or Precision&Recall Graph\")\n    trained_model = load_model(model_name)\n    print (\"Model loaded!\")\n    print (\"Evaluation Type: \", evaluation_type)\n    print (\"Use the model from \", model_name)\n    result_Mon, result_Unmon = Prediction(trained_model = trained_model, dataset = dataset)\n    monitored_label = list(y_test_Mon)\n    unmonitored_label = list(y_test_Unmon)\n    log_file.writelines(\"%s,%s,%s,%s,%s,%s  ,%s  ,  %s, %s\\n\" % ('Threshold', 'TP', 'FP', 'TN', 'FN', 'TPR', 'FPR', 'Precision', 'Recall'))\n    for th in threshold:\n        Evaluation(threshold_val = th, monitored_label = monitored_label,\n                   unmonitored_label = unmonitored_label, result_Mon = result_Mon,\n                   result_Unmon = result_Unmon, log_file = log_file)\n    log_file.close()  \n\nOW_Evaluation()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T17:37:52.328983Z","iopub.execute_input":"2024-04-09T17:37:52.329332Z","iopub.status.idle":"2024-04-09T17:40:30.611249Z","shell.execute_reply.started":"2024-04-09T17:37:52.329306Z","shell.execute_reply":"2024-04-09T17:40:30.610268Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Evaluation type:  OpenWorld_WTFPAD\nLoading data ...\nLoading defended dataset for open-world scenario for evaluation\nData loaded!\nLoading DF model ...\nThe log file will be saved at  ../results/OpenWorld_WTFPAD.csv\n-- The log file will contains\n-- TP, FP, TN, FN, TPR, FPR, Precision, and Recall for each different threshold\n-- These results will be used to plot the ROC or Precision&Recall Graph\nModel loaded!\nEvaluation Type:  OpenWorld_WTFPAD\nUse the model from  /kaggle/working/DFNet_OpenWorld_WTFPAD.keras\nTotal testing data  29500\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1712684277.150353     110 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"297/297 - 2s - 8ms/step\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1712684278.954388     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"625/625 - 2s - 4ms/step\nTesting with threshold =  0.10874906186625455\nTP :  8689\nFP :  10259\nTN :  9741\nFN :  811\nTotal  :  29500\nTPR :  0.9146315789473685\nFPR :  0.51295\nPrecision :  0.45857082541693056\nRecall :  0.9146315789473685\n\n\nTesting with threshold =  0.35328298736360164\nTP :  7185\nFP :  6799\nTN :  13201\nFN :  2315\nTotal  :  29500\nTPR :  0.7563157894736842\nFPR :  0.33995\nPrecision :  0.5138014874141876\nRecall :  0.7563157894736842\n\n\nTesting with threshold =  0.5307237540651162\nTP :  5493\nFP :  3369\nTN :  16631\nFN :  4007\nTotal  :  29500\nTPR :  0.5782105263157895\nFPR :  0.16845\nPrecision :  0.6198375084631009\nRecall :  0.5782105263157895\n\n\nTesting with threshold =  0.6594798177629646\nTP :  4442\nFP :  1780\nTN :  18220\nFN :  5058\nTotal  :  29500\nTPR :  0.46757894736842104\nFPR :  0.089\nPrecision :  0.7139183542269367\nRecall :  0.46757894736842104\n\n\nTesting with threshold =  0.7529088772014396\nTP :  3691\nFP :  1048\nTN :  18952\nFN :  5809\nTotal  :  29500\nTPR :  0.38852631578947366\nFPR :  0.0524\nPrecision :  0.7788562987972146\nRecall :  0.38852631578947366\n\n\nTesting with threshold =  0.8207036582537898\nTP :  3062\nFP :  616\nTN :  19384\nFN :  6438\nTotal  :  29500\nTPR :  0.3223157894736842\nFPR :  0.0308\nPrecision :  0.8325176726481783\nRecall :  0.3223157894736842\n\n\nTesting with threshold =  0.8698974783089168\nTP :  2609\nFP :  386\nTN :  19614\nFN :  6891\nTotal  :  29500\nTPR :  0.27463157894736845\nFPR :  0.0193\nPrecision :  0.8711185308848081\nRecall :  0.27463157894736845\n\n\nTesting with threshold =  0.9055939123714077\nTP :  2208\nFP :  223\nTN :  19777\nFN :  7292\nTotal  :  29500\nTPR :  0.23242105263157894\nFPR :  0.01115\nPrecision :  0.9082682023858495\nRecall :  0.23242105263157894\n\n\nTesting with threshold =  0.9314962595229367\nTP :  1874\nFP :  133\nTN :  19867\nFN :  7626\nTotal  :  29500\nTPR :  0.19726315789473683\nFPR :  0.00665\nPrecision :  0.9337319382162431\nRecall :  0.19726315789473683\n\n\nTesting with threshold =  0.9502917388356261\nTP :  1562\nFP :  68\nTN :  19932\nFN :  7938\nTotal  :  29500\nTPR :  0.16442105263157894\nFPR :  0.0034\nPrecision :  0.9582822085889571\nRecall :  0.16442105263157894\n\n\nTesting with threshold =  0.963930272846737\nTP :  1271\nFP :  42\nTN :  19958\nFN :  8229\nTotal  :  29500\nTPR :  0.1337894736842105\nFPR :  0.0021\nPrecision :  0.968012185833968\nRecall :  0.1337894736842105\n\n\nTesting with threshold =  0.9738267807717386\nTP :  1021\nFP :  27\nTN :  19973\nFN :  8479\nTotal  :  29500\nTPR :  0.10747368421052632\nFPR :  0.00135\nPrecision :  0.9742366412213741\nRecall :  0.10747368421052632\n\n\nTesting with threshold =  0.9810079682094667\nTP :  826\nFP :  15\nTN :  19985\nFN :  8674\nTotal  :  29500\nTPR :  0.08694736842105263\nFPR :  0.00075\nPrecision :  0.9821640903686087\nRecall :  0.08694736842105263\n\n\nTesting with threshold =  0.9862188419243761\nTP :  641\nFP :  9\nTN :  19991\nFN :  8859\nTotal  :  29500\nTPR :  0.06747368421052631\nFPR :  0.00045\nPrecision :  0.9861538461538462\nRecall :  0.06747368421052631\n\n\nTesting with threshold =  0.99\nTP :  502\nFP :  4\nTN :  19996\nFN :  8998\nTotal  :  29500\nTPR :  0.05284210526315789\nFPR :  0.0002\nPrecision :  0.9920948616600791\nRecall :  0.05284210526315789\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**FOR defended WalkieTalkie open world**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport pickle\nimport numpy as np  # Add this line\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, BatchNormalization\nfrom keras.layers import Activation, Flatten, Dense, Dropout, ReLU\nfrom keras.initializers import glorot_uniform\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom keras import layers, models\n\n# Load data for non-defended dataset for OW training\ndef LoadDataWalkieTalkie_Training():\n\n    print(\"Loading non-defended dataset for open-world scenario for training\")\n    # Point to the directory storing data\n    dataset_dir = '/kaggle/input/defended-open-world-walkietalkie-dataset/'\n\n    # X represents a sequence of traffic directions\n    # y represents a sequence of corresponding label (website's label)\n\n    try:\n        # Load training data\n        with open(dataset_dir + 'X_train_WalkieTalkie.pkl', 'rb') as handle:\n            X_train = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"X_train loaded\")\n        with open(dataset_dir + 'y_train_WalkieTalkie.pkl', 'rb') as handle:\n            y_train = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"y_train loaded\")\n\n        # Load validation data\n        with open(dataset_dir + 'X_valid_WalkieTalkie.pkl', 'rb') as handle:\n            X_valid = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"X_valid loaded\")\n        with open(dataset_dir + 'y_valid_WalkieTalkie.pkl', 'rb') as handle:\n            y_valid = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"y_valid loaded\")\n\n        print(\"Data dimensions:\")\n        print(\"X: Training data's shape : \", X_train.shape)\n        print(\"y: Training data's shape : \", y_train.shape)\n        print(\"X: Validation data's shape : \", X_valid.shape)\n        print(\"y: Validation data's shape : \", y_valid.shape)\n        \n        print(\"Any None values in X_train:\", np.any(np.isnan(X_train)))\n        print(\"Any None values in y_train:\", np.any(np.isnan(y_train)))\n        print(\"Any None values in X_valid:\", np.any(np.isnan(X_valid)))\n        print(\"Any None values in y_valid:\", np.any(np.isnan(y_valid)))\n        \n        # Merge datasets\n        X_all = np.concatenate((X_train, X_valid), axis=0)\n        y_all = np.concatenate((y_train, y_valid), axis=0)\n\n        print(\"Merged data dimensions:\")\n        print(\"X: Merged data's shape : \", X_all.shape)\n        print(\"y: Merged data's shape : \", y_all.shape)\n        \n        # Print features of the merged dataset\n        print(\"Features of the merged dataset:\")\n        print(X_all)\n         # Print features of the merged dataset\n        print(\"Features of the merged dataset:\")\n        print(y_all)\n        \n       \n        return X_all, y_all\n\n\n    except Exception as e:\n        print(\"An error occurred:\", str(e))\n        return None\n\n# Call the function to load, merge, and balance data\nX_all, y_all = LoadDataWalkieTalkie_Training()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T08:01:25.072388Z","iopub.execute_input":"2024-04-15T08:01:25.073156Z","iopub.status.idle":"2024-04-15T08:02:38.077134Z","shell.execute_reply.started":"2024-04-15T08:01:25.073104Z","shell.execute_reply":"2024-04-15T08:02:38.076077Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Loading non-defended dataset for open-world scenario for training\nX_train loaded\ny_train loaded\nX_valid loaded\ny_valid loaded\nData dimensions:\nX: Training data's shape :  (99000, 5000)\ny: Training data's shape :  (99000,)\nX: Validation data's shape :  (2000, 5000)\ny: Validation data's shape :  (2000,)\nAny None values in X_train: False\nAny None values in y_train: False\nAny None values in X_valid: False\nAny None values in y_valid: False\nMerged data dimensions:\nX: Merged data's shape :  (101000, 5000)\ny: Merged data's shape :  (101000,)\nFeatures of the merged dataset:\n[[ 1  1 -1 ... -1 -1 -1]\n [ 1 -1 -1 ... -1 -1 -1]\n [ 1 -1  1 ... -1 -1 -1]\n ...\n [ 1 -1  1 ...  0  0  0]\n [ 1  1  1 ... -1 -1 -1]\n [ 1 -1  1 ... -1 -1 -1]]\nFeatures of the merged dataset:\n[ 82  88  32 ... 100  40  53]\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport pickle\nimport numpy as np  # Add this line\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, BatchNormalization\nfrom keras.layers import Activation, Flatten, Dense, Dropout,ReLU\nfrom keras.initializers import glorot_uniform\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom keras import layers, models\n\n# Load data for non-defended dataset for OW training\ndef LoadDataWalkieTalkie_Training():\n\n    print(\"Loading defended dataset for open-world scenario for training\")\n    # Point to the directory storing data\n    dataset_dir = '/kaggle/input/defended-open-world-walkietalkie-dataset/'\n\n    # X represents a sequence of traffic directions\n    # y represents a sequence of corresponding label (website's label)\n\n    try:\n        # Load training data\n        with open(dataset_dir + 'X_train_WalkieTalkie.pkl', 'rb') as handle:\n            X_train = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"X_train loaded\")\n        with open(dataset_dir + 'y_train_WalkieTalkie.pkl', 'rb') as handle:\n            y_train = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"y_train loaded\")\n\n        # Load validation data\n        with open(dataset_dir + 'X_valid_WalkieTalkie.pkl', 'rb') as handle:\n            X_valid = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"X_valid loaded\")\n        with open(dataset_dir + 'y_valid_WalkieTalkie.pkl', 'rb') as handle:\n            y_valid = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"y_valid loaded\")\n\n        print(\"Data dimensions:\")\n        print(\"X: Training data's shape : \", X_train.shape)\n        print(\"y: Training data's shape : \", y_train.shape)\n        print(\"X: Validation data's shape : \", X_valid.shape)\n        print(\"y: Validation data's shape : \", y_valid.shape)\n        \n        print(\"Any None values in X_train:\", np.any(np.isnan(X_train)))\n        print(\"Any None values in y_train:\", np.any(np.isnan(y_train)))\n        print(\"Any None values in X_valid:\", np.any(np.isnan(X_valid)))\n        print(\"Any None values in y_valid:\", np.any(np.isnan(y_valid)))\n        \n        # Merge datasets\n        X_all = np.concatenate((X_train, X_valid), axis=0)\n        y_all = np.concatenate((y_train, y_valid), axis=0)\n\n        print(\"Merged data dimensions:\")\n        print(\"X: Merged data's shape : \", X_all.shape)\n        print(\"y: Merged data's shape : \", y_all.shape)\n        \n        # Print features of the merged dataset\n        print(\"Features of the merged dataset:\")\n        print(X_all)\n         # Print features of the merged dataset\n        print(\"Features of the merged dataset:\")\n        print(y_all)\n        \n       \n        return X_all, y_all\n\n\n    except Exception as e:\n        print(\"An error occurred:\", str(e))\n        return None\n\n# Call the function to load, merge, and balance data\nX_all, y_all = LoadDataWalkieTalkie_Training()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T08:05:05.073071Z","iopub.execute_input":"2024-04-15T08:05:05.073533Z","iopub.status.idle":"2024-04-15T08:06:17.931507Z","shell.execute_reply.started":"2024-04-15T08:05:05.073500Z","shell.execute_reply":"2024-04-15T08:06:17.930620Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Loading defended dataset for open-world scenario for training\nX_train loaded\ny_train loaded\nX_valid loaded\ny_valid loaded\nData dimensions:\nX: Training data's shape :  (99000, 5000)\ny: Training data's shape :  (99000,)\nX: Validation data's shape :  (2000, 5000)\ny: Validation data's shape :  (2000,)\nAny None values in X_train: False\nAny None values in y_train: False\nAny None values in X_valid: False\nAny None values in y_valid: False\nMerged data dimensions:\nX: Merged data's shape :  (101000, 5000)\ny: Merged data's shape :  (101000,)\nFeatures of the merged dataset:\n[[ 1  1 -1 ... -1 -1 -1]\n [ 1 -1 -1 ... -1 -1 -1]\n [ 1 -1  1 ... -1 -1 -1]\n ...\n [ 1 -1  1 ...  0  0  0]\n [ 1  1  1 ... -1 -1 -1]\n [ 1 -1  1 ... -1 -1 -1]]\nFeatures of the merged dataset:\n[ 82  88  32 ... 100  40  53]\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, BatchNormalization\nfrom keras.layers import Activation, Flatten, Dense, Dropout, ReLU\nfrom keras.initializers import glorot_uniform\nfrom keras.optimizers import Adamax\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom keras.models import save_model\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom keras import layers, models\nimport numpy as np\nfrom keras.layers import Activation\n\nclass DFNet:\n    @staticmethod\n    def build(input_shape, classes, dropout_rate1=0.3, dropout_rate2=0.5, dropout_rate_fc=0.7):\n        model = Sequential()\n\n        # ... (previous model definition remains unchanged)\n        # Block 1\n        filter_num = [None,32, 64, 128, 256]\n        kernel_size = [None, 8, 8, 8, 8]\n        conv_stride_size = [None, 1, 1, 1, 1]\n        pool_stride_size = [None, 4, 4, 4, 4]\n        pool_size = [None, 8, 8, 8, 8]\n\n        for i in range(1, 4):\n            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n                             strides=conv_stride_size[i], padding='same',\n                             input_shape=input_shape if i == 1 else (None, input_shape[1]),\n                             name=f'block{i}_conv1'))\n            model.add(BatchNormalization(name=f'batch_normalization_{i * 2 - 1}'))\n            model.add(Activation('relu', name=f'block{i}_adv_act1'))\n            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n                             strides=conv_stride_size[i], padding='same',\n                             name=f'block{i}_conv2'))\n            model.add(BatchNormalization(name=f'batch_normalization_{i * 2}'))\n            model.add(Activation('relu', name=f'block{i}_adv_act2'))\n\n            model.add(MaxPooling1D(pool_size=pool_size[i], strides=pool_stride_size[i],\n                                   padding='same', name=f'block{i}_pool'))\n            model.add(Dropout(0.1, name=f'block{i}_dropout'))\n\n        # ... (rest of the model remains unchanged)\n\n        model.add(Flatten(name='flatten'))\n\n        # Fully connected layers\n        for i in range(1, 3):\n            model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name=f'fc{i}'))\n            model.add(BatchNormalization(name=f'batch_normalization_{i + 8}'))\n            model.add(Activation('relu', name=f'fc{i}_act'))\n\n            # Experiment with different dropout rates\n            model.add(Dropout(dropout_rate_fc, name=f'fc{i}_dropout'))\n\n        # Output layer\n        model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='fc_final'))\n        model.add(Activation('softmax', name=\"softmax\"))\n\n        print(\"Model built successfully.\")\n        return model\n    # Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\ninput_shape = (5000, 1)\nnum_classes = 96\n\n# Experiment with different dropout rates\ndropout_rate1 = 0.6\ndropout_rate2 = 0.5\ndropout_rate_fc = 0.4\n\n# Build the model with the specified dropout rates\nmodel = DFNet.build(input_shape=input_shape, classes=num_classes,\n                    dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n\n\n# Print the model summary\nprint(\"Model Summary:\")\nmodel.summary()\n\n# Compile the model\noptimizer = Adamax(learning_rate=0.002)\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n\n# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\ninput_shape = (5000, 1)\nnum_classes = 96\n\n# Experiment with different dropout rates\ndropout_rate1 = 0.6\ndropout_rate2 = 0.5\ndropout_rate_fc = 0.4\n\n# Load and merge data using the function you defined\nX_all, y_all = LoadDataWalkieTalkie_Training()\n# Define the number of folds\nn_splits = 5  # You can adjust this as needed\n\n# Initialize Stratified K-Fold\nstratkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Lists to store results\nall_histories = []\n\n# Iterate over folds\nfor fold, (train_index, test_index) in enumerate(stratkf.split(X_all, y_all)):\n    print(f\"\\nTraining on Fold {fold + 1}/{n_splits}\")\n\n    # Split the data\n    X_train, X_test = X_all[train_index], X_all[test_index]\n    y_train, y_test = y_all[train_index], y_all[test_index]\n    \n        # Build the model\n    model = DFNet.build(input_shape=input_shape, classes=num_classes,\n                        dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n\n    # Compile the model\n    optimizer = Adamax(learning_rate=0.002)\n    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n     # Define early stopping callback\n    early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n\n    # Train the model with early stopping\n    history = model.fit(X_train, y_train, epochs= 25, batch_size=64, \n                        validation_data=(X_test, y_test), verbose=2, callbacks=[early_stopping])\n    \n    # Train the model\n    #history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n\n    # Evaluate the model on the test set\n    loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n    print(f\"\\nFold {fold + 1} - Test Accuracy: {accuracy * 100:.2f}%\")\n\n    # Save history for later analysis if needed\n    all_histories.append(history)\n    # Check unique labels in y_train\n    \n    # Update num_classes based on the actual number of unique classes in your dataset\n#num_classes = len(np.unique(y_all))\nmodel_save_path = '/kaggle/working/DFNet_OpenWorld_WALKIETALKIE.keras'\nsave_model(model,model_save_path)\n\nprint(\"Trained model saved successfully at:\", model_save_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T08:06:17.932598Z","iopub.execute_input":"2024-04-15T08:06:17.932867Z","iopub.status.idle":"2024-04-15T11:33:30.636175Z","shell.execute_reply.started":"2024-04-15T08:06:17.932845Z","shell.execute_reply":"2024-04-15T11:33:30.635185Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Model built successfully.\nModel Summary:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_3\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ block1_conv1 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m288\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act1 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_conv2 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m8,224\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act2 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_pool (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv1 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m16,448\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act1 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv2 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m32,832\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act2 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_pool (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv1 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m65,664\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act1 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv2 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m131,200\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act2 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_pool (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10112\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m5,177,856\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_act (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_dropout (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_act (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_dropout (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc_final (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │        \u001b[38;5;34m49,248\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ softmax (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ block1_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,224</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10112</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,177,856</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_act (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_act (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc_final (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,248</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,750,304\u001b[0m (21.94 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,750,304</span> (21.94 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,747,360\u001b[0m (21.92 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,747,360</span> (21.92 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,944\u001b[0m (11.50 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,944</span> (11.50 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Loading defended dataset for open-world scenario for training\nX_train loaded\ny_train loaded\nX_valid loaded\ny_valid loaded\nData dimensions:\nX: Training data's shape :  (99000, 5000)\ny: Training data's shape :  (99000,)\nX: Validation data's shape :  (2000, 5000)\ny: Validation data's shape :  (2000,)\nAny None values in X_train: False\nAny None values in y_train: False\nAny None values in X_valid: False\nAny None values in y_valid: False\nMerged data dimensions:\nX: Merged data's shape :  (101000, 5000)\ny: Merged data's shape :  (101000,)\nFeatures of the merged dataset:\n[[ 1  1 -1 ... -1 -1 -1]\n [ 1 -1 -1 ... -1 -1 -1]\n [ 1 -1  1 ... -1 -1 -1]\n ...\n [ 1 -1  1 ...  0  0  0]\n [ 1  1  1 ... -1 -1 -1]\n [ 1 -1  1 ... -1 -1 -1]]\nFeatures of the merged dataset:\n[ 82  88  32 ... 100  40  53]\n\nTraining on Fold 1/5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Model built successfully.\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1713168485.589422     199 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1713168485.615563     199 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1713168592.549493     202 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1713168595.443508     201 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1263/1263 - 139s - 110ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 2/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713168599.716981     199 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 3/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 4/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 5/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 6/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 7/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 8/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 9/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 10/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 11/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 12/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 13/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 14/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 15/25\n1263/1263 - 142s - 112ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 16/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 17/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 18/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 19/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 20/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 21/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 22/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 23/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 24/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 25/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713170933.756620     200 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"632/632 - 5s - 7ms/step - accuracy: 0.0080 - loss: nan\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713170937.782893     201 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\nFold 1 - Test Accuracy: 0.80%\n\nTraining on Fold 2/5\nModel built successfully.\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713170960.385220     202 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1713171062.347481     199 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1713171065.039131     200 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1263/1263 - 121s - 96ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 2/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713171067.944646     201 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 3/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 4/25\n1263/1263 - 142s - 112ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 5/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 6/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 7/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 8/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 9/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 10/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 11/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 12/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 13/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 15/25\n1263/1263 - 142s - 113ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 16/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 17/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 18/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 19/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 20/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 21/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 22/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 23/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 24/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 25/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713173446.952130     200 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"632/632 - 4s - 6ms/step - accuracy: 0.0080 - loss: nan\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713173450.207578     199 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\nFold 2 - Test Accuracy: 0.80%\n\nTraining on Fold 3/5\nModel built successfully.\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713173473.246275     202 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1713173575.804054     202 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1713173578.528359     199 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1263/1263 - 122s - 97ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 2/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713173581.440620     200 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 3/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 4/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 5/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 6/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 7/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 8/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 9/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 10/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 11/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 12/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 13/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 14/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 15/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 16/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 17/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 18/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 19/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 20/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 21/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 22/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 23/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 24/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 25/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713175868.530578     202 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"632/632 - 4s - 6ms/step - accuracy: 0.0080 - loss: nan\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713175871.734266     200 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\nFold 3 - Test Accuracy: 0.80%\n\nTraining on Fold 4/5\nModel built successfully.\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713175894.676360     199 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1713175997.116211     202 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1713175999.805790     200 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1263/1263 - 122s - 97ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 2/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713176002.765986     202 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 3/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 4/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 5/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 6/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 7/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 8/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 9/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 10/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 11/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 12/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 13/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 14/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 15/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 16/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 17/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 18/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 19/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 20/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 21/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 22/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 23/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 24/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 25/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713178288.773698     201 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"632/632 - 4s - 6ms/step - accuracy: 0.0080 - loss: nan\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713178291.977251     200 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\nFold 4 - Test Accuracy: 0.80%\n\nTraining on Fold 5/5\nModel built successfully.\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713178315.608024     199 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1713178417.305489     201 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1713178420.004652     199 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1263/1263 - 122s - 97ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 2/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713178422.997283     201 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 3/25\n1263/1263 - 95s - 76ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 4/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 5/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 6/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 7/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 8/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 9/25\n1263/1263 - 95s - 76ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 10/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 11/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 12/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 13/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 14/25\n1263/1263 - 95s - 76ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 15/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 16/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 17/25\n1263/1263 - 95s - 76ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 18/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 19/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 20/25\n1263/1263 - 142s - 113ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 21/25\n1263/1263 - 95s - 76ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 22/25\n1263/1263 - 142s - 112ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 23/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 24/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\nEpoch 25/25\n1263/1263 - 95s - 75ms/step - accuracy: 0.0080 - loss: nan - val_accuracy: 0.0080 - val_loss: nan\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713180806.952930     199 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"632/632 - 4s - 6ms/step - accuracy: 0.0080 - loss: nan\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713180810.245735     199 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\nFold 5 - Test Accuracy: 0.80%\nTrained model saved successfully at: /kaggle/working/DFNet_OpenWorld_WALKIETALKIE.keras\n","output_type":"stream"}]},{"cell_type":"code","source":" #Save the model to the Kaggle output directory\nmodel_save_path = '/kaggle/working/DFNet_OpenWorld_WTFPAD.keras'\nmodel.save(model_save_path)\nprint(\"Trained model saved successfully at:\", model_save_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:13:47.002387Z","iopub.execute_input":"2024-04-15T12:13:47.003298Z","iopub.status.idle":"2024-04-15T12:13:47.302513Z","shell.execute_reply.started":"2024-04-15T12:13:47.003264Z","shell.execute_reply":"2024-04-15T12:13:47.301544Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Trained model saved successfully at: /kaggle/working/DFNet_OpenWorld_WTFPAD.keras\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n# Define the directory for saving the model\nexpected_directory = '/kaggle/working/'\nexpected_filename = 'DFNet_OpenWorld_WTFPAD.keras'\n\n# Construct the full path\nsaved_path_keras = os.path.join(expected_directory, expected_filename)\n\n# Check if the directory exists\nif not os.path.exists(expected_directory):\n    print(\"Error: Directory does not exist:\", expected_directory)\nelse:\n    # Check if the model was saved successfully\n    try:\n        # Your code for saving the model goes here\n        # For example:\n        # model.save(saved_path_keras)\n        # If an error occurs during the saving process, it will be caught here\n        print(\"Model saved successfully.\")\n    except Exception as e:\n        print(\"Error occurred while saving the model:\", e)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:14:23.237170Z","iopub.execute_input":"2024-04-15T12:14:23.237828Z","iopub.status.idle":"2024-04-15T12:14:23.244303Z","shell.execute_reply.started":"2024-04-15T12:14:23.237795Z","shell.execute_reply":"2024-04-15T12:14:23.243275Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Model saved successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load data for non-defended dataset for OW evaluation\nimport pickle\nimport numpy as np\ndef  LoadDataWalkieTalkie_Evaluation():\n\n    print(\"Loading defended dataset for open-world scenario for evaluation\")\n    # Point to the directory storing data\n    dataset_dir = '/kaggle/input/defended-open-world-walkietalkie-dataset/'\n\n    # X represents a sequence of traffic directions\n    # y represents a sequence of corresponding label (website's label)\n    try:\n        # Load testing data\n        with open(dataset_dir + 'X_test_Mon_WalkieTalkie.pkl', 'rb') as handle:\n            X_test_Mon = np.array(pickle.load(handle, encoding='latin1'))\n        with open(dataset_dir + 'y_test_Mon_WalkieTalkie.pkl', 'rb') as handle:\n            y_test_Mon = np.array(pickle.load(handle, encoding='latin1'))\n        with open(dataset_dir + 'X_test_Unmon_WalkieTalkie.pkl', 'rb') as handle:\n            X_test_Unmon = np.array(pickle.load(handle, encoding='latin1'))\n        with open(dataset_dir + 'y_test_Unmon_WalkieTalkie.pkl', 'rb') as handle:\n            y_test_Unmon = np.array(pickle.load(handle, encoding='latin1'))\n\n        X_test_Mon = np.array(X_test_Mon)\n        y_test_Mon = np.array(y_test_Mon)\n        X_test_Unmon = np.array(X_test_Unmon)\n        y_test_Unmon = np.array(y_test_Unmon)\n\n        return X_test_Mon, y_test_Mon, X_test_Unmon, y_test_Unmon\n\n    except Exception as e:\n        print(\"An error occurred:\", str(e))\n        return None\n\n# Call the function to load evaluation data\nLoadDataWalkieTalkie_Evaluation()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:20:36.378918Z","iopub.execute_input":"2024-04-15T12:20:36.379270Z","iopub.status.idle":"2024-04-15T12:21:01.555429Z","shell.execute_reply.started":"2024-04-15T12:20:36.379243Z","shell.execute_reply":"2024-04-15T12:21:01.554486Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Loading defended dataset for open-world scenario for evaluation\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"(array([[ 1, -1,  1, ..., -1, -1, -1],\n        [ 1,  1, -1, ..., -1, -1, -1],\n        [ 1,  1, -1, ..., -1, -1, -1],\n        ...,\n        [ 1,  1, -1, ...,  0,  0,  0],\n        [ 1,  1, -1, ..., -1,  1, -1],\n        [ 1, -1,  1, ..., -1, -1, -1]]),\n array([ 0,  0,  0, ..., 99, 99, 99]),\n array([[ 1, -1,  1, ..., -1, -1, -1],\n        [ 1, -1,  1, ...,  0,  0,  0],\n        [ 1,  1,  1, ..., -1, -1, -1],\n        ...,\n        [ 1, -1,  1, ..., -1, -1, -1],\n        [ 1,  1, -1, ...,  0,  0,  0],\n        [ 1, -1,  1, ..., -1, -1, -1]]),\n array([100, 100, 100, ..., 100, 100, 100]))"},"metadata":{}}]},{"cell_type":"code","source":"from keras.models import load_model\nimport numpy as np\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\ndef Prediction(trained_model = None, dataset = None):\n    X_test_Mon = dataset['X_test_Mon'].astype('float32')\n    X_test_Unmon = dataset['X_test_Unmon'].astype('float32')\n    print (\"Total testing data \", len(X_test_Mon) + len(X_test_Unmon))\n    X_test_Mon = X_test_Mon[:, :, np.newaxis]\n    X_test_Unmon = X_test_Unmon[:, :, np.newaxis]\n    result_Mon = trained_model.predict(X_test_Mon, verbose=2)\n    result_Unmon = trained_model.predict(X_test_Unmon, verbose=2)\n    return result_Mon, result_Unmon\ndef Evaluation(threshold_val = None, monitored_label = None,\n                   unmonitored_label = None, result_Mon = None,\n                   result_Unmon = None, log_file = None):\n    print (\"Testing with threshold = \", threshold_val)\n    TP = 0\n    FP = 0\n    TN = 0\n    FN = 0\n\n    # ==============================================================\n    # Test with Monitored testing instances\n    # evaluation\n    for i in range(len(result_Mon)):\n        sm_vector = result_Mon[i]\n        predicted_class = np.argmax(sm_vector)\n        max_prob = max(sm_vector)\n\n        if predicted_class in monitored_label: # predicted as Monitored\n            if max_prob >= threshold_val: # predicted as Monitored and actual site is Monitored\n                TP = TP + 1\n            else: # predicted as Unmonitored and actual site is Monitored\n                FN = FN + 1\n        elif predicted_class in unmonitored_label: # predicted as Unmonitored and actual site is Monitored\n            FN = FN + 1\n\n    # ==============================================================\n    # Test with Unmonitored testing instances\n    # evaluation\n    for i in range(len(result_Unmon)):\n        sm_vector = result_Unmon[i]\n        predicted_class = np.argmax(sm_vector)\n        max_prob = max(sm_vector)\n\n        if predicted_class in monitored_label: # predicted as Monitored\n            if max_prob >= threshold_val: # predicted as Monitored and actual site is Unmonitored\n                FP = FP + 1\n            else: # predicted as Unmonitored and actual site is Unmonitored\n                TN = TN + 1\n        elif predicted_class in unmonitored_label: # predicted as Unmonitored and actual site is Unmonitored\n            TN = TN + 1\n            \n    print (\"TP : \", TP)\n    print (\"FP : \", FP)\n    print (\"TN : \", TN)\n    print (\"FN : \", FN)\n    print (\"Total  : \", TP + FP + TN + FN)\n    TPR = float(TP) / (TP + FN)\n    print (\"TPR : \", TPR)\n    FPR = float(FP) / (FP + TN)\n    print (\"FPR : \",  FPR)\n    Precision = float(TP) / (TP + FP)\n    print (\"Precision : \", Precision)\n    Recall = float(TP) / (TP + FN)\n    print (\"Recall : \", Recall)\n    print (\"\\n\")\n    log_file.writelines(\"%.6f,%d,%d,%d,%d,%.6f,%.6f,%.6f,%.6f\\n\"%(threshold_val, TP, FP, TN, FN, TPR, FPR, Precision, Recall))\n\n# The evaluation of Open World scenario\ndef OW_Evaluation():\n    evaluation_type = 'OpenWorld_WalkieTalkie'\n    print(\"Evaluation type: \", evaluation_type)\n\n    # Create the 'results' directory if it doesn't exist\n    results_dir = '../results/'\n    os.makedirs(results_dir, exist_ok=True)\n\n    threshold = 1.0 - 1 / np.logspace(0.05, 2, num=15, endpoint=True)\n    file_name = f'{results_dir}{evaluation_type}.csv'\n    log_file = open(file_name, \"w\")  # Open the file in text mode, not binary mode\n    # Load data\n    dataset = {}\n    model_name = ''\n    print (\"Loading data ...\")\n    #from utility import LoadDataNoDefOW_Evaluation\n    X_test_Mon, y_test_Mon, X_test_Unmon, y_test_Unmon = LoadDataWalkieTalkie_Evaluation()\n    # Load pre-trained model saved from 'Open_World_DF_***_Training.py'\n    #model_name = '/kaggle/working/DFNet_OpenWorld_NoDef.keras'\n    model_name = '/kaggle/working/DFNet_OpenWorld_WALKIETALKIE.keras'\n\n\n    dataset['X_test_Mon'] = X_test_Mon\n    dataset['y_test_Mon'] = y_test_Mon\n    dataset['X_test_Unmon'] = X_test_Unmon\n    dataset['y_test_Unmon'] = y_test_Unmon\n\n    print (\"Data loaded!\")\n    print (\"Loading DF model ...\")\n    print (\"The log file will be saved at \", file_name)\n    print (\"-- The log file will contains\")\n    print (\"-- TP, FP, TN, FN, TPR, FPR, Precision, and Recall for each different threshold\")\n    print (\"-- These results will be used to plot the ROC or Precision&Recall Graph\")\n    trained_model = load_model(model_name)\n    print (\"Model loaded!\")\n    print (\"Evaluation Type: \", evaluation_type)\n    print (\"Use the model from \", model_name)\n    result_Mon, result_Unmon = Prediction(trained_model = trained_model, dataset = dataset)\n    monitored_label = list(y_test_Mon)\n    unmonitored_label = list(y_test_Unmon)\n    log_file.writelines(\"%s,%s,%s,%s,%s,%s  ,%s  ,  %s, %s\\n\" % ('Threshold', 'TP', 'FP', 'TN', 'FN', 'TPR', 'FPR', 'Precision', 'Recall'))\n    for th in threshold:\n        Evaluation(threshold_val = th, monitored_label = monitored_label,\n                   unmonitored_label = unmonitored_label, result_Mon = result_Mon,\n                   result_Unmon = result_Unmon, log_file = log_file)\n    log_file.close()  \n\nOW_Evaluation()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:21:43.087845Z","iopub.execute_input":"2024-04-15T12:21:43.088254Z","iopub.status.idle":"2024-04-15T12:22:12.581229Z","shell.execute_reply.started":"2024-04-15T12:21:43.088223Z","shell.execute_reply":"2024-04-15T12:22:12.579965Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Evaluation type:  OpenWorld_WalkieTalkie\nLoading data ...\nLoading defended dataset for open-world scenario for evaluation\nData loaded!\nLoading DF model ...\nThe log file will be saved at  ../results/OpenWorld_WalkieTalkie.csv\n-- The log file will contains\n-- TP, FP, TN, FN, TPR, FPR, Precision, and Recall for each different threshold\n-- These results will be used to plot the ROC or Precision&Recall Graph\nModel loaded!\nEvaluation Type:  OpenWorld_WalkieTalkie\nUse the model from  /kaggle/working/DFNet_OpenWorld_WALKIETALKIE.keras\nTotal testing data  29000\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713183726.597534     202 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"282/282 - 3s - 9ms/step\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713183728.376683     201 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"625/625 - 2s - 3ms/step\nTesting with threshold =  0.10874906186625455\nTP :  0\nFP :  0\nTN :  20000\nFN :  9000\nTotal  :  29000\nTPR :  0.0\nFPR :  0.0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 120\u001b[0m\n\u001b[1;32m    115\u001b[0m         Evaluation(threshold_val \u001b[38;5;241m=\u001b[39m th, monitored_label \u001b[38;5;241m=\u001b[39m monitored_label,\n\u001b[1;32m    116\u001b[0m                    unmonitored_label \u001b[38;5;241m=\u001b[39m unmonitored_label, result_Mon \u001b[38;5;241m=\u001b[39m result_Mon,\n\u001b[1;32m    117\u001b[0m                    result_Unmon \u001b[38;5;241m=\u001b[39m result_Unmon, log_file \u001b[38;5;241m=\u001b[39m log_file)\n\u001b[1;32m    118\u001b[0m     log_file\u001b[38;5;241m.\u001b[39mclose()  \n\u001b[0;32m--> 120\u001b[0m \u001b[43mOW_Evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[22], line 115\u001b[0m, in \u001b[0;36mOW_Evaluation\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m log_file\u001b[38;5;241m.\u001b[39mwritelines(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m  ,\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m  ,  \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThreshold\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTPR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFPR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m th \u001b[38;5;129;01min\u001b[39;00m threshold:\n\u001b[0;32m--> 115\u001b[0m     \u001b[43mEvaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthreshold_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitored_label\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmonitored_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m               \u001b[49m\u001b[43munmonitored_label\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43munmonitored_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_Mon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresult_Mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m               \u001b[49m\u001b[43mresult_Unmon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresult_Unmon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlog_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m log_file\u001b[38;5;241m.\u001b[39mclose()\n","Cell \u001b[0;32mIn[22], line 65\u001b[0m, in \u001b[0;36mEvaluation\u001b[0;34m(threshold_val, monitored_label, unmonitored_label, result_Mon, result_Unmon, log_file)\u001b[0m\n\u001b[1;32m     63\u001b[0m FPR \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(FP) \u001b[38;5;241m/\u001b[39m (FP \u001b[38;5;241m+\u001b[39m TN)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFPR : \u001b[39m\u001b[38;5;124m\"\u001b[39m,  FPR)\n\u001b[0;32m---> 65\u001b[0m Precision \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTP\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mTP\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision : \u001b[39m\u001b[38;5;124m\"\u001b[39m, Precision)\n\u001b[1;32m     67\u001b[0m Recall \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(TP) \u001b[38;5;241m/\u001b[39m (TP \u001b[38;5;241m+\u001b[39m FN)\n","\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"],"ename":"ZeroDivisionError","evalue":"float division by zero","output_type":"error"}]}]}