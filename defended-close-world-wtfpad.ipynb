{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7762823,"sourceType":"datasetVersion","datasetId":4540211},{"sourceId":7942534,"sourceType":"datasetVersion","datasetId":4669794}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, BatchNormalization\nfrom keras.layers import Activation, Flatten, Dense, Dropout, ReLU\nfrom keras.initializers import glorot_uniform\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom keras import layers, models\n\ndef LoadDataWTFPADCW():\n\n    print(\"Loading defended dataset for closed-world scenario\")\n\n    # Point to the directory storing data\n    dataset_dir = '/kaggle/input/dataset-defended-close-world/'\n    # X represents a sequence of traffic directions\n    # y represents a sequence of corresponding label (website's label)\n    # Debug: Print dataset directory\n    print(\"Dataset directory:\", dataset_dir)\n\n    try:\n        # Load training data\n        with open(dataset_dir + 'X_train_WTFPAD.pkl', 'rb') as handle:\n            X_train = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"X_train loaded\")\n\n        with open(dataset_dir + 'y_train_WTFPAD.pkl', 'rb') as handle:\n            y_train = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"y_train loaded\")\n\n        # Load validation data\n        with open(dataset_dir + 'X_valid_WTFPAD.pkl', 'rb') as handle:\n            X_valid = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"X_valid loaded\")\n\n        with open(dataset_dir + 'y_valid_WTFPAD.pkl', 'rb') as handle:\n            y_valid = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"y_valid loaded\")\n\n        # Load testing data\n        with open(dataset_dir + 'X_test_WTFPAD.pkl', 'rb') as handle:\n            X_test = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"X_test loaded\")\n\n        with open(dataset_dir + 'y_test_WTFPAD.pkl', 'rb') as handle:\n            y_test = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"y_test loaded\")\n\n        print(\"Data dimensions:\")\n        print(\"X: Training data's shape : \", X_train.shape)\n        print(\"y: Training data's shape : \", y_train.shape)\n        print(\"X: Validation data's shape : \", X_valid.shape)\n        print(\"y: Validation data's shape : \", y_valid.shape)\n        print(\"X: Testing data's shape : \", X_test.shape)\n        print(\"y: Testing data's shape : \", y_test.shape)\n\n        # Merge datasets\n        X_all = np.concatenate((X_train, X_valid, X_test), axis=0)\n        y_all = np.concatenate((y_train, y_valid, y_test), axis=0)\n\n        print(\"Merged data dimensions:\")\n        print(\"X: Merged data's shape : \", X_all.shape)\n        print(\"y: Merged data's shape : \", y_all.shape)\n        \n        # Print features of the merged dataset\n        print(\"Features of the merged dataset:\")\n        print(X_all)\n        \n        # Check if the class distribution is balanced\n        unique_classes, class_counts = np.unique(y_all, return_counts=True)\n        class_distribution = dict(zip(unique_classes, class_counts))\n\n        print(\"Class distribution:\")\n        for class_label, count in class_distribution.items():\n            print(f\"Class {class_label}: {count} samples\")\n\n        # Plot the class distribution\n       # plt.bar(class_distribution.keys(), class_distribution.values())\n       # plt.xlabel('Class Label')\n       # plt.ylabel('Number of Samples')\n      #  plt.title('Class Distribution')\n      #  plt.show()\n        \n        return X_all, y_all\n\n    except Exception as e:\n        print(\"An error occurred:\", str(e))\n        return None\n\n# Call the function to load and merge data\nX_all, y_all = LoadDataWTFPADCW()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T13:30:16.158558Z","iopub.execute_input":"2024-04-26T13:30:16.158953Z","iopub.status.idle":"2024-04-26T13:30:22.513262Z","shell.execute_reply.started":"2024-04-26T13:30:16.158923Z","shell.execute_reply":"2024-04-26T13:30:22.512353Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Loading defended dataset for closed-world scenario\nDataset directory: /kaggle/input/dataset-defended-close-world/\nX_train loaded\ny_train loaded\nX_valid loaded\ny_valid loaded\nX_test loaded\ny_test loaded\nData dimensions:\nX: Training data's shape :  (76000, 5000)\ny: Training data's shape :  (76000,)\nX: Validation data's shape :  (9500, 5000)\ny: Validation data's shape :  (9500,)\nX: Testing data's shape :  (9500, 5000)\ny: Testing data's shape :  (9500,)\nMerged data dimensions:\nX: Merged data's shape :  (95000, 5000)\ny: Merged data's shape :  (95000,)\nFeatures of the merged dataset:\n[[ 1. -1.  1. ... -1. -1.  1.]\n [-1.  1. -1. ...  0.  0.  0.]\n [ 1.  1.  1. ...  0.  0.  0.]\n ...\n [-1.  1. -1. ...  0.  0.  0.]\n [-1.  1. -1. ...  0.  0.  0.]\n [ 1.  1. -1. ...  0.  0.  0.]]\nClass distribution:\nClass 0: 1000 samples\nClass 1: 1000 samples\nClass 2: 1000 samples\nClass 3: 1000 samples\nClass 4: 1000 samples\nClass 5: 1000 samples\nClass 6: 1000 samples\nClass 7: 1000 samples\nClass 8: 1000 samples\nClass 9: 1000 samples\nClass 10: 1000 samples\nClass 11: 1000 samples\nClass 12: 1000 samples\nClass 13: 1000 samples\nClass 14: 1000 samples\nClass 15: 1000 samples\nClass 16: 1000 samples\nClass 17: 1000 samples\nClass 18: 1000 samples\nClass 19: 1000 samples\nClass 20: 1000 samples\nClass 21: 1000 samples\nClass 22: 1000 samples\nClass 23: 1000 samples\nClass 24: 1000 samples\nClass 25: 1000 samples\nClass 26: 1000 samples\nClass 27: 1000 samples\nClass 28: 1000 samples\nClass 29: 1000 samples\nClass 30: 1000 samples\nClass 31: 1000 samples\nClass 32: 1000 samples\nClass 33: 1000 samples\nClass 34: 1000 samples\nClass 35: 1000 samples\nClass 36: 1000 samples\nClass 37: 1000 samples\nClass 38: 1000 samples\nClass 39: 1000 samples\nClass 40: 1000 samples\nClass 41: 1000 samples\nClass 42: 1000 samples\nClass 43: 1000 samples\nClass 44: 1000 samples\nClass 45: 1000 samples\nClass 46: 1000 samples\nClass 47: 1000 samples\nClass 48: 1000 samples\nClass 49: 1000 samples\nClass 50: 1000 samples\nClass 51: 1000 samples\nClass 52: 1000 samples\nClass 53: 1000 samples\nClass 54: 1000 samples\nClass 55: 1000 samples\nClass 56: 1000 samples\nClass 57: 1000 samples\nClass 58: 1000 samples\nClass 59: 1000 samples\nClass 60: 1000 samples\nClass 61: 1000 samples\nClass 62: 1000 samples\nClass 63: 1000 samples\nClass 64: 1000 samples\nClass 65: 1000 samples\nClass 66: 1000 samples\nClass 67: 1000 samples\nClass 68: 1000 samples\nClass 69: 1000 samples\nClass 70: 1000 samples\nClass 71: 1000 samples\nClass 72: 1000 samples\nClass 73: 1000 samples\nClass 74: 1000 samples\nClass 75: 1000 samples\nClass 76: 1000 samples\nClass 77: 1000 samples\nClass 78: 1000 samples\nClass 79: 1000 samples\nClass 80: 1000 samples\nClass 81: 1000 samples\nClass 82: 1000 samples\nClass 83: 1000 samples\nClass 84: 1000 samples\nClass 85: 1000 samples\nClass 86: 1000 samples\nClass 87: 1000 samples\nClass 88: 1000 samples\nClass 89: 1000 samples\nClass 90: 1000 samples\nClass 91: 1000 samples\nClass 92: 1000 samples\nClass 93: 1000 samples\nClass 94: 1000 samples\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**adamax + LR = 0.002**","metadata":{}},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, BatchNormalization\nfrom keras.layers import Activation, Flatten, Dense, Dropout\nfrom keras.initializers import glorot_uniform\nfrom keras.optimizers import Adamax  # Change here\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\n\nclass DFNet:\n    @staticmethod\n    def build(input_shape, classes, dropout_rate1=0.3, dropout_rate2=0.5, dropout_rate_fc=0.7):\n        model = Sequential()\n\n        # ... (previous model definition remains unchanged)\n        # Block 1\n        filter_num = [None,32, 64, 128, 256]\n        kernel_size = [None, 8, 8, 8, 8]\n        conv_stride_size = [None, 1, 1, 1, 1]\n        pool_stride_size = [None, 4, 4, 4, 4]\n        pool_size = [None, 8, 8, 8, 8]\n\n        for i in range(1, 4):\n            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n                             strides=conv_stride_size[i], padding='same',\n                             input_shape=input_shape if i == 1 else (None, input_shape[1]),\n                             name=f'block{i}_conv1'))\n            model.add(BatchNormalization(name=f'batch_normalization_{i * 2 - 1}'))\n            model.add(Activation('relu', name=f'block{i}_adv_act1'))\n            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n                             strides=conv_stride_size[i], padding='same',\n                             name=f'block{i}_conv2'))\n            model.add(BatchNormalization(name=f'batch_normalization_{i * 2}'))\n            model.add(Activation('relu', name=f'block{i}_adv_act2'))\n\n            model.add(MaxPooling1D(pool_size=pool_size[i], strides=pool_stride_size[i],\n                                   padding='same', name=f'block{i}_pool'))\n            model.add(Dropout(0.1, name=f'block{i}_dropout'))\n\n        # ... (rest of the model remains unchanged)\n\n        model.add(Flatten(name='flatten'))\n\n        # Fully connected layers\n        for i in range(1, 3):\n            model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name=f'fc{i}'))\n            model.add(BatchNormalization(name=f'batch_normalization_{i + 8}'))\n            model.add(Activation('relu', name=f'fc{i}_act'))\n\n            model.add(Dropout(dropout_rate_fc, name=f'fc{i}_dropout'))\n\n        # Output layer\n        model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='fc_final'))\n        model.add(Activation('softmax', name=\"softmax\"))\n\n        print(\"Model built successfully.\")\n        return model\n\n# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\ninput_shape = (5000, 1)\nnum_classes = 95\n\n# Experiment with different dropout rates\ndropout_rate1 = 0.6\ndropout_rate2 = 0.5\ndropout_rate_fc = 0.4\n\n# Build the model with the specified dropout rates\nmodel = DFNet.build(input_shape=input_shape, classes=num_classes,\n                    dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n\n\n# Print the model summary\nprint(\"Model Summary:\")\nmodel.summary()\n\n# Compile the model\noptimizer = Adamax(learning_rate=0.002)  # Change here\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\ninput_shape = (5000, 1)\nnum_classes = 95\n\n# Experiment with different dropout rates\ndropout_rate1 = 0.6\ndropout_rate2 = 0.5\ndropout_rate_fc = 0.4\n\n# Load and merge data using the function you defined\nX_all, y_all =   LoadDataWTFPADCW()\n# Define the number of folds\nn_splits = 5  # You can adjust this as needed\n\n# Initialize Stratified K-Fold\nstratkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Lists to store results\nall_histories = []\n\n# Iterate over folds\nfor fold, (train_index, test_index) in enumerate(stratkf.split(X_all, y_all)):\n    print(f\"\\nTraining on Fold {fold + 1}/{n_splits}\")\n\n    # Split the data\n    X_train, X_test = X_all[train_index], X_all[test_index]\n    y_train, y_test = y_all[train_index], y_all[test_index]\n    \n        # Build the model\n    model = DFNet.build(input_shape=input_shape, classes=num_classes,\n                        dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n\n\n    # Compile the model\n    optimizer = Adam(learning_rate=0.001)\n    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n     # Define early stopping callback\n    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n\n    # Train the model with early stopping\n    history = model.fit(X_train, y_train, epochs=25, batch_size=64, \n                        validation_data=(X_test, y_test), verbose=2, callbacks=[early_stopping])\n    \n   \n\n\n    # Train the model\n    #history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n\n    # Evaluate the model on the test set\n    loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n    print(f\"\\nFold {fold + 1} - Test Accuracy: {accuracy * 100:.2f}%\")\n\n    # Save history for later analysis if needed\n    all_histories.append(history)\n    # Check unique labels in y_train\n    \n    # Update num_classes based on the actual number of unique classes in your dataset\n#num_classes = len(np.unique(y_all))\nsaved_path_keras = '/kaggle/working/DFNet_OpenWorld_WTFPAD.keras'\nmodel.save(saved_path_keras)\n\nprint(\"Trained model saved successfully at:\", saved_path_keras)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T13:30:28.425455Z","iopub.execute_input":"2024-04-26T13:30:28.425789Z","iopub.status.idle":"2024-04-26T16:51:16.312115Z","shell.execute_reply.started":"2024-04-26T13:30:28.425763Z","shell.execute_reply":"2024-04-26T16:51:16.311162Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Model built successfully.\nModel Summary:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_6\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ block1_conv1 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m288\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act1 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_conv2 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m8,224\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act2 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_pool (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv1 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m16,448\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act1 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv2 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m32,832\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act2 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_pool (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv1 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m65,664\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act1 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv2 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m131,200\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act2 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_pool (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10112\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m5,177,856\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_act (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_dropout (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_act (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_dropout (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc_final (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m95\u001b[0m)             │        \u001b[38;5;34m48,735\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ softmax (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m95\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ block1_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,224</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10112</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,177,856</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_act (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_act (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc_final (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">95</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">48,735</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">95</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,749,791\u001b[0m (21.93 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,749,791</span> (21.93 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,746,847\u001b[0m (21.92 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,746,847</span> (21.92 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,944\u001b[0m (11.50 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,944</span> (11.50 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Loading defended dataset for closed-world scenario\nDataset directory: /kaggle/input/dataset-defended-close-world/\nX_train loaded\ny_train loaded\nX_valid loaded\ny_valid loaded\nX_test loaded\ny_test loaded\nData dimensions:\nX: Training data's shape :  (76000, 5000)\ny: Training data's shape :  (76000,)\nX: Validation data's shape :  (9500, 5000)\ny: Validation data's shape :  (9500,)\nX: Testing data's shape :  (9500, 5000)\ny: Testing data's shape :  (9500,)\nMerged data dimensions:\nX: Merged data's shape :  (95000, 5000)\ny: Merged data's shape :  (95000,)\nFeatures of the merged dataset:\n[[ 1. -1.  1. ... -1. -1.  1.]\n [-1.  1. -1. ...  0.  0.  0.]\n [ 1.  1.  1. ...  0.  0.  0.]\n ...\n [-1.  1. -1. ...  0.  0.  0.]\n [-1.  1. -1. ...  0.  0.  0.]\n [ 1.  1. -1. ...  0.  0.  0.]]\nClass distribution:\nClass 0: 1000 samples\nClass 1: 1000 samples\nClass 2: 1000 samples\nClass 3: 1000 samples\nClass 4: 1000 samples\nClass 5: 1000 samples\nClass 6: 1000 samples\nClass 7: 1000 samples\nClass 8: 1000 samples\nClass 9: 1000 samples\nClass 10: 1000 samples\nClass 11: 1000 samples\nClass 12: 1000 samples\nClass 13: 1000 samples\nClass 14: 1000 samples\nClass 15: 1000 samples\nClass 16: 1000 samples\nClass 17: 1000 samples\nClass 18: 1000 samples\nClass 19: 1000 samples\nClass 20: 1000 samples\nClass 21: 1000 samples\nClass 22: 1000 samples\nClass 23: 1000 samples\nClass 24: 1000 samples\nClass 25: 1000 samples\nClass 26: 1000 samples\nClass 27: 1000 samples\nClass 28: 1000 samples\nClass 29: 1000 samples\nClass 30: 1000 samples\nClass 31: 1000 samples\nClass 32: 1000 samples\nClass 33: 1000 samples\nClass 34: 1000 samples\nClass 35: 1000 samples\nClass 36: 1000 samples\nClass 37: 1000 samples\nClass 38: 1000 samples\nClass 39: 1000 samples\nClass 40: 1000 samples\nClass 41: 1000 samples\nClass 42: 1000 samples\nClass 43: 1000 samples\nClass 44: 1000 samples\nClass 45: 1000 samples\nClass 46: 1000 samples\nClass 47: 1000 samples\nClass 48: 1000 samples\nClass 49: 1000 samples\nClass 50: 1000 samples\nClass 51: 1000 samples\nClass 52: 1000 samples\nClass 53: 1000 samples\nClass 54: 1000 samples\nClass 55: 1000 samples\nClass 56: 1000 samples\nClass 57: 1000 samples\nClass 58: 1000 samples\nClass 59: 1000 samples\nClass 60: 1000 samples\nClass 61: 1000 samples\nClass 62: 1000 samples\nClass 63: 1000 samples\nClass 64: 1000 samples\nClass 65: 1000 samples\nClass 66: 1000 samples\nClass 67: 1000 samples\nClass 68: 1000 samples\nClass 69: 1000 samples\nClass 70: 1000 samples\nClass 71: 1000 samples\nClass 72: 1000 samples\nClass 73: 1000 samples\nClass 74: 1000 samples\nClass 75: 1000 samples\nClass 76: 1000 samples\nClass 77: 1000 samples\nClass 78: 1000 samples\nClass 79: 1000 samples\nClass 80: 1000 samples\nClass 81: 1000 samples\nClass 82: 1000 samples\nClass 83: 1000 samples\nClass 84: 1000 samples\nClass 85: 1000 samples\nClass 86: 1000 samples\nClass 87: 1000 samples\nClass 88: 1000 samples\nClass 89: 1000 samples\nClass 90: 1000 samples\nClass 91: 1000 samples\nClass 92: 1000 samples\nClass 93: 1000 samples\nClass 94: 1000 samples\n\nTraining on Fold 1/5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Model built successfully.\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1714138254.903827     108 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1714138353.912346     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1714138355.743905     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1188/1188 - 119s - 100ms/step - accuracy: 0.4246 - loss: 2.1454 - val_accuracy: 0.6744 - val_loss: 1.1844\nEpoch 2/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1714138358.632612     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1188/1188 - 93s - 78ms/step - accuracy: 0.6474 - loss: 1.2573 - val_accuracy: 0.7487 - val_loss: 0.8946\nEpoch 3/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.7179 - loss: 1.0000 - val_accuracy: 0.7863 - val_loss: 0.7642\nEpoch 4/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.7593 - loss: 0.8428 - val_accuracy: 0.8104 - val_loss: 0.6945\nEpoch 5/25\n1188/1188 - 93s - 78ms/step - accuracy: 0.7878 - loss: 0.7409 - val_accuracy: 0.8251 - val_loss: 0.6304\nEpoch 6/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.8087 - loss: 0.6598 - val_accuracy: 0.8239 - val_loss: 0.6322\nEpoch 7/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.8233 - loss: 0.6058 - val_accuracy: 0.8467 - val_loss: 0.5432\nEpoch 8/25\n1188/1188 - 142s - 120ms/step - accuracy: 0.8400 - loss: 0.5446 - val_accuracy: 0.8391 - val_loss: 0.5904\nEpoch 9/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.8522 - loss: 0.5005 - val_accuracy: 0.8647 - val_loss: 0.4966\nEpoch 10/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.8616 - loss: 0.4608 - val_accuracy: 0.8655 - val_loss: 0.5068\nEpoch 11/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.8698 - loss: 0.4342 - val_accuracy: 0.8731 - val_loss: 0.4710\nEpoch 12/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.8790 - loss: 0.4001 - val_accuracy: 0.8743 - val_loss: 0.4779\nEpoch 13/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.8832 - loss: 0.3852 - val_accuracy: 0.8770 - val_loss: 0.4600\nEpoch 14/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.8921 - loss: 0.3543 - val_accuracy: 0.8713 - val_loss: 0.4899\nEpoch 15/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.8979 - loss: 0.3363 - val_accuracy: 0.8789 - val_loss: 0.4668\nEpoch 16/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.9010 - loss: 0.3230 - val_accuracy: 0.8785 - val_loss: 0.4995\nEpoch 17/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.9047 - loss: 0.3077 - val_accuracy: 0.8747 - val_loss: 0.4981\nEpoch 18/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.9096 - loss: 0.2944 - val_accuracy: 0.8806 - val_loss: 0.4814\nEpoch 19/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.9125 - loss: 0.2833 - val_accuracy: 0.8733 - val_loss: 0.5461\nEpoch 20/25\n1188/1188 - 142s - 120ms/step - accuracy: 0.9159 - loss: 0.2733 - val_accuracy: 0.8723 - val_loss: 0.5401\nEpoch 21/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.9204 - loss: 0.2578 - val_accuracy: 0.8851 - val_loss: 0.4698\nEpoch 22/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.9227 - loss: 0.2483 - val_accuracy: 0.8830 - val_loss: 0.4938\nEpoch 23/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.9239 - loss: 0.2403 - val_accuracy: 0.8816 - val_loss: 0.5025\nEpoch 24/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9281 - loss: 0.2320 - val_accuracy: 0.8793 - val_loss: 0.5133\nEpoch 25/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.9294 - loss: 0.2262 - val_accuracy: 0.8848 - val_loss: 0.5001\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1714140673.683235     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"594/594 - 4s - 6ms/step - accuracy: 0.8770 - loss: 0.4601\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1714140676.896801     106 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\nFold 1 - Test Accuracy: 87.70%\n\nTraining on Fold 2/5\nModel built successfully.\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1714140696.805493     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1714140796.148290     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1714140797.951149     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1188/1188 - 118s - 100ms/step - accuracy: 0.4219 - loss: 2.1610 - val_accuracy: 0.6324 - val_loss: 1.3246\nEpoch 2/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1714140800.841968     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1188/1188 - 92s - 78ms/step - accuracy: 0.6482 - loss: 1.2528 - val_accuracy: 0.7432 - val_loss: 0.9144\nEpoch 3/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.7191 - loss: 0.9916 - val_accuracy: 0.7901 - val_loss: 0.7673\nEpoch 4/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.7632 - loss: 0.8346 - val_accuracy: 0.7914 - val_loss: 0.7646\nEpoch 5/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.7892 - loss: 0.7367 - val_accuracy: 0.8286 - val_loss: 0.6263\nEpoch 6/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.8119 - loss: 0.6510 - val_accuracy: 0.8307 - val_loss: 0.5998\nEpoch 7/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.8258 - loss: 0.5923 - val_accuracy: 0.8471 - val_loss: 0.5503\nEpoch 8/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.8400 - loss: 0.5403 - val_accuracy: 0.8378 - val_loss: 0.5912\nEpoch 9/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.8524 - loss: 0.4976 - val_accuracy: 0.8536 - val_loss: 0.5335\nEpoch 10/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.8621 - loss: 0.4597 - val_accuracy: 0.8611 - val_loss: 0.5303\nEpoch 11/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.8714 - loss: 0.4279 - val_accuracy: 0.8676 - val_loss: 0.4917\nEpoch 12/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.8789 - loss: 0.3985 - val_accuracy: 0.8681 - val_loss: 0.4981\nEpoch 13/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.8851 - loss: 0.3817 - val_accuracy: 0.8663 - val_loss: 0.4956\nEpoch 14/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.8917 - loss: 0.3508 - val_accuracy: 0.8714 - val_loss: 0.4905\nEpoch 15/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8980 - loss: 0.3351 - val_accuracy: 0.8739 - val_loss: 0.4968\nEpoch 16/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9023 - loss: 0.3190 - val_accuracy: 0.8794 - val_loss: 0.4850\nEpoch 17/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9051 - loss: 0.3107 - val_accuracy: 0.8329 - val_loss: 0.7177\nEpoch 18/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9107 - loss: 0.2902 - val_accuracy: 0.8734 - val_loss: 0.5138\nEpoch 19/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9138 - loss: 0.2775 - val_accuracy: 0.8735 - val_loss: 0.5331\nEpoch 20/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9151 - loss: 0.2736 - val_accuracy: 0.8821 - val_loss: 0.4846\nEpoch 21/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9204 - loss: 0.2546 - val_accuracy: 0.8803 - val_loss: 0.5029\nEpoch 22/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.9214 - loss: 0.2503 - val_accuracy: 0.8837 - val_loss: 0.4873\nEpoch 23/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.9252 - loss: 0.2378 - val_accuracy: 0.8783 - val_loss: 0.5182\nEpoch 24/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9288 - loss: 0.2277 - val_accuracy: 0.8685 - val_loss: 0.5646\nEpoch 25/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9292 - loss: 0.2274 - val_accuracy: 0.8803 - val_loss: 0.5322\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1714143013.311104     109 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"594/594 - 4s - 6ms/step - accuracy: 0.8821 - loss: 0.4847\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1714143016.488054     106 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\nFold 2 - Test Accuracy: 88.21%\n\nTraining on Fold 3/5\nModel built successfully.\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1714143036.929081     109 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1714143136.142081     106 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1714143137.917009     109 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1188/1188 - 118s - 100ms/step - accuracy: 0.4284 - loss: 2.1387 - val_accuracy: 0.5467 - val_loss: 1.6457\nEpoch 2/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1714143140.846466     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1188/1188 - 92s - 78ms/step - accuracy: 0.6514 - loss: 1.2462 - val_accuracy: 0.7496 - val_loss: 0.9030\nEpoch 3/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.7188 - loss: 0.9899 - val_accuracy: 0.7922 - val_loss: 0.7524\nEpoch 4/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.7611 - loss: 0.8407 - val_accuracy: 0.8117 - val_loss: 0.6749\nEpoch 5/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.7897 - loss: 0.7304 - val_accuracy: 0.8336 - val_loss: 0.6061\nEpoch 6/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8085 - loss: 0.6584 - val_accuracy: 0.8487 - val_loss: 0.5532\nEpoch 7/25\n1188/1188 - 142s - 120ms/step - accuracy: 0.8253 - loss: 0.5974 - val_accuracy: 0.8558 - val_loss: 0.5245\nEpoch 8/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8404 - loss: 0.5399 - val_accuracy: 0.8537 - val_loss: 0.5354\nEpoch 9/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8522 - loss: 0.4988 - val_accuracy: 0.8512 - val_loss: 0.5543\nEpoch 10/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8620 - loss: 0.4610 - val_accuracy: 0.8698 - val_loss: 0.4859\nEpoch 11/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8703 - loss: 0.4290 - val_accuracy: 0.8678 - val_loss: 0.4946\nEpoch 12/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8786 - loss: 0.4011 - val_accuracy: 0.8784 - val_loss: 0.4607\nEpoch 13/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8847 - loss: 0.3775 - val_accuracy: 0.8404 - val_loss: 0.6146\nEpoch 14/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8910 - loss: 0.3583 - val_accuracy: 0.8747 - val_loss: 0.4927\nEpoch 15/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8962 - loss: 0.3375 - val_accuracy: 0.8785 - val_loss: 0.4615\nEpoch 16/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9018 - loss: 0.3201 - val_accuracy: 0.8684 - val_loss: 0.5197\nEpoch 17/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9064 - loss: 0.3065 - val_accuracy: 0.8836 - val_loss: 0.4584\nEpoch 18/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9093 - loss: 0.2924 - val_accuracy: 0.8812 - val_loss: 0.4812\nEpoch 19/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9123 - loss: 0.2793 - val_accuracy: 0.8811 - val_loss: 0.4841\nEpoch 20/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9157 - loss: 0.2692 - val_accuracy: 0.8798 - val_loss: 0.4933\nEpoch 21/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9186 - loss: 0.2578 - val_accuracy: 0.8804 - val_loss: 0.4714\nEpoch 22/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9190 - loss: 0.2565 - val_accuracy: 0.8848 - val_loss: 0.4786\nEpoch 23/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9237 - loss: 0.2419 - val_accuracy: 0.8834 - val_loss: 0.4703\nEpoch 24/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9279 - loss: 0.2310 - val_accuracy: 0.8844 - val_loss: 0.4838\nEpoch 25/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9285 - loss: 0.2261 - val_accuracy: 0.8847 - val_loss: 0.5040\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1714145399.919374     108 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"594/594 - 4s - 6ms/step - accuracy: 0.8836 - loss: 0.4585\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1714145403.121393     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\nFold 3 - Test Accuracy: 88.36%\n\nTraining on Fold 4/5\nModel built successfully.\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1714145423.482505     106 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1714145522.843898     108 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1714145524.662105     108 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1188/1188 - 119s - 100ms/step - accuracy: 0.4286 - loss: 2.1354 - val_accuracy: 0.6894 - val_loss: 1.1460\nEpoch 2/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1714145527.577656     106 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1188/1188 - 92s - 78ms/step - accuracy: 0.6494 - loss: 1.2551 - val_accuracy: 0.7630 - val_loss: 0.8483\nEpoch 3/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.7162 - loss: 1.0005 - val_accuracy: 0.7596 - val_loss: 0.8452\nEpoch 4/25\n1188/1188 - 93s - 78ms/step - accuracy: 0.7562 - loss: 0.8490 - val_accuracy: 0.7519 - val_loss: 0.8913\nEpoch 5/25\n1188/1188 - 93s - 78ms/step - accuracy: 0.7837 - loss: 0.7479 - val_accuracy: 0.8196 - val_loss: 0.6519\nEpoch 6/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8069 - loss: 0.6618 - val_accuracy: 0.8256 - val_loss: 0.6265\nEpoch 7/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.8238 - loss: 0.6037 - val_accuracy: 0.8526 - val_loss: 0.5346\nEpoch 8/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8408 - loss: 0.5426 - val_accuracy: 0.8548 - val_loss: 0.5215\nEpoch 9/25\n1188/1188 - 93s - 78ms/step - accuracy: 0.8498 - loss: 0.5074 - val_accuracy: 0.8613 - val_loss: 0.4986\nEpoch 10/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8614 - loss: 0.4666 - val_accuracy: 0.8617 - val_loss: 0.4885\nEpoch 11/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8711 - loss: 0.4346 - val_accuracy: 0.8676 - val_loss: 0.4820\nEpoch 12/25\n1188/1188 - 142s - 120ms/step - accuracy: 0.8770 - loss: 0.4070 - val_accuracy: 0.8664 - val_loss: 0.4886\nEpoch 13/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8822 - loss: 0.3863 - val_accuracy: 0.8658 - val_loss: 0.5128\nEpoch 14/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8912 - loss: 0.3596 - val_accuracy: 0.8595 - val_loss: 0.5147\nEpoch 15/25\n1188/1188 - 142s - 120ms/step - accuracy: 0.8956 - loss: 0.3434 - val_accuracy: 0.8714 - val_loss: 0.4823\nEpoch 16/25\n1188/1188 - 92s - 78ms/step - accuracy: 0.8997 - loss: 0.3266 - val_accuracy: 0.8763 - val_loss: 0.4788\nEpoch 17/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9043 - loss: 0.3108 - val_accuracy: 0.8758 - val_loss: 0.4782\nEpoch 18/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9095 - loss: 0.2939 - val_accuracy: 0.8751 - val_loss: 0.5130\nEpoch 19/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9126 - loss: 0.2844 - val_accuracy: 0.8725 - val_loss: 0.5274\nEpoch 20/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9148 - loss: 0.2736 - val_accuracy: 0.8746 - val_loss: 0.5031\nEpoch 21/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9191 - loss: 0.2604 - val_accuracy: 0.8812 - val_loss: 0.4951\nEpoch 22/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9209 - loss: 0.2537 - val_accuracy: 0.8865 - val_loss: 0.4636\nEpoch 23/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9258 - loss: 0.2391 - val_accuracy: 0.8836 - val_loss: 0.4673\nEpoch 24/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9261 - loss: 0.2370 - val_accuracy: 0.8839 - val_loss: 0.4788\nEpoch 25/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9298 - loss: 0.2262 - val_accuracy: 0.8844 - val_loss: 0.4774\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1714147839.651083     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"594/594 - 4s - 6ms/step - accuracy: 0.8865 - loss: 0.4637\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1714147842.819242     106 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\nFold 4 - Test Accuracy: 88.65%\n\nTraining on Fold 5/5\nModel built successfully.\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1714147863.487797     109 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1714147962.400411     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1714147964.219368     108 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1188/1188 - 118s - 100ms/step - accuracy: 0.4217 - loss: 2.1638 - val_accuracy: 0.6044 - val_loss: 1.4113\nEpoch 2/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1714147967.116823     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1188/1188 - 92s - 77ms/step - accuracy: 0.6460 - loss: 1.2721 - val_accuracy: 0.7605 - val_loss: 0.8526\nEpoch 3/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.7183 - loss: 0.9996 - val_accuracy: 0.8019 - val_loss: 0.7005\nEpoch 4/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.7598 - loss: 0.8503 - val_accuracy: 0.7913 - val_loss: 0.7328\nEpoch 5/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.7855 - loss: 0.7479 - val_accuracy: 0.8331 - val_loss: 0.5940\nEpoch 6/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8074 - loss: 0.6667 - val_accuracy: 0.8348 - val_loss: 0.5910\nEpoch 7/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8252 - loss: 0.6010 - val_accuracy: 0.8258 - val_loss: 0.6217\nEpoch 8/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8385 - loss: 0.5519 - val_accuracy: 0.8390 - val_loss: 0.5781\nEpoch 9/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8511 - loss: 0.5022 - val_accuracy: 0.8705 - val_loss: 0.4706\nEpoch 10/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8612 - loss: 0.4638 - val_accuracy: 0.8668 - val_loss: 0.4797\nEpoch 11/25\n1188/1188 - 142s - 120ms/step - accuracy: 0.8730 - loss: 0.4268 - val_accuracy: 0.8610 - val_loss: 0.5117\nEpoch 12/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8794 - loss: 0.4010 - val_accuracy: 0.8734 - val_loss: 0.4883\nEpoch 13/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8842 - loss: 0.3778 - val_accuracy: 0.8792 - val_loss: 0.4489\nEpoch 14/25\n1188/1188 - 142s - 120ms/step - accuracy: 0.8913 - loss: 0.3586 - val_accuracy: 0.8741 - val_loss: 0.4883\nEpoch 15/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.8964 - loss: 0.3394 - val_accuracy: 0.8773 - val_loss: 0.4745\nEpoch 16/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9015 - loss: 0.3188 - val_accuracy: 0.8844 - val_loss: 0.4486\nEpoch 17/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9067 - loss: 0.3023 - val_accuracy: 0.8777 - val_loss: 0.4686\nEpoch 18/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9096 - loss: 0.2926 - val_accuracy: 0.8814 - val_loss: 0.4703\nEpoch 19/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9133 - loss: 0.2819 - val_accuracy: 0.8850 - val_loss: 0.4557\nEpoch 20/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9166 - loss: 0.2695 - val_accuracy: 0.8778 - val_loss: 0.4650\nEpoch 21/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9184 - loss: 0.2602 - val_accuracy: 0.8861 - val_loss: 0.4620\nEpoch 22/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9208 - loss: 0.2523 - val_accuracy: 0.8853 - val_loss: 0.4636\nEpoch 23/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9265 - loss: 0.2342 - val_accuracy: 0.8861 - val_loss: 0.4869\nEpoch 24/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9278 - loss: 0.2327 - val_accuracy: 0.8841 - val_loss: 0.4850\nEpoch 25/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9294 - loss: 0.2251 - val_accuracy: 0.8852 - val_loss: 0.4898\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1714150272.807610     106 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"594/594 - 4s - 6ms/step - accuracy: 0.8844 - loss: 0.4487\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1714150275.968515     106 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\nFold 5 - Test Accuracy: 88.44%\nTrained model saved successfully at: /kaggle/working/DFNet_OpenWorld_WTFPAD.keras\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#using adam","metadata":{}},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, BatchNormalization\nfrom keras.layers import Activation, Flatten, Dense, Dropout, ReLU\nfrom keras.initializers import glorot_uniform\nfrom keras.optimizers import Adam\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom keras import layers, models\nimport numpy as np\nfrom keras.layers import Activation\n\nclass DFNet:\n    @staticmethod\n    def build(input_shape, classes, dropout_rate1=0.3, dropout_rate2=0.5, dropout_rate_fc=0.7):\n        model = Sequential()\n\n        # ... (previous model definition remains unchanged)\n        # Block 1\n        filter_num = [None,32, 64, 128, 256]\n        kernel_size = [None, 8, 8, 8, 8]\n        conv_stride_size = [None, 1, 1, 1, 1]\n        pool_stride_size = [None, 4, 4, 4, 4]\n        pool_size = [None, 8, 8, 8, 8]\n\n        for i in range(1, 4):\n            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n                             strides=conv_stride_size[i], padding='same',\n                             input_shape=input_shape if i == 1 else (None, input_shape[1]),\n                             name=f'block{i}_conv1'))\n            model.add(BatchNormalization(name=f'batch_normalization_{i * 2 - 1}'))\n            model.add(Activation('relu', name=f'block{i}_adv_act1'))\n            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n                             strides=conv_stride_size[i], padding='same',\n                             name=f'block{i}_conv2'))\n            model.add(BatchNormalization(name=f'batch_normalization_{i * 2}'))\n            model.add(Activation('relu', name=f'block{i}_adv_act2'))\n\n            model.add(MaxPooling1D(pool_size=pool_size[i], strides=pool_stride_size[i],\n                                   padding='same', name=f'block{i}_pool'))\n            model.add(Dropout(0.1, name=f'block{i}_dropout'))\n\n        # ... (rest of the model remains unchanged)\n\n        model.add(Flatten(name='flatten'))\n\n        # Fully connected layers\n        for i in range(1, 3):\n            model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name=f'fc{i}'))\n            model.add(BatchNormalization(name=f'batch_normalization_{i + 8}'))\n            model.add(Activation('relu', name=f'fc{i}_act'))\n\n            # Experiment with different dropout rates\n            model.add(Dropout(dropout_rate_fc, name=f'fc{i}_dropout'))\n\n        # Output layer\n        model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='fc_final'))\n        model.add(Activation('softmax', name=\"softmax\"))\n\n        print(\"Model built successfully.\")\n        return model\n    # Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\ninput_shape = (5000, 1)\nnum_classes = 95\n\n# Experiment with different dropout rates\ndropout_rate1 = 0.6\ndropout_rate2 = 0.5\ndropout_rate_fc = 0.4\n\n# Build the model with the specified dropout rates\nmodel = DFNet.build(input_shape=input_shape, classes=num_classes,\n                    dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n\n\n# Print the model summary\nprint(\"Model Summary:\")\nmodel.summary()\n\n# Compile the model\noptimizer = Adam(learning_rate=0.001)\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n\n# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\ninput_shape = (5000, 1)\nnum_classes = 95\n\n# Experiment with different dropout rates\ndropout_rate1 = 0.6\ndropout_rate2 = 0.5\ndropout_rate_fc = 0.4\n\n# Load and merge data using the function you defined\nX_all, y_all =   LoadDataWTFPADCW()\n# Define the number of folds\nn_splits = 5  # You can adjust this as needed\n\n# Initialize Stratified K-Fold\nstratkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Lists to store results\nall_histories = []\n\n# Iterate over folds\nfor fold, (train_index, test_index) in enumerate(stratkf.split(X_all, y_all)):\n    print(f\"\\nTraining on Fold {fold + 1}/{n_splits}\")\n\n    # Split the data\n    X_train, X_test = X_all[train_index], X_all[test_index]\n    y_train, y_test = y_all[train_index], y_all[test_index]\n    \n        # Build the model\n    model = DFNet.build(input_shape=input_shape, classes=num_classes,\n                        dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n\n\n    # Compile the model\n    optimizer = Adam(learning_rate=0.001)\n    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n     # Define early stopping callback\n    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n\n    # Train the model with early stopping\n    history = model.fit(X_train, y_train, epochs=20, batch_size=64, \n                        validation_data=(X_test, y_test), verbose=2, callbacks=[early_stopping])\n    \n   \n\n\n    # Train the model\n    #history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n\n    # Evaluate the model on the test set\n    loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n    print(f\"\\nFold {fold + 1} - Test Accuracy: {accuracy * 100:.2f}%\")\n\n    # Save history for later analysis if needed\n    all_histories.append(history)\n    # Check unique labels in y_train\n    \n    # Update num_classes based on the actual number of unique classes in your dataset\n#num_classes = len(np.unique(y_all))\nsaved_path_keras = '/kaggle/working/DFNet_OpenWorld_WTFPAD.keras'\nmodel.save(saved_path_keras)\n\nprint(\"Trained model saved successfully at:\", saved_path_keras)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ****# for Close World Defended Walkie Talkie","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, BatchNormalization\nfrom keras.layers import Activation, Flatten, Dense, Dropout, ReLU\nfrom keras.initializers import glorot_uniform\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom keras import layers, models\n\ndef LoadDataWALKIETALKIEOW():\n\n    print(\"Loading defended dataset for closed-world scenario\")\n\n    # Point to the directory storing data\n    dataset_dir = '/kaggle/input/defended-close-world-walkietalkie-dataset/'\n    # X represents a sequence of traffic directions\n    # y represents a sequence of corresponding label (website's label)\n    # Debug: Print dataset directory\n    print(\"Dataset directory:\", dataset_dir)\n\n    try:\n        # Load training data\n        with open(dataset_dir + 'X_train_WalkieTalkie.pkl', 'rb') as handle:\n            X_train = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"X_train loaded\")\n\n        with open(dataset_dir + 'y_train_WalkieTalkie.pkl', 'rb') as handle:\n            y_train = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"y_train loaded\")\n\n        # Load validation data\n        with open(dataset_dir + 'X_valid_WalkieTalkie.pkl', 'rb') as handle:\n            X_valid = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"X_valid loaded\")\n\n        with open(dataset_dir + 'y_valid_WalkieTalkie.pkl', 'rb') as handle:\n            y_valid = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"y_valid loaded\")\n\n        # Load testing data\n        with open(dataset_dir + 'X_test_WalkieTalkie.pkl', 'rb') as handle:\n            X_test = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"X_test loaded\")\n\n        with open(dataset_dir + 'y_test_WalkieTalkie.pkl', 'rb') as handle:\n            y_test = np.array(pickle.load(handle, encoding='latin1'))\n        print(\"y_test loaded\")\n\n        print(\"Data dimensions:\")\n        print(\"X: Training data's shape : \", X_train.shape)\n        print(\"y: Training data's shape : \", y_train.shape)\n        print(\"X: Validation data's shape : \", X_valid.shape)\n        print(\"y: Validation data's shape : \", y_valid.shape)\n        print(\"X: Testing data's shape : \", X_test.shape)\n        print(\"y: Testing data's shape : \", y_test.shape)\n\n        # Merge datasets\n        X_all = np.concatenate((X_train, X_valid, X_test), axis=0)\n        y_all = np.concatenate((y_train, y_valid, y_test), axis=0)\n\n        print(\"Merged data dimensions:\")\n        print(\"X: Merged data's shape : \", X_all.shape)\n        print(\"y: Merged data's shape : \", y_all.shape)\n        \n        # Print features of the merged dataset\n        print(\"Features of the merged dataset:\")\n        print(X_all)\n        \n        # Check if the class distribution is balanced\n        unique_classes, class_counts = np.unique(y_all, return_counts=True)\n        class_distribution = dict(zip(unique_classes, class_counts))\n\n        print(\"Class distribution:\")\n        for class_label, count in class_distribution.items():\n            print(f\"Class {class_label}: {count} samples\")\n\n        # Plot the class distribution\n       # plt.bar(class_distribution.keys(), class_distribution.values())\n       # plt.xlabel('Class Label')\n       # plt.ylabel('Number of Samples')\n      #  plt.title('Class Distribution')\n      #  plt.show()\n        \n        return X_all, y_all\n\n    except Exception as e:\n        print(\"An error occurred:\", str(e))\n        return None\n\n# Call the function to load and merge data\nX_all, y_all = LoadDataWALKIETALKIEOW()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T04:14:40.081890Z","iopub.execute_input":"2024-04-15T04:14:40.082192Z","iopub.status.idle":"2024-04-15T04:16:15.331959Z","shell.execute_reply.started":"2024-04-15T04:14:40.082164Z","shell.execute_reply":"2024-04-15T04:16:15.330870Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-04-15 04:14:41.759329: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-15 04:14:41.759418: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-15 04:14:41.888997: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Loading defended dataset for closed-world scenario\nDataset directory: /kaggle/input/defended-close-world-walkietalkie-dataset/\nX_train loaded\ny_train loaded\nX_valid loaded\ny_valid loaded\nX_test loaded\ny_test loaded\nData dimensions:\nX: Training data's shape :  (80000, 5000)\ny: Training data's shape :  (80000,)\nX: Validation data's shape :  (5000, 5000)\ny: Validation data's shape :  (5000,)\nX: Testing data's shape :  (5000, 5000)\ny: Testing data's shape :  (5000,)\nMerged data dimensions:\nX: Merged data's shape :  (90000, 5000)\ny: Merged data's shape :  (90000,)\nFeatures of the merged dataset:\n[[ 1  1  1 ... -1 -1 -1]\n [ 1 -1  1 ... -1 -1 -1]\n [ 1 -1 -1 ...  0  0  0]\n ...\n [ 1 -1  1 ...  0  0  0]\n [ 1 -1 -1 ...  0  0  0]\n [ 1 -1  1 ...  0  0  0]]\nClass distribution:\nClass 0: 900 samples\nClass 1: 900 samples\nClass 2: 900 samples\nClass 3: 900 samples\nClass 4: 900 samples\nClass 5: 900 samples\nClass 6: 900 samples\nClass 7: 900 samples\nClass 8: 900 samples\nClass 9: 900 samples\nClass 10: 900 samples\nClass 11: 900 samples\nClass 12: 900 samples\nClass 13: 900 samples\nClass 14: 900 samples\nClass 15: 900 samples\nClass 16: 900 samples\nClass 17: 900 samples\nClass 18: 900 samples\nClass 19: 900 samples\nClass 20: 900 samples\nClass 21: 900 samples\nClass 22: 900 samples\nClass 23: 900 samples\nClass 24: 900 samples\nClass 25: 900 samples\nClass 26: 900 samples\nClass 27: 900 samples\nClass 28: 900 samples\nClass 29: 900 samples\nClass 30: 900 samples\nClass 31: 900 samples\nClass 32: 900 samples\nClass 33: 900 samples\nClass 34: 900 samples\nClass 35: 900 samples\nClass 36: 900 samples\nClass 37: 900 samples\nClass 38: 900 samples\nClass 39: 900 samples\nClass 40: 900 samples\nClass 41: 900 samples\nClass 42: 900 samples\nClass 43: 900 samples\nClass 44: 900 samples\nClass 45: 900 samples\nClass 46: 900 samples\nClass 47: 900 samples\nClass 48: 900 samples\nClass 49: 900 samples\nClass 50: 900 samples\nClass 51: 900 samples\nClass 52: 900 samples\nClass 53: 900 samples\nClass 54: 900 samples\nClass 55: 900 samples\nClass 56: 900 samples\nClass 57: 900 samples\nClass 58: 900 samples\nClass 59: 900 samples\nClass 60: 900 samples\nClass 61: 900 samples\nClass 62: 900 samples\nClass 63: 900 samples\nClass 64: 900 samples\nClass 65: 900 samples\nClass 66: 900 samples\nClass 67: 900 samples\nClass 68: 900 samples\nClass 69: 900 samples\nClass 70: 900 samples\nClass 71: 900 samples\nClass 72: 900 samples\nClass 73: 900 samples\nClass 74: 900 samples\nClass 75: 900 samples\nClass 76: 900 samples\nClass 77: 900 samples\nClass 78: 900 samples\nClass 79: 900 samples\nClass 80: 900 samples\nClass 81: 900 samples\nClass 82: 900 samples\nClass 83: 900 samples\nClass 84: 900 samples\nClass 85: 900 samples\nClass 86: 900 samples\nClass 87: 900 samples\nClass 88: 900 samples\nClass 89: 900 samples\nClass 90: 900 samples\nClass 91: 900 samples\nClass 92: 900 samples\nClass 93: 900 samples\nClass 94: 900 samples\nClass 95: 900 samples\nClass 96: 900 samples\nClass 97: 900 samples\nClass 98: 900 samples\nClass 99: 900 samples\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**#using Adamax+ LR = 0.002**","metadata":{}},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, BatchNormalization\nfrom keras.layers import Activation, Flatten, Dense, Dropout\nfrom keras.initializers import glorot_uniform\nfrom keras.optimizers import Adamax  # Change here\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\n\nclass DFNet:\n    @staticmethod\n    def build(input_shape, classes, dropout_rate1=0.3, dropout_rate2=0.5, dropout_rate_fc=0.7):\n        model = Sequential()\n\n        # ... (previous model definition remains unchanged)\n        # Block 1\n        filter_num = [None,32, 64, 128, 256]\n        kernel_size = [None, 8, 8, 8, 8]\n        conv_stride_size = [None, 1, 1, 1, 1]\n        pool_stride_size = [None, 4, 4, 4, 4]\n        pool_size = [None, 8, 8, 8, 8]\n\n        for i in range(1, 4):\n            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n                             strides=conv_stride_size[i], padding='same',\n                             input_shape=input_shape if i == 1 else (None, input_shape[1]),\n                             name=f'block{i}_conv1'))\n            model.add(BatchNormalization(name=f'batch_normalization_{i * 2 - 1}'))\n            model.add(Activation('relu', name=f'block{i}_adv_act1'))\n            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n                             strides=conv_stride_size[i], padding='same',\n                             name=f'block{i}_conv2'))\n            model.add(BatchNormalization(name=f'batch_normalization_{i * 2}'))\n            model.add(Activation('relu', name=f'block{i}_adv_act2'))\n\n            model.add(MaxPooling1D(pool_size=pool_size[i], strides=pool_stride_size[i],\n                                   padding='same', name=f'block{i}_pool'))\n            model.add(Dropout(0.1, name=f'block{i}_dropout'))\n\n        # ... (rest of the model remains unchanged)\n\n        model.add(Flatten(name='flatten'))\n\n        # Fully connected layers\n        for i in range(1, 3):\n            model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name=f'fc{i}'))\n            model.add(BatchNormalization(name=f'batch_normalization_{i + 8}'))\n            model.add(Activation('relu', name=f'fc{i}_act'))\n\n            model.add(Dropout(dropout_rate_fc, name=f'fc{i}_dropout'))\n\n        # Output layer\n        model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='fc_final'))\n        model.add(Activation('softmax', name=\"softmax\"))\n\n        print(\"Model built successfully.\")\n        return model\n\n# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\ninput_shape = (5000, 1)\nnum_classes = 95\n\n# Experiment with different dropout rates\ndropout_rate1 = 0.6\ndropout_rate2 = 0.5\ndropout_rate_fc = 0.4\n\n# Build the model with the specified dropout rates\nmodel = DFNet.build(input_shape=input_shape, classes=num_classes,\n                    dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n\n\n# Print the model summary\nprint(\"Model Summary:\")\nmodel.summary()\n\n# Compile the model\noptimizer = Adamax(learning_rate=0.002)  # Change here\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\ninput_shape = (5000, 1)\nnum_classes = 95\n\n# Experiment with different dropout rates\ndropout_rate1 = 0.6\ndropout_rate2 = 0.5\ndropout_rate_fc = 0.4\n\n# Load and merge data using the function you defined\nX_all, y_all =   LoadDataWALKIETALKIEOW()\n# Define the number of folds\nn_splits = 5  # You can adjust this as needed\n\n# Initialize Stratified K-Fold\nstratkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Lists to store results\nall_histories = []\n\n# Iterate over folds\nfor fold, (train_index, test_index) in enumerate(stratkf.split(X_all, y_all)):\n    print(f\"\\nTraining on Fold {fold + 1}/{n_splits}\")\n\n    # Split the data\n    X_train, X_test = X_all[train_index], X_all[test_index]\n    y_train, y_test = y_all[train_index], y_all[test_index]\n    \n        # Build the model\n    model = DFNet.build(input_shape=input_shape, classes=num_classes,\n                        dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n\n\n    # Compile the model\n    optimizer = Adamax(learning_rate=0.002)\n    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n     # Define early stopping callback\n    early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n\n    # Train the model with early stopping\n    history = model.fit(X_train, y_train, epochs=25, batch_size=64, \n                        validation_data=(X_test, y_test), verbose=2, callbacks=[early_stopping])\n    \n   \n\n\n    # Train the model\n    #history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n\n    # Evaluate the model on the test set\n    loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n    print(f\"\\nFold {fold + 1} - Test Accuracy: {accuracy * 100:.2f}%\")\n\n    # Save history for later analysis if needed\n    all_histories.append(history)\n    # Check unique labels in y_train\n    \n    # Update num_classes based on the actual number of unique classes in your dataset\n#num_classes = len(np.unique(y_all))\nsaved_path_keras = '/kaggle/working/DFNet_OpenWorld_WalkieTalkie.keras'\nmodel.save(saved_path_keras)\n\nprint(\"Trained model saved successfully at:\", saved_path_keras)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T04:17:25.363951Z","iopub.execute_input":"2024-04-15T04:17:25.364628Z","iopub.status.idle":"2024-04-15T07:23:18.229563Z","shell.execute_reply.started":"2024-04-15T04:17:25.364583Z","shell.execute_reply":"2024-04-15T07:23:18.228644Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Model built successfully.\nModel Summary:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ block1_conv1 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m288\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act1 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_conv2 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m8,224\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act2 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_pool (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv1 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m16,448\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act1 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv2 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m32,832\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act2 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_pool (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv1 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m65,664\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act1 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv2 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m131,200\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act2 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_pool (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10112\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m5,177,856\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_act (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_dropout (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_act (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_dropout (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc_final (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m95\u001b[0m)             │        \u001b[38;5;34m48,735\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ softmax (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m95\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ block1_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,224</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10112</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,177,856</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_act (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_act (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc_final (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">95</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">48,735</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">95</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,749,791\u001b[0m (21.93 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,749,791</span> (21.93 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,746,847\u001b[0m (21.92 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,746,847</span> (21.92 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,944\u001b[0m (11.50 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,944</span> (11.50 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Loading defended dataset for closed-world scenario\nDataset directory: /kaggle/input/defended-close-world-walkietalkie-dataset/\nX_train loaded\ny_train loaded\nX_valid loaded\ny_valid loaded\nX_test loaded\ny_test loaded\nData dimensions:\nX: Training data's shape :  (80000, 5000)\ny: Training data's shape :  (80000,)\nX: Validation data's shape :  (5000, 5000)\ny: Validation data's shape :  (5000,)\nX: Testing data's shape :  (5000, 5000)\ny: Testing data's shape :  (5000,)\nMerged data dimensions:\nX: Merged data's shape :  (90000, 5000)\ny: Merged data's shape :  (90000,)\nFeatures of the merged dataset:\n[[ 1  1  1 ... -1 -1 -1]\n [ 1 -1  1 ... -1 -1 -1]\n [ 1 -1 -1 ...  0  0  0]\n ...\n [ 1 -1  1 ...  0  0  0]\n [ 1 -1 -1 ...  0  0  0]\n [ 1 -1  1 ...  0  0  0]]\nClass distribution:\nClass 0: 900 samples\nClass 1: 900 samples\nClass 2: 900 samples\nClass 3: 900 samples\nClass 4: 900 samples\nClass 5: 900 samples\nClass 6: 900 samples\nClass 7: 900 samples\nClass 8: 900 samples\nClass 9: 900 samples\nClass 10: 900 samples\nClass 11: 900 samples\nClass 12: 900 samples\nClass 13: 900 samples\nClass 14: 900 samples\nClass 15: 900 samples\nClass 16: 900 samples\nClass 17: 900 samples\nClass 18: 900 samples\nClass 19: 900 samples\nClass 20: 900 samples\nClass 21: 900 samples\nClass 22: 900 samples\nClass 23: 900 samples\nClass 24: 900 samples\nClass 25: 900 samples\nClass 26: 900 samples\nClass 27: 900 samples\nClass 28: 900 samples\nClass 29: 900 samples\nClass 30: 900 samples\nClass 31: 900 samples\nClass 32: 900 samples\nClass 33: 900 samples\nClass 34: 900 samples\nClass 35: 900 samples\nClass 36: 900 samples\nClass 37: 900 samples\nClass 38: 900 samples\nClass 39: 900 samples\nClass 40: 900 samples\nClass 41: 900 samples\nClass 42: 900 samples\nClass 43: 900 samples\nClass 44: 900 samples\nClass 45: 900 samples\nClass 46: 900 samples\nClass 47: 900 samples\nClass 48: 900 samples\nClass 49: 900 samples\nClass 50: 900 samples\nClass 51: 900 samples\nClass 52: 900 samples\nClass 53: 900 samples\nClass 54: 900 samples\nClass 55: 900 samples\nClass 56: 900 samples\nClass 57: 900 samples\nClass 58: 900 samples\nClass 59: 900 samples\nClass 60: 900 samples\nClass 61: 900 samples\nClass 62: 900 samples\nClass 63: 900 samples\nClass 64: 900 samples\nClass 65: 900 samples\nClass 66: 900 samples\nClass 67: 900 samples\nClass 68: 900 samples\nClass 69: 900 samples\nClass 70: 900 samples\nClass 71: 900 samples\nClass 72: 900 samples\nClass 73: 900 samples\nClass 74: 900 samples\nClass 75: 900 samples\nClass 76: 900 samples\nClass 77: 900 samples\nClass 78: 900 samples\nClass 79: 900 samples\nClass 80: 900 samples\nClass 81: 900 samples\nClass 82: 900 samples\nClass 83: 900 samples\nClass 84: 900 samples\nClass 85: 900 samples\nClass 86: 900 samples\nClass 87: 900 samples\nClass 88: 900 samples\nClass 89: 900 samples\nClass 90: 900 samples\nClass 91: 900 samples\nClass 92: 900 samples\nClass 93: 900 samples\nClass 94: 900 samples\nClass 95: 900 samples\nClass 96: 900 samples\nClass 97: 900 samples\nClass 98: 900 samples\nClass 99: 900 samples\n\nTraining on Fold 1/5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Model built successfully.\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1713154744.970550     109 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1713154744.999057     109 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1713154828.938835     110 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1125/1125 - 112s - 99ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 2/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713154832.508894     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1125/1125 - 84s - 75ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 3/25\n1125/1125 - 85s - 75ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 4/25\n1125/1125 - 85s - 75ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 5/25\n1125/1125 - 85s - 75ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 6/25\n1125/1125 - 85s - 75ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 7/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 8/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 9/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 10/25\n1125/1125 - 85s - 75ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 11/25\n1125/1125 - 142s - 126ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 12/25\n1125/1125 - 85s - 75ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 13/25\n1125/1125 - 85s - 75ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 14/25\n1125/1125 - 85s - 75ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 15/25\n1125/1125 - 85s - 75ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 16/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 17/25\n1125/1125 - 85s - 75ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 18/25\n1125/1125 - 142s - 126ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 19/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 20/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 21/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 22/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 23/25\n1125/1125 - 85s - 75ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 24/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 25/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713156989.076013     109 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"563/563 - 4s - 8ms/step - accuracy: 0.0100 - loss: nan\n\nFold 1 - Test Accuracy: 1.00%\n\nTraining on Fold 2/5\nModel built successfully.\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713157013.537215     109 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1713157099.107486     108 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1125/1125 - 102s - 91ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 2/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713157101.758393     108 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1125/1125 - 85s - 75ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 3/25\n1125/1125 - 85s - 75ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 4/25\n1125/1125 - 85s - 75ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 5/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 6/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 7/25\n1125/1125 - 85s - 75ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 8/25\n1125/1125 - 85s - 75ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 9/25\n1125/1125 - 84s - 75ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 10/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 11/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 12/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 13/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 14/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 15/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 16/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 17/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 18/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 19/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 20/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 21/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 22/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 23/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 24/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 25/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713159147.986949     109 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"563/563 - 3s - 6ms/step - accuracy: 0.0100 - loss: nan\n\nFold 2 - Test Accuracy: 1.00%\n\nTraining on Fold 3/5\nModel built successfully.\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713159172.422599     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1713159258.243693     108 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1125/1125 - 102s - 91ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 2/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713159260.897346     108 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 3/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 4/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 5/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 6/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 7/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 8/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 9/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 10/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 11/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 12/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 13/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 14/25\n1125/1125 - 142s - 126ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 15/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 16/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 17/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 18/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 19/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 20/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 21/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 22/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 23/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 24/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 25/25\n1125/1125 - 86s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713161363.819227     110 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"563/563 - 3s - 5ms/step - accuracy: 0.0100 - loss: nan\n\nFold 3 - Test Accuracy: 1.00%\n\nTraining on Fold 4/5\nModel built successfully.\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713161388.613047     109 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1713161474.046478     110 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1125/1125 - 102s - 91ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 2/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713161476.669141     110 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 3/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 4/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 5/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 6/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 7/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 8/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 9/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 10/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 11/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 12/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 13/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 14/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 15/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 16/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 17/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 18/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 19/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 20/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 21/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 22/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 23/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 24/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 25/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713163521.504061     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"563/563 - 3s - 5ms/step - accuracy: 0.0100 - loss: nan\n\nFold 4 - Test Accuracy: 1.00%\n\nTraining on Fold 5/5\nModel built successfully.\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713163545.361165     110 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1713163631.040838     108 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1125/1125 - 102s - 90ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 2/25\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713163633.726943     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 3/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 4/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 5/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 6/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 7/25\n1125/1125 - 142s - 126ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 8/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 9/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 10/25\n1125/1125 - 142s - 126ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 11/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 12/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 13/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 14/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 15/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 16/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 17/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 18/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 19/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 20/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 21/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 22/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 23/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 24/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\nEpoch 25/25\n1125/1125 - 85s - 76ms/step - accuracy: 0.0100 - loss: nan - val_accuracy: 0.0100 - val_loss: nan\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1713165795.379705     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"563/563 - 3s - 5ms/step - accuracy: 0.0100 - loss: nan\n\nFold 5 - Test Accuracy: 1.00%\nTrained model saved successfully at: /kaggle/working/DFNet_OpenWorld_WalkieTalkie.keras\n","output_type":"stream"}]}]}