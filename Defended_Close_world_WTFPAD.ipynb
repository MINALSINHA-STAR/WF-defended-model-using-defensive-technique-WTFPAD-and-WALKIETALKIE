{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 7762823,
          "sourceType": "datasetVersion",
          "datasetId": 4540211
        }
      ],
      "dockerImageVersionId": 30664,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Defended_Close_world_WTFPAD",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'dataset-defended-close-world:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4540211%2F7762823%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240331%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240331T055132Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0371dc26f063aae4d7002482cf9c57635ef14b342c13b7e4fb19e503fe62c65b774d8c66f7c1a8d2850271410a1787c60f8ae655b0517337c473eeead3183846d5986bc81a3e9a72396df3fb32a7a6338b881343163d2d57ed532ce250e9da5b0f886cace5b3438e7b88c08107f873cc04326ff6a5f5a68807efbb8f2560bc438156c3971a58240ae3872024a6cbf02914307e8730d531843fa8fdccb281d88adfd6ad639c7cca3f59c787005ea9f9900548594e5770ccb565bf1596cf15e1b6f4f1a22f58f78dbe322903fc110617d9bf1229baf921a533558b3ed40d1291ad6cd0b9d684150e99ed16362be7a82220d190c10658f99aeb46d5bfca1a72c63a'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "XdTS5PexEEFz"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
        "from keras.layers import Activation, Flatten, Dense, Dropout, ReLU\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from keras import layers, models\n",
        "\n",
        "def LoadDataWTFPADCW():\n",
        "\n",
        "    print(\"Loading non-defended dataset for closed-world scenario\")\n",
        "\n",
        "    # Point to the directory storing data\n",
        "    dataset_dir = '/kaggle/input/dataset-defended-close-world/'\n",
        "    # X represents a sequence of traffic directions\n",
        "    # y represents a sequence of corresponding label (website's label)\n",
        "    # Debug: Print dataset directory\n",
        "    print(\"Dataset directory:\", dataset_dir)\n",
        "\n",
        "    try:\n",
        "        # Load training data\n",
        "        with open(dataset_dir + 'X_train_WTFPAD.pkl', 'rb') as handle:\n",
        "            X_train = np.array(pickle.load(handle, encoding='latin1'))\n",
        "        print(\"X_train loaded\")\n",
        "\n",
        "        with open(dataset_dir + 'y_train_WTFPAD.pkl', 'rb') as handle:\n",
        "            y_train = np.array(pickle.load(handle, encoding='latin1'))\n",
        "        print(\"y_train loaded\")\n",
        "\n",
        "        # Load validation data\n",
        "        with open(dataset_dir + 'X_valid_WTFPAD.pkl', 'rb') as handle:\n",
        "            X_valid = np.array(pickle.load(handle, encoding='latin1'))\n",
        "        print(\"X_valid loaded\")\n",
        "\n",
        "        with open(dataset_dir + 'y_valid_WTFPAD.pkl', 'rb') as handle:\n",
        "            y_valid = np.array(pickle.load(handle, encoding='latin1'))\n",
        "        print(\"y_valid loaded\")\n",
        "\n",
        "        # Load testing data\n",
        "        with open(dataset_dir + 'X_test_WTFPAD.pkl', 'rb') as handle:\n",
        "            X_test = np.array(pickle.load(handle, encoding='latin1'))\n",
        "        print(\"X_test loaded\")\n",
        "\n",
        "        with open(dataset_dir + 'y_test_WTFPAD.pkl', 'rb') as handle:\n",
        "            y_test = np.array(pickle.load(handle, encoding='latin1'))\n",
        "        print(\"y_test loaded\")\n",
        "\n",
        "        print(\"Data dimensions:\")\n",
        "        print(\"X: Training data's shape : \", X_train.shape)\n",
        "        print(\"y: Training data's shape : \", y_train.shape)\n",
        "        print(\"X: Validation data's shape : \", X_valid.shape)\n",
        "        print(\"y: Validation data's shape : \", y_valid.shape)\n",
        "        print(\"X: Testing data's shape : \", X_test.shape)\n",
        "        print(\"y: Testing data's shape : \", y_test.shape)\n",
        "\n",
        "        # Merge datasets\n",
        "        X_all = np.concatenate((X_train, X_valid, X_test), axis=0)\n",
        "        y_all = np.concatenate((y_train, y_valid, y_test), axis=0)\n",
        "\n",
        "        print(\"Merged data dimensions:\")\n",
        "        print(\"X: Merged data's shape : \", X_all.shape)\n",
        "        print(\"y: Merged data's shape : \", y_all.shape)\n",
        "\n",
        "        # Print features of the merged dataset\n",
        "        print(\"Features of the merged dataset:\")\n",
        "        print(X_all)\n",
        "\n",
        "        # Check if the class distribution is balanced\n",
        "        unique_classes, class_counts = np.unique(y_all, return_counts=True)\n",
        "        class_distribution = dict(zip(unique_classes, class_counts))\n",
        "\n",
        "        print(\"Class distribution:\")\n",
        "        for class_label, count in class_distribution.items():\n",
        "            print(f\"Class {class_label}: {count} samples\")\n",
        "\n",
        "        # Plot the class distribution\n",
        "       # plt.bar(class_distribution.keys(), class_distribution.values())\n",
        "       # plt.xlabel('Class Label')\n",
        "       # plt.ylabel('Number of Samples')\n",
        "      #  plt.title('Class Distribution')\n",
        "      #  plt.show()\n",
        "\n",
        "        return X_all, y_all\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", str(e))\n",
        "        return None\n",
        "\n",
        "# Call the function to load and merge data\n",
        "X_all, y_all = LoadDataWTFPADCW()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-29T03:59:38.496152Z",
          "iopub.execute_input": "2024-03-29T03:59:38.496911Z",
          "iopub.status.idle": "2024-03-29T04:00:22.123006Z",
          "shell.execute_reply.started": "2024-03-29T03:59:38.496879Z",
          "shell.execute_reply": "2024-03-29T04:00:22.12201Z"
        },
        "trusted": true,
        "id": "KQbfH8OQEEF3",
        "outputId": "8ba36818-747c-488a-d2f1-4d4c8963c40e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-03-29 03:59:40.268322: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-29 03:59:40.268447: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-29 03:59:40.397729: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Loading non-defended dataset for closed-world scenario\nDataset directory: /kaggle/input/dataset-defended-close-world/\nX_train loaded\ny_train loaded\nX_valid loaded\ny_valid loaded\nX_test loaded\ny_test loaded\nData dimensions:\nX: Training data's shape :  (76000, 5000)\ny: Training data's shape :  (76000,)\nX: Validation data's shape :  (9500, 5000)\ny: Validation data's shape :  (9500,)\nX: Testing data's shape :  (9500, 5000)\ny: Testing data's shape :  (9500,)\nMerged data dimensions:\nX: Merged data's shape :  (95000, 5000)\ny: Merged data's shape :  (95000,)\nFeatures of the merged dataset:\n[[ 1. -1.  1. ... -1. -1.  1.]\n [-1.  1. -1. ...  0.  0.  0.]\n [ 1.  1.  1. ...  0.  0.  0.]\n ...\n [-1.  1. -1. ...  0.  0.  0.]\n [-1.  1. -1. ...  0.  0.  0.]\n [ 1.  1. -1. ...  0.  0.  0.]]\nClass distribution:\nClass 0: 1000 samples\nClass 1: 1000 samples\nClass 2: 1000 samples\nClass 3: 1000 samples\nClass 4: 1000 samples\nClass 5: 1000 samples\nClass 6: 1000 samples\nClass 7: 1000 samples\nClass 8: 1000 samples\nClass 9: 1000 samples\nClass 10: 1000 samples\nClass 11: 1000 samples\nClass 12: 1000 samples\nClass 13: 1000 samples\nClass 14: 1000 samples\nClass 15: 1000 samples\nClass 16: 1000 samples\nClass 17: 1000 samples\nClass 18: 1000 samples\nClass 19: 1000 samples\nClass 20: 1000 samples\nClass 21: 1000 samples\nClass 22: 1000 samples\nClass 23: 1000 samples\nClass 24: 1000 samples\nClass 25: 1000 samples\nClass 26: 1000 samples\nClass 27: 1000 samples\nClass 28: 1000 samples\nClass 29: 1000 samples\nClass 30: 1000 samples\nClass 31: 1000 samples\nClass 32: 1000 samples\nClass 33: 1000 samples\nClass 34: 1000 samples\nClass 35: 1000 samples\nClass 36: 1000 samples\nClass 37: 1000 samples\nClass 38: 1000 samples\nClass 39: 1000 samples\nClass 40: 1000 samples\nClass 41: 1000 samples\nClass 42: 1000 samples\nClass 43: 1000 samples\nClass 44: 1000 samples\nClass 45: 1000 samples\nClass 46: 1000 samples\nClass 47: 1000 samples\nClass 48: 1000 samples\nClass 49: 1000 samples\nClass 50: 1000 samples\nClass 51: 1000 samples\nClass 52: 1000 samples\nClass 53: 1000 samples\nClass 54: 1000 samples\nClass 55: 1000 samples\nClass 56: 1000 samples\nClass 57: 1000 samples\nClass 58: 1000 samples\nClass 59: 1000 samples\nClass 60: 1000 samples\nClass 61: 1000 samples\nClass 62: 1000 samples\nClass 63: 1000 samples\nClass 64: 1000 samples\nClass 65: 1000 samples\nClass 66: 1000 samples\nClass 67: 1000 samples\nClass 68: 1000 samples\nClass 69: 1000 samples\nClass 70: 1000 samples\nClass 71: 1000 samples\nClass 72: 1000 samples\nClass 73: 1000 samples\nClass 74: 1000 samples\nClass 75: 1000 samples\nClass 76: 1000 samples\nClass 77: 1000 samples\nClass 78: 1000 samples\nClass 79: 1000 samples\nClass 80: 1000 samples\nClass 81: 1000 samples\nClass 82: 1000 samples\nClass 83: 1000 samples\nClass 84: 1000 samples\nClass 85: 1000 samples\nClass 86: 1000 samples\nClass 87: 1000 samples\nClass 88: 1000 samples\nClass 89: 1000 samples\nClass 90: 1000 samples\nClass 91: 1000 samples\nClass 92: 1000 samples\nClass 93: 1000 samples\nClass 94: 1000 samples\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#inplace of adam i want to use adamax"
      ],
      "metadata": {
        "id": "ms63kbldEEF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
        "from keras.layers import Activation, Flatten, Dense, Dropout\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.optimizers import Adamax  # Change here\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "\n",
        "class DFNet:\n",
        "    @staticmethod\n",
        "    def build(input_shape, classes, dropout_rate1=0.3, dropout_rate2=0.5, dropout_rate_fc=0.7):\n",
        "        model = Sequential()\n",
        "\n",
        "        # ... (previous model definition remains unchanged)\n",
        "        # Block 1\n",
        "        filter_num = [None,32, 64, 128, 256]\n",
        "        kernel_size = [None, 8, 8, 8, 8]\n",
        "        conv_stride_size = [None, 1, 1, 1, 1]\n",
        "        pool_stride_size = [None, 4, 4, 4, 4]\n",
        "        pool_size = [None, 8, 8, 8, 8]\n",
        "\n",
        "        for i in range(1, 4):\n",
        "            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n",
        "                             strides=conv_stride_size[i], padding='same',\n",
        "                             input_shape=input_shape if i == 1 else (None, input_shape[1]),\n",
        "                             name=f'block{i}_conv1'))\n",
        "            model.add(BatchNormalization(name=f'batch_normalization_{i * 2 - 1}'))\n",
        "            model.add(Activation('relu', name=f'block{i}_adv_act1'))\n",
        "            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n",
        "                             strides=conv_stride_size[i], padding='same',\n",
        "                             name=f'block{i}_conv2'))\n",
        "            model.add(BatchNormalization(name=f'batch_normalization_{i * 2}'))\n",
        "            model.add(Activation('relu', name=f'block{i}_adv_act2'))\n",
        "\n",
        "            model.add(MaxPooling1D(pool_size=pool_size[i], strides=pool_stride_size[i],\n",
        "                                   padding='same', name=f'block{i}_pool'))\n",
        "            model.add(Dropout(0.1, name=f'block{i}_dropout'))\n",
        "\n",
        "        # ... (rest of the model remains unchanged)\n",
        "\n",
        "        model.add(Flatten(name='flatten'))\n",
        "\n",
        "        # Fully connected layers\n",
        "        for i in range(1, 3):\n",
        "            model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name=f'fc{i}'))\n",
        "            model.add(BatchNormalization(name=f'batch_normalization_{i + 8}'))\n",
        "            model.add(Activation('relu', name=f'fc{i}_act'))\n",
        "\n",
        "            model.add(Dropout(dropout_rate_fc, name=f'fc{i}_dropout'))\n",
        "\n",
        "        # Output layer\n",
        "        model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='fc_final'))\n",
        "        model.add(Activation('softmax', name=\"softmax\"))\n",
        "\n",
        "        print(\"Model built successfully.\")\n",
        "        return model\n",
        "\n",
        "# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\n",
        "input_shape = (5000, 1)\n",
        "num_classes = 95\n",
        "\n",
        "# Experiment with different dropout rates\n",
        "dropout_rate1 = 0.6\n",
        "dropout_rate2 = 0.5\n",
        "dropout_rate_fc = 0.4\n",
        "\n",
        "# Build the model with the specified dropout rates\n",
        "model = DFNet.build(input_shape=input_shape, classes=num_classes,\n",
        "                    dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n",
        "\n",
        "\n",
        "# Print the model summary\n",
        "print(\"Model Summary:\")\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adamax(learning_rate=0.002)  # Change here\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\n",
        "input_shape = (5000, 1)\n",
        "num_classes = 95\n",
        "\n",
        "# Experiment with different dropout rates\n",
        "dropout_rate1 = 0.6\n",
        "dropout_rate2 = 0.5\n",
        "dropout_rate_fc = 0.4\n",
        "\n",
        "# Load and merge data using the function you defined\n",
        "X_all, y_all =   LoadDataWTFPADCW()\n",
        "# Define the number of folds\n",
        "n_splits = 5  # You can adjust this as needed\n",
        "\n",
        "# Initialize Stratified K-Fold\n",
        "stratkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Lists to store results\n",
        "all_histories = []\n",
        "\n",
        "# Iterate over folds\n",
        "for fold, (train_index, test_index) in enumerate(stratkf.split(X_all, y_all)):\n",
        "    print(f\"\\nTraining on Fold {fold + 1}/{n_splits}\")\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test = X_all[train_index], X_all[test_index]\n",
        "    y_train, y_test = y_all[train_index], y_all[test_index]\n",
        "\n",
        "        # Build the model\n",
        "    model = DFNet.build(input_shape=input_shape, classes=num_classes,\n",
        "                        dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n",
        "\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "     # Define early stopping callback\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    history = model.fit(X_train, y_train, epochs=25, batch_size=64,\n",
        "                        validation_data=(X_test, y_test), verbose=2, callbacks=[early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Train the model\n",
        "    #history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
        "    print(f\"\\nFold {fold + 1} - Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Save history for later analysis if needed\n",
        "    all_histories.append(history)\n",
        "    # Check unique labels in y_train\n",
        "\n",
        "    # Update num_classes based on the actual number of unique classes in your dataset\n",
        "#num_classes = len(np.unique(y_all))\n",
        "saved_path_keras = '/kaggle/working/DFNet_OpenWorld_WTFPAD.keras'\n",
        "model.save(saved_path_keras)\n",
        "\n",
        "print(\"Trained model saved successfully at:\", saved_path_keras)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-29T04:00:22.125271Z",
          "iopub.execute_input": "2024-03-29T04:00:22.125846Z",
          "iopub.status.idle": "2024-03-29T07:19:56.681378Z",
          "shell.execute_reply.started": "2024-03-29T04:00:22.125818Z",
          "shell.execute_reply": "2024-03-29T07:19:56.680437Z"
        },
        "trusted": true,
        "id": "8NXLlzzvEEF8",
        "outputId": "2d03fd2e-906e-41a0-bd65-4f04e03086ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Model built successfully.\nModel Summary:\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1mModel: \"sequential\"\u001b[0m\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ block1_conv1 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m288\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act1 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_conv2 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m8,224\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act2 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_pool (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv1 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m16,448\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act1 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv2 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m32,832\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act2 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_pool (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv1 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m65,664\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act1 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv2 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m131,200\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act2 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_pool (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10112\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m5,177,856\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_act (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_dropout (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_act (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_dropout (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc_final (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m95\u001b[0m)             │        \u001b[38;5;34m48,735\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ softmax (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m95\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ block1_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,224</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10112</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,177,856</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_act (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_act (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc_final (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">95</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">48,735</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">95</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,749,791\u001b[0m (21.93 MB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,749,791</span> (21.93 MB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,746,847\u001b[0m (21.92 MB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,746,847</span> (21.92 MB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,944\u001b[0m (11.50 KB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,944</span> (11.50 KB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Loading non-defended dataset for closed-world scenario\nDataset directory: /kaggle/input/dataset-defended-close-world/\nX_train loaded\ny_train loaded\nX_valid loaded\ny_valid loaded\nX_test loaded\ny_test loaded\nData dimensions:\nX: Training data's shape :  (76000, 5000)\ny: Training data's shape :  (76000,)\nX: Validation data's shape :  (9500, 5000)\ny: Validation data's shape :  (9500,)\nX: Testing data's shape :  (9500, 5000)\ny: Testing data's shape :  (9500,)\nMerged data dimensions:\nX: Merged data's shape :  (95000, 5000)\ny: Merged data's shape :  (95000,)\nFeatures of the merged dataset:\n[[ 1. -1.  1. ... -1. -1.  1.]\n [-1.  1. -1. ...  0.  0.  0.]\n [ 1.  1.  1. ...  0.  0.  0.]\n ...\n [-1.  1. -1. ...  0.  0.  0.]\n [-1.  1. -1. ...  0.  0.  0.]\n [ 1.  1. -1. ...  0.  0.  0.]]\nClass distribution:\nClass 0: 1000 samples\nClass 1: 1000 samples\nClass 2: 1000 samples\nClass 3: 1000 samples\nClass 4: 1000 samples\nClass 5: 1000 samples\nClass 6: 1000 samples\nClass 7: 1000 samples\nClass 8: 1000 samples\nClass 9: 1000 samples\nClass 10: 1000 samples\nClass 11: 1000 samples\nClass 12: 1000 samples\nClass 13: 1000 samples\nClass 14: 1000 samples\nClass 15: 1000 samples\nClass 16: 1000 samples\nClass 17: 1000 samples\nClass 18: 1000 samples\nClass 19: 1000 samples\nClass 20: 1000 samples\nClass 21: 1000 samples\nClass 22: 1000 samples\nClass 23: 1000 samples\nClass 24: 1000 samples\nClass 25: 1000 samples\nClass 26: 1000 samples\nClass 27: 1000 samples\nClass 28: 1000 samples\nClass 29: 1000 samples\nClass 30: 1000 samples\nClass 31: 1000 samples\nClass 32: 1000 samples\nClass 33: 1000 samples\nClass 34: 1000 samples\nClass 35: 1000 samples\nClass 36: 1000 samples\nClass 37: 1000 samples\nClass 38: 1000 samples\nClass 39: 1000 samples\nClass 40: 1000 samples\nClass 41: 1000 samples\nClass 42: 1000 samples\nClass 43: 1000 samples\nClass 44: 1000 samples\nClass 45: 1000 samples\nClass 46: 1000 samples\nClass 47: 1000 samples\nClass 48: 1000 samples\nClass 49: 1000 samples\nClass 50: 1000 samples\nClass 51: 1000 samples\nClass 52: 1000 samples\nClass 53: 1000 samples\nClass 54: 1000 samples\nClass 55: 1000 samples\nClass 56: 1000 samples\nClass 57: 1000 samples\nClass 58: 1000 samples\nClass 59: 1000 samples\nClass 60: 1000 samples\nClass 61: 1000 samples\nClass 62: 1000 samples\nClass 63: 1000 samples\nClass 64: 1000 samples\nClass 65: 1000 samples\nClass 66: 1000 samples\nClass 67: 1000 samples\nClass 68: 1000 samples\nClass 69: 1000 samples\nClass 70: 1000 samples\nClass 71: 1000 samples\nClass 72: 1000 samples\nClass 73: 1000 samples\nClass 74: 1000 samples\nClass 75: 1000 samples\nClass 76: 1000 samples\nClass 77: 1000 samples\nClass 78: 1000 samples\nClass 79: 1000 samples\nClass 80: 1000 samples\nClass 81: 1000 samples\nClass 82: 1000 samples\nClass 83: 1000 samples\nClass 84: 1000 samples\nClass 85: 1000 samples\nClass 86: 1000 samples\nClass 87: 1000 samples\nClass 88: 1000 samples\nClass 89: 1000 samples\nClass 90: 1000 samples\nClass 91: 1000 samples\nClass 92: 1000 samples\nClass 93: 1000 samples\nClass 94: 1000 samples\n\nTraining on Fold 1/5\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Model built successfully.\nEpoch 1/25\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1711684858.737408     106 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1711684858.766865     106 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1711684961.773319     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1711684963.314886     105 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1188/1188 - 134s - 112ms/step - accuracy: 0.4038 - loss: 2.2216 - val_accuracy: 0.6335 - val_loss: 1.3392\nEpoch 2/25\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1711684967.805950     104 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1188/1188 - 91s - 77ms/step - accuracy: 0.6375 - loss: 1.2921 - val_accuracy: 0.7486 - val_loss: 0.9027\nEpoch 3/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.7130 - loss: 1.0191 - val_accuracy: 0.8032 - val_loss: 0.7257\nEpoch 4/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.7530 - loss: 0.8668 - val_accuracy: 0.7750 - val_loss: 0.8054\nEpoch 5/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.7823 - loss: 0.7560 - val_accuracy: 0.8053 - val_loss: 0.6957\nEpoch 6/25\n1188/1188 - 142s - 120ms/step - accuracy: 0.8044 - loss: 0.6745 - val_accuracy: 0.8378 - val_loss: 0.6105\nEpoch 7/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8231 - loss: 0.6033 - val_accuracy: 0.8256 - val_loss: 0.6284\nEpoch 8/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8404 - loss: 0.5446 - val_accuracy: 0.8593 - val_loss: 0.5254\nEpoch 9/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8508 - loss: 0.5052 - val_accuracy: 0.8602 - val_loss: 0.5140\nEpoch 10/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8591 - loss: 0.4705 - val_accuracy: 0.8562 - val_loss: 0.5393\nEpoch 11/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8706 - loss: 0.4311 - val_accuracy: 0.8665 - val_loss: 0.5005\nEpoch 12/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8784 - loss: 0.4050 - val_accuracy: 0.8728 - val_loss: 0.4837\nEpoch 13/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8831 - loss: 0.3834 - val_accuracy: 0.8628 - val_loss: 0.5401\nEpoch 14/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8898 - loss: 0.3597 - val_accuracy: 0.8755 - val_loss: 0.5018\nEpoch 15/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8951 - loss: 0.3420 - val_accuracy: 0.8762 - val_loss: 0.4752\nEpoch 16/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8989 - loss: 0.3291 - val_accuracy: 0.8800 - val_loss: 0.4682\nEpoch 17/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9039 - loss: 0.3132 - val_accuracy: 0.8750 - val_loss: 0.5078\nEpoch 18/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9108 - loss: 0.2906 - val_accuracy: 0.8765 - val_loss: 0.4970\nEpoch 19/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9106 - loss: 0.2842 - val_accuracy: 0.8813 - val_loss: 0.4932\nEpoch 20/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9157 - loss: 0.2709 - val_accuracy: 0.8817 - val_loss: 0.4920\nEpoch 21/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9181 - loss: 0.2626 - val_accuracy: 0.8797 - val_loss: 0.5087\nEpoch 22/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9200 - loss: 0.2567 - val_accuracy: 0.8668 - val_loss: 0.5867\nEpoch 23/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9226 - loss: 0.2451 - val_accuracy: 0.8770 - val_loss: 0.5135\nEpoch 24/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9286 - loss: 0.2290 - val_accuracy: 0.8783 - val_loss: 0.5307\nEpoch 25/25\n1188/1188 - 142s - 120ms/step - accuracy: 0.9300 - loss: 0.2235 - val_accuracy: 0.8833 - val_loss: 0.4999\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1711687261.318317     104 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "594/594 - 5s - 8ms/step - accuracy: 0.8800 - loss: 0.4683\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1711687265.482603     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nFold 1 - Test Accuracy: 88.00%\n\nTraining on Fold 2/5\nModel built successfully.\nEpoch 1/25\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1711687285.165374     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1711687383.105198     105 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1711687384.396016     104 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1188/1188 - 116s - 98ms/step - accuracy: 0.4187 - loss: 2.1792 - val_accuracy: 0.6476 - val_loss: 1.2664\nEpoch 2/25\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1711687387.222521     105 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1188/1188 - 92s - 77ms/step - accuracy: 0.6492 - loss: 1.2545 - val_accuracy: 0.7557 - val_loss: 0.8799\nEpoch 3/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.7187 - loss: 0.9908 - val_accuracy: 0.8001 - val_loss: 0.7289\nEpoch 4/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.7589 - loss: 0.8423 - val_accuracy: 0.8196 - val_loss: 0.6474\nEpoch 5/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.7884 - loss: 0.7301 - val_accuracy: 0.8314 - val_loss: 0.6088\nEpoch 6/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8119 - loss: 0.6498 - val_accuracy: 0.8402 - val_loss: 0.5738\nEpoch 7/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8251 - loss: 0.5931 - val_accuracy: 0.8520 - val_loss: 0.5519\nEpoch 8/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8419 - loss: 0.5348 - val_accuracy: 0.8439 - val_loss: 0.5593\nEpoch 9/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8534 - loss: 0.4938 - val_accuracy: 0.8609 - val_loss: 0.5183\nEpoch 10/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8626 - loss: 0.4575 - val_accuracy: 0.8584 - val_loss: 0.5261\nEpoch 11/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8725 - loss: 0.4234 - val_accuracy: 0.8535 - val_loss: 0.5391\nEpoch 12/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8788 - loss: 0.3965 - val_accuracy: 0.8625 - val_loss: 0.5425\nEpoch 13/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8867 - loss: 0.3724 - val_accuracy: 0.8571 - val_loss: 0.5457\nEpoch 14/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8931 - loss: 0.3536 - val_accuracy: 0.8672 - val_loss: 0.5349\nEpoch 15/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8972 - loss: 0.3353 - val_accuracy: 0.8684 - val_loss: 0.5243\nEpoch 16/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9026 - loss: 0.3161 - val_accuracy: 0.8744 - val_loss: 0.4917\nEpoch 17/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9079 - loss: 0.2982 - val_accuracy: 0.8735 - val_loss: 0.4990\nEpoch 18/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9102 - loss: 0.2903 - val_accuracy: 0.8751 - val_loss: 0.4969\nEpoch 19/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9143 - loss: 0.2745 - val_accuracy: 0.8756 - val_loss: 0.5051\nEpoch 20/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9175 - loss: 0.2652 - val_accuracy: 0.8810 - val_loss: 0.4911\nEpoch 21/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9214 - loss: 0.2532 - val_accuracy: 0.8779 - val_loss: 0.4969\nEpoch 22/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9231 - loss: 0.2489 - val_accuracy: 0.8797 - val_loss: 0.5012\nEpoch 23/25\n1188/1188 - 142s - 119ms/step - accuracy: 0.9252 - loss: 0.2420 - val_accuracy: 0.8807 - val_loss: 0.5027\nEpoch 24/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9272 - loss: 0.2317 - val_accuracy: 0.8762 - val_loss: 0.5351\nEpoch 25/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9308 - loss: 0.2205 - val_accuracy: 0.8788 - val_loss: 0.5148\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1711689632.605342     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "594/594 - 4s - 6ms/step - accuracy: 0.8810 - loss: 0.4912\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1711689635.707188     106 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nFold 2 - Test Accuracy: 88.10%\n\nTraining on Fold 3/5\nModel built successfully.\nEpoch 1/25\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1711689655.303118     105 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1711689753.722826     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1711689755.045890     105 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1188/1188 - 117s - 98ms/step - accuracy: 0.4156 - loss: 2.1850 - val_accuracy: 0.5279 - val_loss: 1.7008\nEpoch 2/25\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1711689757.859502     104 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1188/1188 - 91s - 77ms/step - accuracy: 0.6470 - loss: 1.2599 - val_accuracy: 0.7533 - val_loss: 0.8836\nEpoch 3/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.7177 - loss: 0.9959 - val_accuracy: 0.7898 - val_loss: 0.7686\nEpoch 4/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.7582 - loss: 0.8498 - val_accuracy: 0.8260 - val_loss: 0.6310\nEpoch 5/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.7864 - loss: 0.7409 - val_accuracy: 0.7917 - val_loss: 0.7658\nEpoch 6/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8068 - loss: 0.6640 - val_accuracy: 0.8352 - val_loss: 0.5947\nEpoch 7/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8249 - loss: 0.5987 - val_accuracy: 0.8463 - val_loss: 0.5618\nEpoch 8/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8391 - loss: 0.5459 - val_accuracy: 0.8568 - val_loss: 0.5289\nEpoch 9/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8489 - loss: 0.5099 - val_accuracy: 0.8672 - val_loss: 0.4882\nEpoch 10/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8621 - loss: 0.4610 - val_accuracy: 0.8675 - val_loss: 0.4905\nEpoch 11/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8704 - loss: 0.4301 - val_accuracy: 0.8649 - val_loss: 0.5021\nEpoch 12/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8772 - loss: 0.4026 - val_accuracy: 0.8746 - val_loss: 0.4584\nEpoch 13/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8838 - loss: 0.3853 - val_accuracy: 0.8739 - val_loss: 0.4814\nEpoch 14/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8905 - loss: 0.3583 - val_accuracy: 0.8783 - val_loss: 0.4665\nEpoch 15/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8976 - loss: 0.3350 - val_accuracy: 0.8613 - val_loss: 0.5173\nEpoch 16/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9011 - loss: 0.3217 - val_accuracy: 0.8853 - val_loss: 0.4473\nEpoch 17/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9049 - loss: 0.3079 - val_accuracy: 0.8720 - val_loss: 0.4980\nEpoch 18/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9095 - loss: 0.2950 - val_accuracy: 0.8732 - val_loss: 0.4921\nEpoch 19/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9136 - loss: 0.2765 - val_accuracy: 0.8767 - val_loss: 0.4788\nEpoch 20/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9164 - loss: 0.2672 - val_accuracy: 0.8774 - val_loss: 0.4839\nEpoch 21/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9191 - loss: 0.2614 - val_accuracy: 0.8786 - val_loss: 0.5063\nEpoch 22/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9228 - loss: 0.2448 - val_accuracy: 0.8827 - val_loss: 0.4821\nEpoch 23/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9248 - loss: 0.2405 - val_accuracy: 0.8783 - val_loss: 0.5097\nEpoch 24/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9267 - loss: 0.2342 - val_accuracy: 0.8863 - val_loss: 0.4821\nEpoch 25/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9291 - loss: 0.2261 - val_accuracy: 0.8787 - val_loss: 0.5106\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1711691950.166073     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "594/594 - 4s - 6ms/step - accuracy: 0.8853 - loss: 0.4474\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1711691953.336881     105 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nFold 3 - Test Accuracy: 88.53%\n\nTraining on Fold 4/5\nModel built successfully.\nEpoch 1/25\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1711691973.020846     104 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1711692071.312330     104 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1711692072.645383     106 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1188/1188 - 117s - 98ms/step - accuracy: 0.4267 - loss: 2.1478 - val_accuracy: 0.5550 - val_loss: 1.5786\nEpoch 2/25\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1711692075.493742     105 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1188/1188 - 91s - 77ms/step - accuracy: 0.6507 - loss: 1.2547 - val_accuracy: 0.7239 - val_loss: 0.9750\nEpoch 3/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.7207 - loss: 0.9909 - val_accuracy: 0.8085 - val_loss: 0.6813\nEpoch 4/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.7616 - loss: 0.8395 - val_accuracy: 0.8179 - val_loss: 0.6537\nEpoch 5/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.7885 - loss: 0.7354 - val_accuracy: 0.8249 - val_loss: 0.6195\nEpoch 6/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8084 - loss: 0.6618 - val_accuracy: 0.8482 - val_loss: 0.5352\nEpoch 7/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8263 - loss: 0.5935 - val_accuracy: 0.8467 - val_loss: 0.5333\nEpoch 8/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8380 - loss: 0.5452 - val_accuracy: 0.8611 - val_loss: 0.5100\nEpoch 9/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8535 - loss: 0.4943 - val_accuracy: 0.8678 - val_loss: 0.4934\nEpoch 10/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8614 - loss: 0.4640 - val_accuracy: 0.8566 - val_loss: 0.5257\nEpoch 11/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8708 - loss: 0.4319 - val_accuracy: 0.8631 - val_loss: 0.5005\nEpoch 12/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8787 - loss: 0.4019 - val_accuracy: 0.8686 - val_loss: 0.4825\nEpoch 13/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8850 - loss: 0.3783 - val_accuracy: 0.8727 - val_loss: 0.4825\nEpoch 14/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8911 - loss: 0.3579 - val_accuracy: 0.8622 - val_loss: 0.5079\nEpoch 15/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8958 - loss: 0.3409 - val_accuracy: 0.8742 - val_loss: 0.4993\nEpoch 16/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9015 - loss: 0.3216 - val_accuracy: 0.8782 - val_loss: 0.4720\nEpoch 17/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9063 - loss: 0.3087 - val_accuracy: 0.8829 - val_loss: 0.4488\nEpoch 18/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9087 - loss: 0.2955 - val_accuracy: 0.8794 - val_loss: 0.4717\nEpoch 19/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9118 - loss: 0.2850 - val_accuracy: 0.8856 - val_loss: 0.4590\nEpoch 20/25\n1188/1188 - 142s - 120ms/step - accuracy: 0.9151 - loss: 0.2767 - val_accuracy: 0.8851 - val_loss: 0.4659\nEpoch 21/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9205 - loss: 0.2558 - val_accuracy: 0.8797 - val_loss: 0.4839\nEpoch 22/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9212 - loss: 0.2553 - val_accuracy: 0.8811 - val_loss: 0.4754\nEpoch 23/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9257 - loss: 0.2389 - val_accuracy: 0.8860 - val_loss: 0.4502\nEpoch 24/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.9284 - loss: 0.2306 - val_accuracy: 0.8844 - val_loss: 0.4881\nEpoch 25/25\n1188/1188 - 142s - 119ms/step - accuracy: 0.9291 - loss: 0.2270 - val_accuracy: 0.8848 - val_loss: 0.4806\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1711694370.911345     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "594/594 - 4s - 6ms/step - accuracy: 0.8829 - loss: 0.4490\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1711694374.061063     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nFold 4 - Test Accuracy: 88.29%\n\nTraining on Fold 5/5\nModel built successfully.\nEpoch 1/25\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1711694393.993593     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1711694492.458068     105 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1711694493.771472     106 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1188/1188 - 117s - 99ms/step - accuracy: 0.4218 - loss: 2.1659 - val_accuracy: 0.6690 - val_loss: 1.1887\nEpoch 2/25\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1711694496.605322     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1188/1188 - 92s - 77ms/step - accuracy: 0.6454 - loss: 1.2667 - val_accuracy: 0.7141 - val_loss: 0.9879\nEpoch 3/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.7187 - loss: 1.0017 - val_accuracy: 0.7947 - val_loss: 0.7303\nEpoch 4/25\n1188/1188 - 92s - 77ms/step - accuracy: 0.7573 - loss: 0.8505 - val_accuracy: 0.8097 - val_loss: 0.6653\nEpoch 5/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.7868 - loss: 0.7414 - val_accuracy: 0.8358 - val_loss: 0.5891\nEpoch 6/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8077 - loss: 0.6674 - val_accuracy: 0.8341 - val_loss: 0.5852\nEpoch 7/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8264 - loss: 0.5981 - val_accuracy: 0.8504 - val_loss: 0.5402\nEpoch 8/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8377 - loss: 0.5547 - val_accuracy: 0.8661 - val_loss: 0.4855\nEpoch 9/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8509 - loss: 0.5049 - val_accuracy: 0.8529 - val_loss: 0.5274\nEpoch 10/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8589 - loss: 0.4700 - val_accuracy: 0.8500 - val_loss: 0.5474\nEpoch 11/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8691 - loss: 0.4382 - val_accuracy: 0.8540 - val_loss: 0.5137\nEpoch 12/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8767 - loss: 0.4105 - val_accuracy: 0.8645 - val_loss: 0.4997\nEpoch 13/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8822 - loss: 0.3890 - val_accuracy: 0.8733 - val_loss: 0.4694\nEpoch 14/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8897 - loss: 0.3598 - val_accuracy: 0.8718 - val_loss: 0.4763\nEpoch 15/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.8956 - loss: 0.3435 - val_accuracy: 0.8652 - val_loss: 0.5183\nEpoch 16/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9003 - loss: 0.3262 - val_accuracy: 0.8792 - val_loss: 0.4574\nEpoch 17/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9040 - loss: 0.3133 - val_accuracy: 0.8700 - val_loss: 0.5111\nEpoch 18/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9073 - loss: 0.2985 - val_accuracy: 0.8765 - val_loss: 0.4795\nEpoch 19/25\n1188/1188 - 142s - 120ms/step - accuracy: 0.9122 - loss: 0.2844 - val_accuracy: 0.8877 - val_loss: 0.4380\nEpoch 20/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9169 - loss: 0.2705 - val_accuracy: 0.8823 - val_loss: 0.4639\nEpoch 21/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9185 - loss: 0.2630 - val_accuracy: 0.8756 - val_loss: 0.4850\nEpoch 22/25\n1188/1188 - 142s - 119ms/step - accuracy: 0.9209 - loss: 0.2528 - val_accuracy: 0.8812 - val_loss: 0.4879\nEpoch 23/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9227 - loss: 0.2493 - val_accuracy: 0.8785 - val_loss: 0.5031\nEpoch 24/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9252 - loss: 0.2403 - val_accuracy: 0.8833 - val_loss: 0.4484\nEpoch 25/25\n1188/1188 - 91s - 77ms/step - accuracy: 0.9277 - loss: 0.2305 - val_accuracy: 0.8843 - val_loss: 0.4903\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1711696793.302442     105 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "594/594 - 4s - 6ms/step - accuracy: 0.8877 - loss: 0.4381\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1711696796.425589     105 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nFold 5 - Test Accuracy: 88.77%\nTrained model saved successfully at: /kaggle/working/DFNet_OpenWorld_WTFPAD.keras\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#using adam"
      ],
      "metadata": {
        "id": "Jl3Xan6lEEF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
        "from keras.layers import Activation, Flatten, Dense, Dropout, ReLU\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from keras import layers, models\n",
        "import numpy as np\n",
        "from keras.layers import Activation\n",
        "\n",
        "class DFNet:\n",
        "    @staticmethod\n",
        "    def build(input_shape, classes, dropout_rate1=0.3, dropout_rate2=0.5, dropout_rate_fc=0.7):\n",
        "        model = Sequential()\n",
        "\n",
        "        # ... (previous model definition remains unchanged)\n",
        "        # Block 1\n",
        "        filter_num = [None,32, 64, 128, 256]\n",
        "        kernel_size = [None, 8, 8, 8, 8]\n",
        "        conv_stride_size = [None, 1, 1, 1, 1]\n",
        "        pool_stride_size = [None, 4, 4, 4, 4]\n",
        "        pool_size = [None, 8, 8, 8, 8]\n",
        "\n",
        "        for i in range(1, 4):\n",
        "            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n",
        "                             strides=conv_stride_size[i], padding='same',\n",
        "                             input_shape=input_shape if i == 1 else (None, input_shape[1]),\n",
        "                             name=f'block{i}_conv1'))\n",
        "            model.add(BatchNormalization(name=f'batch_normalization_{i * 2 - 1}'))\n",
        "            model.add(Activation('relu', name=f'block{i}_adv_act1'))\n",
        "            model.add(Conv1D(filters=filter_num[i], kernel_size=kernel_size[i],\n",
        "                             strides=conv_stride_size[i], padding='same',\n",
        "                             name=f'block{i}_conv2'))\n",
        "            model.add(BatchNormalization(name=f'batch_normalization_{i * 2}'))\n",
        "            model.add(Activation('relu', name=f'block{i}_adv_act2'))\n",
        "\n",
        "            model.add(MaxPooling1D(pool_size=pool_size[i], strides=pool_stride_size[i],\n",
        "                                   padding='same', name=f'block{i}_pool'))\n",
        "            model.add(Dropout(0.1, name=f'block{i}_dropout'))\n",
        "\n",
        "        # ... (rest of the model remains unchanged)\n",
        "\n",
        "        model.add(Flatten(name='flatten'))\n",
        "\n",
        "        # Fully connected layers\n",
        "        for i in range(1, 3):\n",
        "            model.add(Dense(512, kernel_initializer=glorot_uniform(seed=0), name=f'fc{i}'))\n",
        "            model.add(BatchNormalization(name=f'batch_normalization_{i + 8}'))\n",
        "            model.add(Activation('relu', name=f'fc{i}_act'))\n",
        "\n",
        "            # Experiment with different dropout rates\n",
        "            model.add(Dropout(dropout_rate_fc, name=f'fc{i}_dropout'))\n",
        "\n",
        "        # Output layer\n",
        "        model.add(Dense(classes, kernel_initializer=glorot_uniform(seed=0), name='fc_final'))\n",
        "        model.add(Activation('softmax', name=\"softmax\"))\n",
        "\n",
        "        print(\"Model built successfully.\")\n",
        "        return model\n",
        "    # Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\n",
        "input_shape = (5000, 1)\n",
        "num_classes = 95\n",
        "\n",
        "# Experiment with different dropout rates\n",
        "dropout_rate1 = 0.6\n",
        "dropout_rate2 = 0.5\n",
        "dropout_rate_fc = 0.4\n",
        "\n",
        "# Build the model with the specified dropout rates\n",
        "model = DFNet.build(input_shape=input_shape, classes=num_classes,\n",
        "                    dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n",
        "\n",
        "\n",
        "# Print the model summary\n",
        "print(\"Model Summary:\")\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Assuming input_shape and num_classes are defined as follows (adjust based on your actual data)\n",
        "input_shape = (5000, 1)\n",
        "num_classes = 95\n",
        "\n",
        "# Experiment with different dropout rates\n",
        "dropout_rate1 = 0.6\n",
        "dropout_rate2 = 0.5\n",
        "dropout_rate_fc = 0.4\n",
        "\n",
        "# Load and merge data using the function you defined\n",
        "X_all, y_all =   LoadDataWTFPADCW()\n",
        "# Define the number of folds\n",
        "n_splits = 5  # You can adjust this as needed\n",
        "\n",
        "# Initialize Stratified K-Fold\n",
        "stratkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Lists to store results\n",
        "all_histories = []\n",
        "\n",
        "# Iterate over folds\n",
        "for fold, (train_index, test_index) in enumerate(stratkf.split(X_all, y_all)):\n",
        "    print(f\"\\nTraining on Fold {fold + 1}/{n_splits}\")\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test = X_all[train_index], X_all[test_index]\n",
        "    y_train, y_test = y_all[train_index], y_all[test_index]\n",
        "\n",
        "        # Build the model\n",
        "    model = DFNet.build(input_shape=input_shape, classes=num_classes,\n",
        "                        dropout_rate1=dropout_rate1, dropout_rate2=dropout_rate2, dropout_rate_fc=dropout_rate_fc)\n",
        "\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "     # Define early stopping callback\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=64,\n",
        "                        validation_data=(X_test, y_test), verbose=2, callbacks=[early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Train the model\n",
        "    #history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
        "    print(f\"\\nFold {fold + 1} - Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Save history for later analysis if needed\n",
        "    all_histories.append(history)\n",
        "    # Check unique labels in y_train\n",
        "\n",
        "    # Update num_classes based on the actual number of unique classes in your dataset\n",
        "#num_classes = len(np.unique(y_all))\n",
        "saved_path_keras = '/kaggle/working/DFNet_OpenWorld_WTFPAD.keras'\n",
        "model.save(saved_path_keras)\n",
        "\n",
        "print(\"Trained model saved successfully at:\", saved_path_keras)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T04:56:16.469148Z",
          "iopub.execute_input": "2024-03-16T04:56:16.470094Z",
          "iopub.status.idle": "2024-03-16T07:38:10.718906Z",
          "shell.execute_reply.started": "2024-03-16T04:56:16.470059Z",
          "shell.execute_reply": "2024-03-16T07:38:10.717837Z"
        },
        "trusted": true,
        "id": "IG3bTRwsEEF_",
        "outputId": "6af7908f-0a24-4926-8c0d-dd797b5b6152"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Model built successfully.\nModel Summary:\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1mModel: \"sequential\"\u001b[0m\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ block1_conv1 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m288\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act1 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_conv2 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m8,224\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act2 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_pool (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv1 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m16,448\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act1 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv2 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m32,832\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act2 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_pool (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv1 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m65,664\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act1 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv2 (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m131,200\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act2 (\u001b[38;5;33mActivation\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_pool (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_dropout (\u001b[38;5;33mDropout\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10112\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m5,177,856\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_act (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_dropout (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_act (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_dropout (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc_final (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m95\u001b[0m)             │        \u001b[38;5;34m48,735\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ softmax (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m95\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ block1_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,224</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_adv_act2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_adv_act2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_adv_act2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10112</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,177,856</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_act (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_act (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc_final (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">95</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">48,735</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">95</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,749,791\u001b[0m (21.93 MB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,749,791</span> (21.93 MB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,746,847\u001b[0m (21.92 MB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,746,847</span> (21.92 MB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,944\u001b[0m (11.50 KB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,944</span> (11.50 KB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Loading non-defended dataset for closed-world scenario\nDataset directory: /kaggle/input/dataset-defended-close-world/\nX_train loaded\ny_train loaded\nX_valid loaded\ny_valid loaded\nX_test loaded\ny_test loaded\nData dimensions:\nX: Training data's shape :  (76000, 5000)\ny: Training data's shape :  (76000,)\nX: Validation data's shape :  (9500, 5000)\ny: Validation data's shape :  (9500,)\nX: Testing data's shape :  (9500, 5000)\ny: Testing data's shape :  (9500,)\nMerged data dimensions:\nX: Merged data's shape :  (95000, 5000)\ny: Merged data's shape :  (95000,)\nFeatures of the merged dataset:\n[[ 1. -1.  1. ... -1. -1.  1.]\n [-1.  1. -1. ...  0.  0.  0.]\n [ 1.  1.  1. ...  0.  0.  0.]\n ...\n [-1.  1. -1. ...  0.  0.  0.]\n [-1.  1. -1. ...  0.  0.  0.]\n [ 1.  1. -1. ...  0.  0.  0.]]\nClass distribution:\nClass 0: 1000 samples\nClass 1: 1000 samples\nClass 2: 1000 samples\nClass 3: 1000 samples\nClass 4: 1000 samples\nClass 5: 1000 samples\nClass 6: 1000 samples\nClass 7: 1000 samples\nClass 8: 1000 samples\nClass 9: 1000 samples\nClass 10: 1000 samples\nClass 11: 1000 samples\nClass 12: 1000 samples\nClass 13: 1000 samples\nClass 14: 1000 samples\nClass 15: 1000 samples\nClass 16: 1000 samples\nClass 17: 1000 samples\nClass 18: 1000 samples\nClass 19: 1000 samples\nClass 20: 1000 samples\nClass 21: 1000 samples\nClass 22: 1000 samples\nClass 23: 1000 samples\nClass 24: 1000 samples\nClass 25: 1000 samples\nClass 26: 1000 samples\nClass 27: 1000 samples\nClass 28: 1000 samples\nClass 29: 1000 samples\nClass 30: 1000 samples\nClass 31: 1000 samples\nClass 32: 1000 samples\nClass 33: 1000 samples\nClass 34: 1000 samples\nClass 35: 1000 samples\nClass 36: 1000 samples\nClass 37: 1000 samples\nClass 38: 1000 samples\nClass 39: 1000 samples\nClass 40: 1000 samples\nClass 41: 1000 samples\nClass 42: 1000 samples\nClass 43: 1000 samples\nClass 44: 1000 samples\nClass 45: 1000 samples\nClass 46: 1000 samples\nClass 47: 1000 samples\nClass 48: 1000 samples\nClass 49: 1000 samples\nClass 50: 1000 samples\nClass 51: 1000 samples\nClass 52: 1000 samples\nClass 53: 1000 samples\nClass 54: 1000 samples\nClass 55: 1000 samples\nClass 56: 1000 samples\nClass 57: 1000 samples\nClass 58: 1000 samples\nClass 59: 1000 samples\nClass 60: 1000 samples\nClass 61: 1000 samples\nClass 62: 1000 samples\nClass 63: 1000 samples\nClass 64: 1000 samples\nClass 65: 1000 samples\nClass 66: 1000 samples\nClass 67: 1000 samples\nClass 68: 1000 samples\nClass 69: 1000 samples\nClass 70: 1000 samples\nClass 71: 1000 samples\nClass 72: 1000 samples\nClass 73: 1000 samples\nClass 74: 1000 samples\nClass 75: 1000 samples\nClass 76: 1000 samples\nClass 77: 1000 samples\nClass 78: 1000 samples\nClass 79: 1000 samples\nClass 80: 1000 samples\nClass 81: 1000 samples\nClass 82: 1000 samples\nClass 83: 1000 samples\nClass 84: 1000 samples\nClass 85: 1000 samples\nClass 86: 1000 samples\nClass 87: 1000 samples\nClass 88: 1000 samples\nClass 89: 1000 samples\nClass 90: 1000 samples\nClass 91: 1000 samples\nClass 92: 1000 samples\nClass 93: 1000 samples\nClass 94: 1000 samples\n\nTraining on Fold 1/5\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Model built successfully.\nEpoch 1/20\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1710565014.557227     104 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1710565014.585482     104 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1710565117.422812     104 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1710565119.482677     104 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1188/1188 - 135s - 114ms/step - accuracy: 0.4262 - loss: 2.1454 - val_accuracy: 0.4718 - val_loss: 1.9703\nEpoch 2/20\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1710565124.139436     105 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1188/1188 - 92s - 77ms/step - accuracy: 0.6474 - loss: 1.2643 - val_accuracy: 0.7596 - val_loss: 0.8677\nEpoch 3/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.7178 - loss: 1.0019 - val_accuracy: 0.7934 - val_loss: 0.7447\nEpoch 4/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.7580 - loss: 0.8473 - val_accuracy: 0.8121 - val_loss: 0.6779\nEpoch 5/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.7869 - loss: 0.7437 - val_accuracy: 0.8434 - val_loss: 0.5765\nEpoch 6/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.8101 - loss: 0.6599 - val_accuracy: 0.8471 - val_loss: 0.5688\nEpoch 7/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.8268 - loss: 0.5893 - val_accuracy: 0.8362 - val_loss: 0.5988\nEpoch 8/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.8419 - loss: 0.5369 - val_accuracy: 0.8279 - val_loss: 0.6330\nEpoch 9/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.8525 - loss: 0.4959 - val_accuracy: 0.8579 - val_loss: 0.5347\nEpoch 10/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.8629 - loss: 0.4567 - val_accuracy: 0.8614 - val_loss: 0.5327\nEpoch 11/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.8724 - loss: 0.4253 - val_accuracy: 0.8726 - val_loss: 0.4801\nEpoch 12/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.8781 - loss: 0.3997 - val_accuracy: 0.8739 - val_loss: 0.4811\nEpoch 13/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.8852 - loss: 0.3769 - val_accuracy: 0.8735 - val_loss: 0.4975\nEpoch 14/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.8921 - loss: 0.3518 - val_accuracy: 0.8719 - val_loss: 0.5025\nEpoch 15/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.8960 - loss: 0.3404 - val_accuracy: 0.8741 - val_loss: 0.4980\nEpoch 16/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.9026 - loss: 0.3176 - val_accuracy: 0.8780 - val_loss: 0.4945\nEpoch 17/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.9070 - loss: 0.3048 - val_accuracy: 0.8800 - val_loss: 0.4800\nEpoch 18/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.9098 - loss: 0.2925 - val_accuracy: 0.8670 - val_loss: 0.5331\nEpoch 19/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.9139 - loss: 0.2795 - val_accuracy: 0.8761 - val_loss: 0.5199\nEpoch 20/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.9176 - loss: 0.2631 - val_accuracy: 0.8842 - val_loss: 0.4909\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1710566871.071842     104 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "594/594 - 5s - 8ms/step - accuracy: 0.8800 - loss: 0.4801\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1710566875.286351     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nFold 1 - Test Accuracy: 88.00%\n\nTraining on Fold 2/5\nModel built successfully.\nEpoch 1/20\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1710566895.616742     104 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1710566993.929612     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1710566995.475736     106 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1188/1188 - 118s - 99ms/step - accuracy: 0.4200 - loss: 2.1692 - val_accuracy: 0.6583 - val_loss: 1.2186\nEpoch 2/20\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1710566998.419396     105 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1188/1188 - 92s - 78ms/step - accuracy: 0.6441 - loss: 1.2647 - val_accuracy: 0.6851 - val_loss: 1.0952\nEpoch 3/20\n1188/1188 - 92s - 78ms/step - accuracy: 0.7157 - loss: 1.0011 - val_accuracy: 0.7738 - val_loss: 0.8208\nEpoch 4/20\n1188/1188 - 92s - 78ms/step - accuracy: 0.7558 - loss: 0.8569 - val_accuracy: 0.8057 - val_loss: 0.6970\nEpoch 5/20\n1188/1188 - 92s - 78ms/step - accuracy: 0.7870 - loss: 0.7393 - val_accuracy: 0.8155 - val_loss: 0.6706\nEpoch 6/20\n1188/1188 - 92s - 78ms/step - accuracy: 0.8079 - loss: 0.6639 - val_accuracy: 0.8463 - val_loss: 0.5598\nEpoch 7/20\n1188/1188 - 92s - 78ms/step - accuracy: 0.8273 - loss: 0.5953 - val_accuracy: 0.8428 - val_loss: 0.5677\nEpoch 8/20\n1188/1188 - 92s - 78ms/step - accuracy: 0.8389 - loss: 0.5451 - val_accuracy: 0.8603 - val_loss: 0.5171\nEpoch 9/20\n1188/1188 - 92s - 78ms/step - accuracy: 0.8520 - loss: 0.4990 - val_accuracy: 0.8570 - val_loss: 0.5317\nEpoch 10/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.8625 - loss: 0.4595 - val_accuracy: 0.8483 - val_loss: 0.5567\nEpoch 11/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.8707 - loss: 0.4310 - val_accuracy: 0.8572 - val_loss: 0.5168\nEpoch 12/20\n1188/1188 - 92s - 78ms/step - accuracy: 0.8786 - loss: 0.4015 - val_accuracy: 0.8611 - val_loss: 0.5280\nEpoch 13/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.8867 - loss: 0.3725 - val_accuracy: 0.8696 - val_loss: 0.5170\nEpoch 14/20\n1188/1188 - 92s - 77ms/step - accuracy: 0.8897 - loss: 0.3613 - val_accuracy: 0.8662 - val_loss: 0.5244\nEpoch 15/20\n1188/1188 - 92s - 78ms/step - accuracy: 0.8973 - loss: 0.3419 - val_accuracy: 0.8716 - val_loss: 0.5095\nEpoch 16/20\n1188/1188 - 92s - 78ms/step - accuracy: 0.9027 - loss: 0.3214 - val_accuracy: 0.8729 - val_loss: 0.4866\nEpoch 17/20\n1188/1188 - 92s - 78ms/step - accuracy: 0.9051 - loss: 0.3060 - val_accuracy: 0.8715 - val_loss: 0.5123\nEpoch 18/20\n1188/1188 - 143s - 120ms/step - accuracy: 0.9095 - loss: 0.2947 - val_accuracy: 0.8765 - val_loss: 0.5071\nEpoch 19/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.9131 - loss: 0.2811 - val_accuracy: 0.8768 - val_loss: 0.5080\nEpoch 20/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.9167 - loss: 0.2702 - val_accuracy: 0.8734 - val_loss: 0.5144\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1710568802.090471     105 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "594/594 - 4s - 6ms/step - accuracy: 0.8729 - loss: 0.4867\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1710568805.328290     105 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nFold 2 - Test Accuracy: 87.29%\n\nTraining on Fold 3/5\nModel built successfully.\nEpoch 1/20\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1710568826.114383     106 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1710568926.150462     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1710568927.751898     106 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1188/1188 - 120s - 101ms/step - accuracy: 0.4283 - loss: 2.1381 - val_accuracy: 0.6582 - val_loss: 1.2341\nEpoch 2/20\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1710568930.730801     104 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1188/1188 - 93s - 78ms/step - accuracy: 0.6501 - loss: 1.2528 - val_accuracy: 0.7377 - val_loss: 0.9336\nEpoch 3/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.7205 - loss: 0.9878 - val_accuracy: 0.7987 - val_loss: 0.7230\nEpoch 4/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.7583 - loss: 0.8396 - val_accuracy: 0.8078 - val_loss: 0.6923\nEpoch 5/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.7881 - loss: 0.7351 - val_accuracy: 0.8297 - val_loss: 0.6115\nEpoch 6/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8105 - loss: 0.6525 - val_accuracy: 0.8151 - val_loss: 0.6617\nEpoch 7/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8270 - loss: 0.5906 - val_accuracy: 0.8242 - val_loss: 0.6283\nEpoch 8/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8399 - loss: 0.5426 - val_accuracy: 0.8668 - val_loss: 0.4937\nEpoch 9/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8529 - loss: 0.4944 - val_accuracy: 0.8553 - val_loss: 0.5544\nEpoch 10/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8616 - loss: 0.4617 - val_accuracy: 0.8413 - val_loss: 0.5880\nEpoch 11/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8722 - loss: 0.4263 - val_accuracy: 0.8575 - val_loss: 0.5408\nEpoch 12/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8808 - loss: 0.3955 - val_accuracy: 0.8729 - val_loss: 0.4766\nEpoch 13/20\n1188/1188 - 142s - 120ms/step - accuracy: 0.8850 - loss: 0.3759 - val_accuracy: 0.8757 - val_loss: 0.4732\nEpoch 14/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8908 - loss: 0.3544 - val_accuracy: 0.8756 - val_loss: 0.4752\nEpoch 15/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8958 - loss: 0.3395 - val_accuracy: 0.8714 - val_loss: 0.4825\nEpoch 16/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.9034 - loss: 0.3163 - val_accuracy: 0.8788 - val_loss: 0.4781\nEpoch 17/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.9044 - loss: 0.3063 - val_accuracy: 0.8780 - val_loss: 0.4819\nEpoch 18/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.9087 - loss: 0.2942 - val_accuracy: 0.8817 - val_loss: 0.4733\nEpoch 19/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.9116 - loss: 0.2827 - val_accuracy: 0.8807 - val_loss: 0.4832\nEpoch 20/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.9190 - loss: 0.2619 - val_accuracy: 0.8762 - val_loss: 0.4885\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1710570744.765050     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "594/594 - 4s - 6ms/step - accuracy: 0.8757 - loss: 0.4733\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1710570747.929443     105 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nFold 3 - Test Accuracy: 87.57%\n\nTraining on Fold 4/5\nModel built successfully.\nEpoch 1/20\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1710570769.512131     104 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1710570870.023518     106 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1710570871.736360     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1188/1188 - 121s - 102ms/step - accuracy: 0.4222 - loss: 2.1669 - val_accuracy: 0.6468 - val_loss: 1.2416\nEpoch 2/20\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1710570874.716350     105 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1188/1188 - 93s - 79ms/step - accuracy: 0.6461 - loss: 1.2593 - val_accuracy: 0.7446 - val_loss: 0.9061\nEpoch 3/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.7197 - loss: 0.9972 - val_accuracy: 0.7389 - val_loss: 0.9171\nEpoch 4/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.7572 - loss: 0.8518 - val_accuracy: 0.7462 - val_loss: 0.9229\nEpoch 5/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.7862 - loss: 0.7449 - val_accuracy: 0.8324 - val_loss: 0.6020\nEpoch 6/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8090 - loss: 0.6618 - val_accuracy: 0.8241 - val_loss: 0.6313\nEpoch 7/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8243 - loss: 0.6031 - val_accuracy: 0.8362 - val_loss: 0.5805\nEpoch 8/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8403 - loss: 0.5417 - val_accuracy: 0.8516 - val_loss: 0.5465\nEpoch 9/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8495 - loss: 0.5051 - val_accuracy: 0.8666 - val_loss: 0.4870\nEpoch 10/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8614 - loss: 0.4639 - val_accuracy: 0.8619 - val_loss: 0.5065\nEpoch 11/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8691 - loss: 0.4331 - val_accuracy: 0.8761 - val_loss: 0.4624\nEpoch 12/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8789 - loss: 0.4049 - val_accuracy: 0.8715 - val_loss: 0.4712\nEpoch 13/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8851 - loss: 0.3833 - val_accuracy: 0.8729 - val_loss: 0.4816\nEpoch 14/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8894 - loss: 0.3617 - val_accuracy: 0.8764 - val_loss: 0.4773\nEpoch 15/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8962 - loss: 0.3381 - val_accuracy: 0.8775 - val_loss: 0.4677\nEpoch 16/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.9023 - loss: 0.3236 - val_accuracy: 0.8727 - val_loss: 0.4989\nEpoch 17/20\n1188/1188 - 142s - 120ms/step - accuracy: 0.9047 - loss: 0.3082 - val_accuracy: 0.8826 - val_loss: 0.4604\nEpoch 18/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.9121 - loss: 0.2882 - val_accuracy: 0.8842 - val_loss: 0.4533\nEpoch 19/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.9126 - loss: 0.2822 - val_accuracy: 0.8814 - val_loss: 0.4822\nEpoch 20/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.9143 - loss: 0.2736 - val_accuracy: 0.8808 - val_loss: 0.4869\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1710572690.591376     105 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "594/594 - 4s - 6ms/step - accuracy: 0.8842 - loss: 0.4534\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1710572693.839040     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nFold 4 - Test Accuracy: 88.42%\n\nTraining on Fold 5/5\nModel built successfully.\nEpoch 1/20\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1710572716.110131     106 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1710572816.304209     106 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1710572817.918651     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1188/1188 - 120s - 101ms/step - accuracy: 0.4164 - loss: 2.1735 - val_accuracy: 0.5844 - val_loss: 1.4779\nEpoch 2/20\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1710572820.928133     105 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1188/1188 - 93s - 78ms/step - accuracy: 0.6442 - loss: 1.2735 - val_accuracy: 0.6412 - val_loss: 1.2824\nEpoch 3/20\n1188/1188 - 142s - 120ms/step - accuracy: 0.7148 - loss: 1.0045 - val_accuracy: 0.7863 - val_loss: 0.7449\nEpoch 4/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.7576 - loss: 0.8502 - val_accuracy: 0.8227 - val_loss: 0.6319\nEpoch 5/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.7876 - loss: 0.7453 - val_accuracy: 0.8289 - val_loss: 0.6116\nEpoch 6/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8089 - loss: 0.6622 - val_accuracy: 0.8298 - val_loss: 0.6039\nEpoch 7/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8243 - loss: 0.6017 - val_accuracy: 0.8498 - val_loss: 0.5280\nEpoch 8/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8386 - loss: 0.5506 - val_accuracy: 0.8491 - val_loss: 0.5396\nEpoch 9/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8511 - loss: 0.5034 - val_accuracy: 0.8587 - val_loss: 0.5338\nEpoch 10/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8608 - loss: 0.4678 - val_accuracy: 0.8726 - val_loss: 0.4700\nEpoch 11/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8708 - loss: 0.4290 - val_accuracy: 0.8644 - val_loss: 0.5017\nEpoch 12/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8774 - loss: 0.4090 - val_accuracy: 0.8695 - val_loss: 0.4789\nEpoch 13/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8864 - loss: 0.3771 - val_accuracy: 0.8779 - val_loss: 0.4624\nEpoch 14/20\n1188/1188 - 142s - 120ms/step - accuracy: 0.8924 - loss: 0.3575 - val_accuracy: 0.8722 - val_loss: 0.5085\nEpoch 15/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8957 - loss: 0.3415 - val_accuracy: 0.8766 - val_loss: 0.4673\nEpoch 16/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.8999 - loss: 0.3219 - val_accuracy: 0.8775 - val_loss: 0.4545\nEpoch 17/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.9057 - loss: 0.3087 - val_accuracy: 0.8855 - val_loss: 0.4551\nEpoch 18/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.9108 - loss: 0.2892 - val_accuracy: 0.8795 - val_loss: 0.4756\nEpoch 19/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.9133 - loss: 0.2814 - val_accuracy: 0.8822 - val_loss: 0.4756\nEpoch 20/20\n1188/1188 - 93s - 78ms/step - accuracy: 0.9151 - loss: 0.2720 - val_accuracy: 0.8825 - val_loss: 0.4654\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1710574687.159070     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "594/594 - 4s - 6ms/step - accuracy: 0.8775 - loss: 0.4547\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "W0000 00:00:1710574690.399744     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nFold 5 - Test Accuracy: 87.75%\nTrained model saved successfully at: /kaggle/working/DFNet_OpenWorld_WTFPAD.keras\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kZTLw073EEGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e9nKxrLZEEGA"
      }
    }
  ]
}